{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VZXi_KGi0UR"
   },
   "source": [
    "# Task 1: Word Embeddings (10 points)\n",
    "\n",
    "This notebook will guide you through all steps necessary to train a word2vec model (Detailed description in the PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48t-II1vkuau"
   },
   "source": [
    "## Imports\n",
    "\n",
    "This code block is reserved for your imports. \n",
    "\n",
    "You are free to use the following packages: \n",
    "\n",
    "(List of packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "4kh6nh84-AOL"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data as dt\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWmk3hVllEcU"
   },
   "source": [
    "# 1.1 Get the data (0.5 points)\n",
    "\n",
    "The Hindi portion HASOC corpus from [github.io](https://hasocfire.github.io/hasoc/2019/dataset.html) is already available in the repo, at data/hindi_hatespeech.tsv . Load it into a data structure of your choice. Then, split off a small part of the corpus as a development set (~100 data points).\n",
    "\n",
    "If you are using Colab the first two lines will let you upload folders or files from your local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "XtI7DJ-0-AOP"
   },
   "outputs": [],
   "source": [
    "# 0 for whole dataset, 1 for development set\n",
    "dev = 1\n",
    "\n",
    "text_id = dt.Field()\n",
    "text = dt.Field()\n",
    "task_1 = dt.Field()\n",
    "task_2 = dt.Field()\n",
    "task_3 = dt.Field()\n",
    "\n",
    "columns = [\n",
    "    ('id', text_id),\n",
    "    ('content', text),\n",
    "    ('t_1', task_1),\n",
    "    (None, None),\n",
    "    (None, None)\n",
    "]\n",
    "\n",
    "data, v, t = dt.TabularDataset.splits(\n",
    "    path = 'data',\n",
    "    train = 'hindi_hatespeech.tsv',\n",
    "    validation = 'hindi_hatespeech.tsv',\n",
    "    test = 'hindi_hatespeech.tsv',\n",
    "    format = 'tsv',\n",
    "    fields = columns,\n",
    "    skip_header = True\n",
    ")\n",
    "\n",
    "# Take the first 100 datapoints of the corpus into the development set\n",
    "dev_set = data[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-mSJ8nUlupB"
   },
   "source": [
    "## 1.2 Data preparation (0.5 + 0.5 points)\n",
    "\n",
    "* Prepare the data by removing everything that does not contain information. \n",
    "User names (starting with '@') and punctuation symbols clearly do not convey information, but we also want to get rid of so-called [stopwords](https://en.wikipedia.org/wiki/Stop_word), i. e. words that have little to no semantic content (and, but, yes, the...). Hindi stopwords can be found [here](https://github.com/stopwords-iso/stopwords-hi/blob/master/stopwords-hi.txt) Then, standardize the spelling by lowercasing all words.\n",
    "Do this for the development section of the corpus for now.\n",
    "\n",
    "* What about hashtags (starting with '#') and emojis? Should they be removed too? Justify your answer in the report, and explain how you accounted for this in your implementation.\n",
    "\n",
    "+ Decided not to exclude hashtags, since they do definitely contain information that could be able to help deciding if the post contains hatespeech.\n",
    "+ Didn't exclude emojis as well, since they are perfect to capture emotions, especially anger or disappointment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "CHcNeyKi-AOQ"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ahmeds', 'dad', 'beta', 'aaj', 'teri', 'mammy', 'kyu', 'nahi', 'baat', 'kr', 'rhi', 'h', 'ahmed']\n['बांग्लादेश', 'शानदार', 'वापसी', 'भारत', '314', 'रन', 'रोका', '#indvban', '#cwc19']\n['सब', 'रंडी', 'नाच', 'देखने', 'व्यस्त', '#शांतीदूत', 'होगा', 'सब', '#रंडीरोना', 'शुरू', 'देंगे']\n['तुम', 'हरामियों', 'बस', 'जूतों', 'कमी', 'शुक्र', 'तुम्हारी', 'लिंचिंग', 'हिंदुओं', 'जागने', 'देर', 'सच', 'होगी', 'तुम', 'हरामी', 'सुवर', 'ड्रामा', 'बनाएं', 'सुवर', 'कहीं', 'मौलाना।', 'तुम', 'हरामियों', 'कुत्ते', 'मौत', 'मारना', 'चाहिए', 'सुवर', 'जैसी', 'शक्ल', 'रंडी', 'औलाद', 'सुवर', 'कहीं', '।।।।']\n['बीजेपी', 'mla', 'आकाश', 'विजयवर्गीय', 'जेल', 'रिहा', 'जमानत', 'मिलने', 'खुशी', 'समर्थक', 'इंदौर', 'हर्ष', 'फायरिंग', '#akashvijayvargiya', '…']\n['चमकी', 'बुखार', 'विधानसभा', 'परिसर', 'आरजेडी', 'प्रदर्शन', 'तेजस्वी', 'यादव', 'नदारद', '#biharencephalitisdeaths', '…', 'रिपोर्ट']\n['मुंबई', 'बारिश', 'लोगों', 'काफी', 'समस्या', 'रही']\n['ahmeds', 'dad', 'beta', 'aaj', 'teri', 'mammy', 'kyu', 'nahi', 'baat', 'kr', 'rhi', 'h', 'ahmed']\n['5', 'लाख', 'मुसलमान', 'उर्स', 'में', 'अजमेर', 'दरगाह', 'आते', 'हैं', 'सिर्फ', '300', 'पुलिस', 'वालों', 'भरोसे', '2', 'लाख', 'हिंदुओं', 'अमरनाथ', 'यात्रा', 'लिए', '80', 'हजार', 'कमांडो', 'पैरामिलिट्री', 'फोर्स', 'तथा', 'करोड़ों', 'उपकरण', 'लगाए', '#खतरे', '#कौन']\n['do', 'mahashaktiyan', 'mili', 'hain', 'charo', 'taraf', 'khusi', 'ki', 'leher', 'hai', 'khus', 'hone', 'wale', 'khus', 'hi', 'rhe', 'hain', 'aur', 'bhakton', 'ko', 'taklif', 'ho', 'rhi', 'hai', 'khair', 'honi', 'bhi', 'chahiye']\n['chants', 'of', 'jai', 'sri', 'ram', 'as', 'owaisi', 'takes', 'oath', 'aimim', 'chief', 'responds', 'with', 'jai', 'bhim']\n['नृत्य', 'होगा', 'संगीत', 'होगा', 'जीवन', 'उत्सव', 'होगा', 'वहआतुर', 'प्राणों', 'उतरने', 'को', 'तुमहृदय', 'दुवार', 'खोलो', 'जगह', 'खाली', 'करो', '#हिंदीशब्द']\n['गाये', 'नाम', 'जुल्म', 'वालों', 'तुम्हारी', 'औकात', 'देश', 'दस', 'बड़े', 'गाये', 'मांस', 'निर्यात', 'हिंदुओं', 'हिसाब', 'लेने', 'की']\n['डॉ', 'मुखर्जी', 'देश', 'विधान', 'प्रधान', 'निशान', 'विरोध', 'कश्मीर', 'भारत', 'अभिन्न', 'अंग', 'मानकर', 'परमिट', 'राज', 'खत्म', 'लड़ाई', 'लड़ी', 'बलिदान', 'परमिट', 'राज', 'खत्म', 'हुआ', 'श्री']\n['धार्मिक', 'पदों', 'बैठे', 'लोग', 'भगवान', 'समझ', 'लेते', 'हैंकुछ', 'लोग', 'नत', 'मस्तक', 'भगवान', 'दर्जा', 'दिलाने', 'समय', 'बर्बाद', 'हैं']\n['सिक्का', 'मोहब्बत', 'उछाल', 'रक्खा', 'हाँ', 'खुद', 'सम्भाल', 'रक्खा', 'अकेले', 'तुम', 'नही', 'परेशान', 'उसने', 'बहुतों', 'उलझन', 'डाल', 'रक्खा', 'शाहरुख', 'सिद्दीकी', '#shahrukhsiddiqui']\n['कहता', 'आँखो', 'प्यार', 'नही', 'होता।', 'डूब', 'उसमे', 'पार', 'नही', 'होता।', 'इश्क़', 'बाज़ार', 'दस्तूर', 'ज़माने', 'में।', 'लूट', 'कभी', 'खरीदार', 'नही', 'होता।', 'नईम', '#बज़्म', '#हिंदीशब्द']\n['वैसे', 'शपथ', 'अंत', 'जय', 'भारतजय', 'संविधान', 'बोलना', 'अनिवार्य', 'हैं।']\n['लंबे', 'अंतराल', 'आपके', 'बीच', '#mannkibaat', 'जनजन', 'बात', 'जनमन', 'बात', 'हम', 'सिलसिला', 'जारी', 'हैं।', 'चुनाव', 'आपाधापी', 'व्यस्तता', 'ज्यादा', 'मन', 'बात', 'मजा', 'गायब', 'था', 'कमी', 'महसूस', 'था।', 'हम', '130', 'करोड़', 'देशवासियों', 'स्वजन', 'रूप', 'बातें', 'थे', 'पीएम', 'जी', '।']\n['#नीचसमानार्थी', 'शब्द', '#मोदी', 'दिला', 'तर', 'चालेल', 'की', 'लै', 'परफेक्ट', 'बसलं']\n['डॉक्टर', 'लिखी', '7', 'दिन', 'दवा', 'बजाय', '3', 'दिन', 'दवा', 'लेना', 'आम', 'भारतीय', 'गुण', 'है']\n['मादरचोद', 'धुण्डके', 'गांड', 'गोली', 'मरो']\n['भारतीय', 'रेल', 'सोमवार', 'बदल', 'जाएगा', 'टाइमटेबल', 'ईस्टर्न', 'सेंट्रल', 'रेलवे', 'ट्रेनों', 'समय', 'बदलाव']\n['you', 'cry', 'in', 'front', 'of', 'ur', 'god', 'of', 'being', 'deceived', 'by', 'the', 'world', 'some', 'fine', 'day', 'u', 'die', 'reach', 'hell', 'god', 'tells', 'you', 'you', 'fucker', 'or', 'भोसडीके', 'no', 'one', 'deceived', 'you', 'more', 'than', 'urself']\n['बस', 'सवाल', 'जवाब', 'चाहिए', 'चिटफंड', 'घोटाले', 'जांच', 'आदेश', '2014', 'आए', 'उसमें', 'पूछताछकार्रवाई', '2019', 'क्यों', 'शुरू', 'रही', 'है', 'सवाल', 'सब', 'छुपा', 'है।', 'सही', 'गलत', 'पहचान', 'लीजिए।', 'follow', 'करो']\n['नतीजे', 'आने', 'ज़िम्मेदारी', 'मिलनी', 'शुरू', 'गयी', 'केजरीवाल', 'हार', 'ज़िम्मेदारी', 'मुसलमानो', 'दे']\n['क्लिक', 'पढ़ें', 'स्मार्टफोन', 'बारे', 'में']\n['हद', 'जाती', 'आरक्षण', 'लाभ', 'लेने', 'लोग', 'आरक्षण', 'खिलाफ', 'भोंकने', 'लग', 'जाते']\n['#aimim', 'national', 'spokesperson', 'live', 'on', '#aarpaar']\n['हरि', 'ॐ', 'नमो', 'नारायणा', 'भजन', 'सुनकर', 'झूम', 'उठा', 'भक्तों', 'सैलाब', '#hariom', '#narayana', '#radheradhe', '#jaishrikrishna', '#jaishriram', '#kalyug', '#ghanshyam', '#kanhaiya', '#acharyapramodkrishnam']\n['प्लीज', 'कॉल', 'मेरी', 'जबरदस्ती', 'चुदाई', 'करो']\n['ठोकरें', 'खाता', 'हूँ', '‘शान’', 'चलता', 'हूँ', 'मैं', 'खुले', 'आसमान', 'सीना', 'तान', 'चलता', 'हूँ', 'मुश्किलें', '‘साज़’', 'जिंदगी', 'उठूंगा', 'गिरूंगा', 'उठूंगा', 'आखिर', 'में…', 'जीतूंगा', 'मैं', 'ठान', 'चलता', 'हूँआदरणीय', 'अखिलेश', 'यादव', 'जिंदाबाद']\n['#cwc2019', 'भारत', 'पहली', 'हार', 'सामना', 'पड़ा।', 'रोहित', 'शर्मा', 'शतक', 'मोहम्मद', 'शमी', '5', 'विकेट', 'काम', 'आए', 'इंग्लैंड', '31', 'रन', 'जीत']\n['चाहे', 'दिल्ली', 'हो', 'उत्तर', 'प्रदेश', 'मध्य', 'प्रदेश', 'राजस्थान।', 'देश', 'हर', 'कोने', 'नन्हें', 'मासूमों', 'हर', 'रोज', 'रेप', 'हैवानियत', 'ख़बरें', 'आ', 'रही', 'हमारे', 'देश', 'कानून', 'इतना', 'नाकारा', 'अपराधियों', 'हौसले', 'बुलंद', 'हैं।', 'सरकारें', 'राजनीति', 'चक्कर', 'कड़े', 'फैसले', 'ले', 'रही']\n['अमित', 'शाह', 'तू', 'पूछेगी', 'क्या', 'रंडी']\n['sir', 'ap', 'se', 'ak', 'rquest', 'h', 'ak', 'bar', 'fir', 'ap', 'mulayam', 'ji', 'ko', 'aage', 'kr', 'de', 'dekhey', 'parti', 'bhut', 'aage', 'jaygi']\n['फूल', 'काँटों', 'दोस्ती', 'बेमिसाल', 'काँटे', 'दर्द', 'फूलों', 'देते', 'फूल', 'मुस्कुराते', 'खिलते', 'काँटों', 'देखो', 'फूलों', 'रक्षा', 'खुद', 'दर्द', 'सहकर', 'फूलों', 'महफूज़', 'रखते', '।', '#आपकीकाँची', '#हिंदीशब्द', '#शब्दनिधि']\n['question', 'hour', 'in', 'rajya', 'sabha', '1', 'संस्कृत', 'महाविद्यालय', '।', '…']\n['राजधानी', 'दिल्ली', 'गर्मी', 'कहर', 'जारी', 'आज', 'शाम', 'हल्की', 'बारिश', 'संभावना', '#rain', '…']\n['#कुत्ते', '#मुर्गे', 'लड़वाने', 'वालों', 'बेज़ुबान', '#जानवरों', 'बद', 'तरीन', '#लानती', 'हैवान', 'नुमा', '#इंसान', 'दुनिया', 'चंद', '#सिक्कों', 'घिनौनी', 'काम']\n['लोन', 'देने', 'नाम', 'बैंक', 'निजी', 'फाईनैंस', 'कंपनियां', 'पूरे', 'देश', 'लूट', 'रही', 'देश', 'प्रधानमंत्री', 'योग', 'सिखा', 'है']\n['नफरत', 'करनी', 'उतनी', 'लो', 'रंडवो', 'बाजारू', 'रंडी', 'औलादो।।']\n['#bedtet2011बेरोजगार', 'कैसे', 'देशप्रदेश', 'तानाशाह', 'मुखिया', 'हैँ', '#bedtet2011', 'बेरोजगारों', '#दयनीय', 'हालत', 'आँख', 'बंद', 'किये', 'मदमस्त', 'हैँ#गूंगे#बेहरे#कुत्ते#कमीनेनेताओं', 'बदतर']\n['proud', 'of', 'ji', 'आज', 'सोनभद्र', 'सिंगरौली', 'क्षेत्र', 'साँसों', 'कोयला', 'लेने', 'मजबूर', 'करोड़ो', 'लोगों', 'साफ', 'हवा', 'अहसास', 'करवाया', 'गैरकानूनी', 'तरीके', 'चल', '5000', 'कोयला', 'ट्रक', 'बंद', 'करवाये', 'लड़ाई', 'लम्बी', 'इरादे', 'मज़बूत', '#singraulipollution']\n['देश', '#कन्हैयाकुमार', 'पढ़ा', 'लिखा', 'युवा', 'हार', 'जाए', '#आतंकवादीप्रज्ञा', 'जीत', 'जाए', 'देश', 'जनता', 'भगवान', 'भला', 'करे']\n['बात', 'उम्\\u200dमीद', 'आम', 'बजट', 'वित्\\u200dत', 'मंत्री', 'निर्मला', 'सीतारमण', 'मिनिमम', 'बैंलेस', 'लेकर', 'अहम', 'ऐलान', 'सकती', '#budget2019', '#modinomics19']\n['chad', 'gundan', 'ki', 'chati', 'pe10', 'sheet', 'le', 'gyi', 'haati', 'waliaur', 'babua', 'ke', 'haate', 'me', 'de', 'gayi', 'katora']\n['#कमलहासन', 'विचार', 'रखने', 'आज़ाद', 'है', '#प्रज्ञाठाकुर', 'क्यो', 'नही', 'क्या', 'देश', '#अभिव्यक्तिकीआज़ादी', 'परिभाषाये', 'है']\n['बाबा', 'साहब', 'डॉ०', 'भीम', 'राव', 'अम्बेडकर', 'हमारे', 'महादेव', 'हैं।।', 'वो', 'अमर', 'होकर', 'आज', 'हमारी', 'सुरक्षा', 'हैं।।', 'बाबा', 'साहब', 'जय', 'हो']\n['सूअर', 'शब्द', 'किसके', 'इस्तेमाल', 'जाता', 'जस्ट', 'पुचिंग']\n['मैदान', 'आपसे', 'गेंदबाज', 'कांपते', 'थे', 'मैच', 'आपने', 'जिताए', 'यादगार', 'हैं।', 'मैंने', 'क्रिकेट', 'देखना', 'शुरू', 'आपकी', 'इण्डिया', 'टीम', 'इन्ट्री', 'थी।', 'इसलिए', 'मुझपर', 'क्रिकेट', 'जुनून', 'आपको', 'देखकर', 'चढ़ा।', 'स्पेशली', 'छक्के', 'लगाने', 'आपका', 'जवाब', 'था।', 'always', 'miss', 'you', '#yuvrajsingh']\n0 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Load the linked stopwords file into a list\n",
    "with open('data/stopwords-hi.txt') as f:\n",
    "    stopwords = [line.rstrip() for line in f]\n",
    "if dev == 0:\n",
    "    dev_set = data\n",
    "def prepare(data_set):\n",
    "    for line in data_set:\n",
    "        text = vars(line)[\"content\"]\n",
    "        remove_list = []\n",
    "        for i in range(len(text)):\n",
    "            text[i] = text[i].lower()\n",
    "            word = text[i]\n",
    "            # find user names\n",
    "            if (len(word) > 0 and word[0] == '@' or len(word) > 7 and word[0:5] == 'https'):\n",
    "                remove_list.append(word)\n",
    "            # find stopwords\n",
    "            if (word in stopwords):\n",
    "                remove_list.append(word)\n",
    "        # finally remove the names and stopwords\n",
    "        for word in remove_list:\n",
    "            text.remove(word)\n",
    "        \n",
    "    for line in data_set:\n",
    "        text = vars(line)[\"content\"]\n",
    "        remove_list = []\n",
    "        for i in range(len(text)):\n",
    "            # delete punctuation symbols # TODO somehow not deleting the characters\n",
    "            for p_symbol in string.punctuation:\n",
    "                if p_symbol not in ['#', '@']:\n",
    "                    text[i] = text[i].replace(p_symbol, '')\n",
    "            if text[i] == '':\n",
    "                remove_list.append('')\n",
    "        for word in remove_list:\n",
    "            text.remove(word)\n",
    "if dev == 1:\n",
    "    prepare(dev_set)\n",
    "if dev == 0:\n",
    "    prepare(data)\n",
    "# TODO LOWERCASE EVERYTHING\n",
    "contents = [vars(line)[\"content\"] for line in dev_set]\n",
    "if dev == 0:\n",
    "    contents = [vars(line)[\"content\"] for line in data]\n",
    "for line in contents:\n",
    "    if len(line) < 0:\n",
    "        contents.remove(line)\n",
    "print(vars(dev_set[6])[\"content\"])\n",
    "for i in range(50):\n",
    "    print(vars(dev_set[i])[\"content\"])\n",
    "c = 0\n",
    "# remove empty sentences\n",
    "contents = list(filter(None, contents))\n",
    "for sentence in contents:\n",
    "    if len(sentence) < 1:\n",
    "        c += 1\n",
    "print(c,type(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je09nozLmmMm"
   },
   "source": [
    "## 1.3 Build the vocabulary (0.5 + 0.5 points)\n",
    "\n",
    "The input to the first layer of word2vec is an one-hot encoding of the current word. The output of the model is then compared to a numeric class label of the words within the size of the skip-gram window. Now\n",
    "\n",
    "* Compile a list of all words in the development section of your corpus and save it in a variable ```V```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "VpoGmTKx-AOQ"
   },
   "outputs": [],
   "source": [
    "V = set()\n",
    "for line in dev_set:\n",
    "    for word in vars(line)[\"content\"]:\n",
    "        V.add(word)\n",
    "       \n",
    "V_corpus = set()\n",
    "for line in data:\n",
    "    for word in vars(line)[\"content\"]:\n",
    "        V_corpus.add(word)\n",
    "        \n",
    "V = list(V)\n",
    "V_corpus = list(V_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiaVglVNoENY"
   },
   "source": [
    "* Then, write a function ```word_to_one_hot``` that returns a one-hot encoding of an arbitrary word in the vocabulary. The size of the one-hot encoding should be ```len(v)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "yqPNw6IT-AOQ"
   },
   "outputs": [],
   "source": [
    "def word_to_one_hot_dev(word, dev):\n",
    "    res = []\n",
    "    if dev == 0:\n",
    "        for i in range(len(V_corpus)):\n",
    "            if (V[i] == word):\n",
    "                res.append(1)\n",
    "            else:\n",
    "                res.append(0)\n",
    "    else:\n",
    "        for i in range(len(V)):\n",
    "            if (V[i] == word):\n",
    "                res.append(1)\n",
    "            else:\n",
    "                res.append(0)\n",
    "    return res\n",
    "def word_to_one_hot(word):\n",
    "    return torch.Tensor(word_to_one_hot_dev(word, dev)).to(torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKD8zBlxVclh"
   },
   "source": [
    "## 1.4 Subsampling (0.5 points)\n",
    "\n",
    "The probability to keep a word in a context is given by:\n",
    "\n",
    "$P_{keep}(w_i) = \\Big(\\sqrt{\\frac{z(w_i)}{0.001}}+1\\Big) \\cdot \\frac{0.001}{z(w_i)}$\n",
    "\n",
    "Where $z(w_i)$ is the relative frequency of the word $w_i$ in the corpus. Now,\n",
    "* Calculate word frequencies\n",
    "* Define a function ```sampling_prob``` that takes a word (string) as input and returns the probabiliy to **keep** the word in a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "Mj4sDOVMMr0b"
   },
   "outputs": [],
   "source": [
    "frequencies = {}\n",
    "word_count = 0\n",
    "if dev == 0:\n",
    "    for i in V_corpus:\n",
    "        frequencies[i] = 0\n",
    "\n",
    "\n",
    "    for line in data:\n",
    "        for word in vars(line)[\"content\"]:\n",
    "            frequencies[word] += 1\n",
    "            word_count += 1\n",
    "\n",
    "    for key in frequencies:\n",
    "        frequencies[key] = frequencies[key]/word_count\n",
    "else:\n",
    "    for i in V:\n",
    "        frequencies[i] = 0\n",
    "\n",
    "\n",
    "    for line in dev_set:\n",
    "        for word in vars(line)[\"content\"]:\n",
    "            frequencies[word] += 1\n",
    "            word_count += 1\n",
    "\n",
    "    for key in frequencies:\n",
    "        frequencies[key] = frequencies[key]/word_count\n",
    "    \n",
    "def sampling_prob(word):\n",
    "    return (math.sqrt(frequencies[word] * 1000) + 1) * (0.001/frequencies[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxV1P90zplxu"
   },
   "source": [
    "# 1.5 Skip-Grams (1 point)\n",
    "\n",
    "Now that you have the vocabulary and one-hot encodings at hand, you can start to do the actual work. The skip gram model requires training data of the shape ```(current_word, context)```, with ```context``` being the words before and/or after ```current_word``` within ```window_size```. \n",
    "\n",
    "* Have closer look on the original paper. If you feel to understand how skip-gram works, implement a function ```get_target_context``` that takes a sentence as input and [yield](https://docs.python.org/3.9/reference/simple_stmts.html#the-yield-statement)s a ```(current_word, context)```.\n",
    "\n",
    "* Use your ```sampling_prob``` function to drop words from contexts as you sample them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "r8CCTpVy-AOR"
   },
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "def get_target_context(sentence):\n",
    "\t# get a random word (its position in the sentence)\n",
    "    current_index = random.randint(0, len(sentence) - 1)\n",
    "    context = []\n",
    "    min_index = 0 if current_index < window_size else current_index - window_size\n",
    "    max_index = len(sentence)-1 if current_index + window_size > len(sentence) else current_index + window_size\n",
    "    context = sentence[min_index:current_index] + sentence[current_index+1:max_index+1]\n",
    "    current_word = sentence[current_index]\n",
    "    sampled_out = []\n",
    "    for word in context:\n",
    "        p_keep = sampling_prob(word)\n",
    "        if (random.random() > p_keep):\n",
    "            sampled_out.append(word)\n",
    "    for word in sampled_out:\n",
    "        context.remove(word)\n",
    "    valid = 1\n",
    "    if (type(current_word) is not str or type(current_word) is tuple):\n",
    "        valid = 0\n",
    "    for w in context:\n",
    "        if (type(w) is not str or type(w) is tuple):\n",
    "            valid = 0\n",
    "    if valid == 0:\n",
    "        sys.exit(0)\n",
    "    #print(type(current_word))\n",
    "    return current_word, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfEFgtkmuDjL"
   },
   "source": [
    "# 1.6 Hyperparameters (0.5 points)\n",
    "\n",
    "According to the word2vec paper, what would be a good choice for the following hyperparameters? \n",
    "\n",
    "* Embedding dimension\n",
    "* Window size\n",
    "\n",
    "Initialize them in a dictionary or as independent variables in the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "d7xSKuFJcYoD"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "window_size = 10\n",
    "embedding_size = 300\n",
    "\n",
    "# More hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiM2zq-YunPx"
   },
   "source": [
    "# 1.7 Pytorch Module (0.5 + 0.5 + 0.5 points)\n",
    "\n",
    "Pytorch provides a wrapper for your fancy and super-complex models: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The code block below contains a skeleton for such a wrapper. Now,\n",
    "\n",
    "* Initialize the two weight matrices of word2vec as fields of the class.\n",
    "\n",
    "* Override the ```forward``` method of this class. It should take a one-hot encoding as input, perform the matrix multiplications, and finally apply a log softmax on the output layer.\n",
    "\n",
    "* Initialize the model and save its weights in a variable. The Pytorch documentation will tell you how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9sGNytYhwxS",
    "outputId": "41645b64-e4ed-4e6a-e10f-74cb39b92230"
   },
   "outputs": [],
   "source": [
    "# Create model \n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.window_size = window_size\n",
    "    self.embedding_size = embedding_size\n",
    "    if dev == 0:\n",
    "        self.inl = nn.Embedding(len(V_corpus), self.embedding_size)\n",
    "        self.out = nn.Linear(self.embedding_size, len(V_corpus))\n",
    "    else:\n",
    "        self.inl = nn.Embedding(len(V), self.embedding_size)\n",
    "        self.out = nn.Linear(self.embedding_size, len(V))\n",
    "    \n",
    "\n",
    "\n",
    "  def forward(self, one_hot):\n",
    "    res = self.out(self.inl(one_hot)) \n",
    "    #return F.softmax(torch.Tensor(res))\n",
    "    return torch.Tensor(res)\n",
    "\n",
    "model = Word2Vec()\n",
    "path = 'task1.pt'\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XefIDMMHv5zJ"
   },
   "source": [
    "# 1.8 Loss function and optimizer (0.5 points)\n",
    "\n",
    "Initialize variables with [optimizer](https://pytorch.org/docs/stable/optim.html#module-torch.optim) and loss function. You can take what is used in the word2vec paper, but you can use alternative optimizers/loss functions if you explain your choice in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "V9-Ino-e29w3"
   },
   "outputs": [],
   "source": [
    "# Define optimizer and loss\n",
    "\n",
    "# [2] says they use Adagrad\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# [1, 2] say nothing (?) , use negative log likelihood (Stanford video) ? -> weird af\n",
    "criterion = nn.NLLLoss()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckTfK78Ew8wI"
   },
   "source": [
    "# 1.9 Training the model (3 points)\n",
    "\n",
    "As everything is prepared, implement a training loop that performs several passes of the data set through the model. You are free to do this as you please, but your code should:\n",
    "\n",
    "* Load the weights saved in 1.6 at the start of every execution of the code block\n",
    "* Print the accumulated loss at least after every epoch (the accumulate loss should be reset after every epoch)\n",
    "* Define a criterion for the training procedure to terminate if a certain loss value is reached. You can find the threshold by observing the loss for the development set.\n",
    "\n",
    "You can play around with the number of epochs and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "LbMGD5L0mLDx",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training started\n",
      "Epoch  0  - loss :  0.07494012273923316\n",
      "Epoch  1  - loss :  0.025600890920619773\n",
      "Epoch  2  - loss :  0.0033748140840819387\n",
      "Epoch  3  - loss :  0.00136854477000959\n",
      "Epoch  4  - loss :  0.0009109845215624028\n",
      "Epoch  5  - loss :  0.0006944758422446973\n",
      "Epoch  6  - loss :  0.0005664099195990899\n",
      "Epoch  7  - loss :  0.00048123966112281335\n",
      "Epoch  8  - loss :  0.00042026798532466696\n",
      "Epoch  9  - loss :  0.00037435844841629567\n",
      "Epoch  10  - loss :  0.0003384657655701493\n",
      "Epoch  11  - loss :  0.00030958336411100447\n",
      "Epoch  12  - loss :  0.0002858231991830498\n",
      "Epoch  13  - loss :  0.0002658975244772555\n",
      "Epoch  14  - loss :  0.0002489388365336139\n",
      "Epoch  15  - loss :  0.00023429618790896253\n",
      "Epoch  16  - loss :  0.0002215337948967712\n",
      "Epoch  17  - loss :  0.000210289532939593\n",
      "Epoch  18  - loss :  0.00020029860539267762\n",
      "Epoch  19  - loss :  0.00019136257469654083\n",
      "Epoch  20  - loss :  0.00018331547728692643\n",
      "Epoch  21  - loss :  0.0001760170315251206\n",
      "Epoch  22  - loss :  0.0001693744438164162\n",
      "Epoch  23  - loss :  0.00016329507108288582\n",
      "Epoch  24  - loss :  0.0001576775967171698\n",
      "Epoch  25  - loss :  0.0001525306189903105\n",
      "Epoch  26  - loss :  0.00014773779783887092\n",
      "Epoch  27  - loss :  0.00014328832427660623\n",
      "Epoch  28  - loss :  0.00013913699623310205\n",
      "Epoch  29  - loss :  0.00013524258151800946\n",
      "Epoch  30  - loss :  0.00013159450632755203\n",
      "Epoch  31  - loss :  0.0001281796945164902\n",
      "Epoch  32  - loss :  0.00012493321690896546\n",
      "Epoch  33  - loss :  0.00012187624933442684\n",
      "Epoch  34  - loss :  0.00011898565924528873\n",
      "Epoch  35  - loss :  0.00011624894434153432\n",
      "Epoch  36  - loss :  0.00011363819316782132\n",
      "Epoch  37  - loss :  0.000111155832807223\n",
      "Epoch  38  - loss :  0.00010878910696265674\n",
      "Epoch  39  - loss :  0.00010652918218061178\n",
      "Epoch  40  - loss :  0.0001043629070574587\n",
      "Epoch  41  - loss :  0.0001022903850734836\n",
      "Epoch  42  - loss :  0.0001002947489420573\n",
      "Epoch  43  - loss :  9.838409834738934e-05\n",
      "Epoch  44  - loss :  9.654872495718677e-05\n",
      "Epoch  45  - loss :  9.47681020456131e-05\n",
      "Epoch  46  - loss :  9.306692377184376e-05\n",
      "Epoch  47  - loss :  9.14326219847708e-05\n",
      "Epoch  48  - loss :  8.984986278745864e-05\n",
      "Epoch  49  - loss :  8.830016083789594e-05\n",
      "Epoch  50  - loss :  8.681802210783718e-05\n",
      "Epoch  51  - loss :  8.537466967045659e-05\n",
      "Epoch  52  - loss :  8.399057380779825e-05\n",
      "Epoch  53  - loss :  8.262432359083734e-05\n",
      "Epoch  54  - loss :  8.132101760970222e-05\n",
      "Epoch  55  - loss :  8.004532205034989e-05\n",
      "Epoch  56  - loss :  7.880732153702264e-05\n",
      "Epoch  57  - loss :  7.758689385772956e-05\n",
      "Epoch  58  - loss :  7.642794287565982e-05\n",
      "Epoch  59  - loss :  7.527759957193124e-05\n",
      "Epoch  60  - loss :  7.416200682972416e-05\n",
      "Epoch  61  - loss :  7.307679025512753e-05\n",
      "Epoch  62  - loss :  7.201385486758116e-05\n",
      "Epoch  63  - loss :  7.097777731791892e-05\n",
      "Epoch  64  - loss :  6.997023210531534e-05\n",
      "Epoch  65  - loss :  6.898144504638634e-05\n",
      "Epoch  66  - loss :  6.801211228123819e-05\n",
      "Epoch  67  - loss :  6.707289227933595e-05\n",
      "Epoch  68  - loss :  6.614748219197446e-05\n",
      "Epoch  69  - loss :  6.523713318988531e-05\n",
      "Epoch  70  - loss :  6.435250844618287e-05\n",
      "Epoch  71  - loss :  6.348453932488807e-05\n",
      "Epoch  72  - loss :  6.263233213262124e-05\n",
      "Epoch  73  - loss :  6.179683230290509e-05\n",
      "Epoch  74  - loss :  6.0982395415053226e-05\n",
      "Epoch  75  - loss :  6.018151914832568e-05\n",
      "Epoch  76  - loss :  5.939585918729956e-05\n",
      "Epoch  77  - loss :  5.862457828238757e-05\n",
      "Epoch  78  - loss :  5.786710729201635e-05\n",
      "Epoch  79  - loss :  5.7121489498049324e-05\n",
      "Epoch  80  - loss :  5.6387235720952354e-05\n",
      "Epoch  81  - loss :  5.566332526881285e-05\n",
      "Epoch  82  - loss :  5.496327642991085e-05\n",
      "Epoch  83  - loss :  5.426917770745778e-05\n",
      "Epoch  84  - loss :  5.358534705157232e-05\n",
      "Epoch  85  - loss :  5.291536864307192e-05\n",
      "Epoch  86  - loss :  5.225111927950021e-05\n",
      "Epoch  87  - loss :  5.160313043178934e-05\n",
      "Epoch  88  - loss :  5.09526909827584e-05\n",
      "Epoch  89  - loss :  5.0332792329065725e-05\n",
      "Epoch  90  - loss :  4.9706623710767185e-05\n",
      "Epoch  91  - loss :  4.910538913776176e-05\n",
      "Epoch  92  - loss :  4.8502334252451405e-05\n",
      "Epoch  93  - loss :  4.7905243594538085e-05\n",
      "Epoch  94  - loss :  4.733566458177085e-05\n",
      "Epoch  95  - loss :  4.675725681914223e-05\n",
      "Epoch  96  - loss :  4.618898071724959e-05\n",
      "Epoch  97  - loss :  4.563859729754804e-05\n",
      "Epoch  98  - loss :  4.508204268987733e-05\n",
      "Epoch  99  - loss :  4.45406385368169e-05\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# load initial weights\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def sampled(contents):\n",
    "    res = []\n",
    "    for sentence in contents:\n",
    "            res.append(get_target_context(sentence))\n",
    "    return res\n",
    "\n",
    "# Define train procedure\n",
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        train_set = sampled(contents)\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, collate_fn=collate_fn)\n",
    "        accu_loss = 0\n",
    "        i = 0\n",
    "        model.train()\n",
    "        for center, context in train_loader:\n",
    "            i += 1\n",
    "            if type(center) is tuple:\n",
    "                center = center[0]\n",
    "            if type(context[0]) is tuple:\n",
    "                context = [tup[0] for tup in context]\n",
    "            optimizer.zero_grad()\n",
    "            word = word_to_one_hot(center)\n",
    "            prediction = model(word)\n",
    "            expectation = word_to_one_hot(context[0])\n",
    "            for i in range(1, len(context)):\n",
    "                expectation = torch.add(expectation, word_to_one_hot(context[i]))\n",
    "            loss = criterion(prediction, expectation)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            accu_loss += loss.item()\n",
    "        print('Epoch ', epoch, \" - loss : \", accu_loss/i)\n",
    "\n",
    " \n",
    "print(\"Training started\")\n",
    "\n",
    "train()\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgQkaYstyj0Q"
   },
   "source": [
    "# 1.10 Train on the full dataset (0.5 points)\n",
    "\n",
    "Now, go back to 1.1 and remove the restriction on the number of sentences in your corpus. Then, reexecute code blocks 1.2, 1.3 and 1.6 (or those relevant if you created additional ones). \n",
    "\n",
    "* Then, retrain your model on the complete dataset.\n",
    "\n",
    "* Now, the input weights of the model contain the desired word embeddings! Save them together with the corresponding vocabulary items (Pytorch provides a nice [functionality](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x8hQP_bg4_g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNTI_final_project_task_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}