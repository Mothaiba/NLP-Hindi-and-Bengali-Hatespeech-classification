{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VZXi_KGi0UR"
   },
   "source": [
    "# Task 1: Word Embeddings (10 points)\n",
    "\n",
    "This notebook will guide you through all steps necessary to train a word2vec model (Detailed description in the PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48t-II1vkuau"
   },
   "source": [
    "## Imports\n",
    "\n",
    "This code block is reserved for your imports. \n",
    "\n",
    "You are free to use the following packages: \n",
    "\n",
    "(List of packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4kh6nh84-AOL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f76ec823f70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(26)\n",
    "np.random.seed(62)\n",
    "torch.manual_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWmk3hVllEcU"
   },
   "source": [
    "# 1.1 Get the data (0.5 points)\n",
    "\n",
    "The Hindi portion HASOC corpus from [github.io](https://hasocfire.github.io/hasoc/2019/dataset.html) is already available in the repo, at data/hindi_hatespeech.tsv . Load it into a data structure of your choice. Then, split off a small part of the corpus as a development set (~100 data points).\n",
    "\n",
    "If you are using Colab the first two lines will let you upload folders or files from your local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XtI7DJ-0-AOP"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/hindi_hatespeech.tsv', sep='\\t')\n",
    "\n",
    "# uncomment the line below to use a small sample data\n",
    "# data = data.sample(100, replace=False).reset_index(drop=True)\n",
    "\n",
    "sentences = data['text'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out data/statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4665\n",
      "sentence examples: ['बांग्लादेश की शानदार वापसी, भारत को 314 रन पर रोका #INDvBAN #CWC19'\n",
      " 'सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांतीदूत के साथ कुछ होगा सब #रंडीरोना शुरू कर देंगे   '\n",
      " 'तुम जैसे हरामियों के लिए बस जूतों की कमी है शुक्र कर अभी तुम्हारी लिंचिंग हुई नहीं है हिंदुओं के जागने की देर है सच में होगी अभी तो तुम जैसे हरामी सुवर ड्रामा बनाएं हो   सुवर कहीं का मौलाना।   तुम जैसे हरामियों कुत्ते की मौत मारना चाहिए सुवर जैसी शक्ल  रंडी की औलाद सुवर कहीं का ।।।।']\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences:', len(sentences))\n",
    "print('sentence examples:', sentences[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-mSJ8nUlupB"
   },
   "source": [
    "## 1.2 Data preparation (0.5 + 0.5 points)\n",
    "\n",
    "* Prepare the data by removing everything that does not contain information. \n",
    "User names (starting with '@') and punctuation symbols clearly do not convey information, but we also want to get rid of so-called [stopwords](https://en.wikipedia.org/wiki/Stop_word), i. e. words that have little to no semantic content (and, but, yes, the...). Hindi stopwords can be found [here](https://github.com/stopwords-iso/stopwords-hi/blob/master/stopwords-hi.txt) Then, standardize the spelling by lowercasing all words.\n",
    "Do this for the development section of the corpus for now.\n",
    "\n",
    "* What about hashtags (starting with '#') and emojis? Should they be removed too? Justify your answer in the report, and explain how you accounted for this in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove user taggings\n",
    "user_tag_pattern = re.compile(r'\\@\\w*')\n",
    "sentences = [re.sub(user_tag_pattern, ' ', sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "punctuation = string.punctuation[:2] + string.punctuation[3:] # all punctuation without \"#\"\n",
    "translator = str.maketrans(punctuation, ' '*len(punctuation))\n",
    "def remove_punc(s):\n",
    "    s = s.translate(translator)\n",
    "    return s\n",
    "\n",
    "sentences = [remove_punc(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lower case\n",
    "sentences = [sentence.lower() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stopwords = ['अंदर', 'अत', 'अदि', 'अप', 'अपना', 'अपनि', 'अपनी', 'अपने', 'अभि', 'अभी', 'आदि', \n",
    "             'आप', 'इंहिं', 'इंहें', 'इंहों', 'इतयादि', 'इत्यादि', 'इन', 'इनका', 'इन्हीं', 'इन्हें', 'इन्हों', \n",
    "             'इस', 'इसका', 'इसकि', 'इसकी', 'इसके', 'इसमें', 'इसि', 'इसी', 'इसे', 'उंहिं', 'उंहें', \n",
    "             'उंहों', 'उन', 'उनका', 'उनकि', 'उनकी', 'उनके', 'उनको', 'उन्हीं', 'उन्हें', 'उन्हों', 'उस', \n",
    "             'उसके', 'उसि', 'उसी', 'उसे', 'एक', 'एवं', 'एस', 'एसे', 'ऐसे', 'ओर', 'और', 'कइ', \n",
    "             'कई', 'कर', 'करता', 'करते', 'करना', 'करने', 'करें', 'कहते', 'कहा', 'का', 'काफि', \n",
    "             'काफ़ी', 'कि', 'किंहें', 'किंहों', 'कितना', 'किन्हें', 'किन्हों', 'किया', 'किर', 'किस', \n",
    "             'किसि', 'किसी', 'किसे', 'की', 'कुछ', 'कुल', 'के', 'को', 'कोइ', 'कोई', 'कोन', \n",
    "             'कोनसा', 'कौन', 'कौनसा', 'गया', 'घर', 'जब', 'जहाँ', 'जहां', 'जा', 'जिंहें', 'जिंहों', \n",
    "             'जितना', 'जिधर', 'जिन', 'जिन्हें', 'जिन्हों', 'जिस', 'जिसे', 'जीधर', 'जेसा', 'जेसे', \n",
    "             'जैसा', 'जैसे', 'जो', 'तक', 'तब', 'तरह', 'तिंहें', 'तिंहों', 'तिन', 'तिन्हें', 'तिन्हों', \n",
    "             'तिस', 'तिसे', 'तो', 'था', 'थि', 'थी', 'थे', 'दबारा', 'दवारा', 'दिया', 'दुसरा', 'दुसरे', \n",
    "             'दूसरे', 'दो', 'द्वारा', 'न', 'नहिं', 'नहीं', 'ना', 'निचे', 'निहायत', 'नीचे', 'ने', 'पर', \n",
    "             'पहले', 'पुरा', 'पूरा', 'पे', 'फिर', 'बनि', 'बनी', 'बहि', 'बही', 'बहुत', 'बाद', 'बाला', \n",
    "             'बिलकुल', 'भि', 'भितर', 'भी', 'भीतर', 'मगर', 'मानो', 'मे', 'में', 'यदि', 'यह', 'यहाँ', \n",
    "             'यहां', 'यहि', 'यही', 'या', 'यिह', 'ये', 'रखें', 'रवासा', 'रहा', 'रहे', 'ऱ्वासा', 'लिए', \n",
    "             'लिये', 'लेकिन', 'व', 'वगेरह', 'वरग', 'वर्ग', 'वह', 'वहाँ', 'वहां', 'वहिं', 'वहीं', 'वाले', \n",
    "             'वुह', 'वे', 'वग़ैरह', 'संग', 'सकता', 'सकते', 'सबसे', 'सभि', 'सभी', 'साथ', 'साबुत', \n",
    "             'साभ', 'सारा', 'से', 'सो', 'हि', 'ही', 'हुअ', 'हुआ', 'हुइ', 'हुई', 'हुए', 'हे', 'हें', \n",
    "             'है', 'हैं', 'हो', 'होता', 'होति', 'होती', 'होते', 'होना', 'होने']\n",
    "\n",
    "sentences = [[word for word in sentence.split() if word not in stopwords] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out data/statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing is finished\n",
      "['बांग्लादेश', 'शानदार', 'वापसी', 'भारत', '314', 'रन', 'रोका', '#indvban', '#cwc19']\n",
      "['सब', 'रंडी', 'नाच', 'देखने', 'व्यस्त', '#शांतीदूत', 'होगा', 'सब', '#रंडीरोना', 'शुरू', 'देंगे']\n",
      "['तुम', 'हरामियों', 'बस', 'जूतों', 'कमी', 'शुक्र', 'तुम्हारी', 'लिंचिंग', 'हिंदुओं', 'जागने', 'देर', 'सच', 'होगी', 'तुम', 'हरामी', 'सुवर', 'ड्रामा', 'बनाएं', 'सुवर', 'कहीं', 'मौलाना।', 'तुम', 'हरामियों', 'कुत्ते', 'मौत', 'मारना', 'चाहिए', 'सुवर', 'जैसी', 'शक्ल', 'रंडी', 'औलाद', 'सुवर', 'कहीं', '।।।।']\n",
      "['बीजेपी', 'mla', 'आकाश', 'विजयवर्गीय', 'जेल', 'रिहा', 'जमानत', 'मिलने', 'खुशी', 'समर्थक', 'इंदौर', 'हर्ष', 'फायरिंग', '#akashvijayvargiya', 'https', 'abpnews', 'abplive', 'in', 'india', 'news', 'celebratory', 'firing', 'outside', 'bjp', 'mla', 'akash', 'vijayvargiya', 'office', 'in', 'indore', '1157241', '…']\n",
      "['चमकी', 'बुखार', 'विधानसभा', 'परिसर', 'आरजेडी', 'प्रदर्शन', 'तेजस्वी', 'यादव', 'नदारद', '#biharencephalitisdeaths', 'https', 'abpnews', 'abplive', 'in', 'bihar', 'news', 'aes', 'deaths', 'rjd', 'protest', 'in', 'vidhan', 'sabha', 'campus', 'but', 'tejashwi', 'yadav', 'was', 'not', 'present', '1158748', '…', 'रिपोर्ट']\n",
      "['मुंबई', 'बारिश', 'लोगों', 'काफी', 'समस्या', 'रही']\n",
      "['ahmed', 's', 'dad', 'beta', 'aaj', 'teri', 'mammy', 'kyu', 'nahi', 'baat', 'kr', 'rhi', 'h', 'ahmed']\n",
      "['5', 'लाख', 'मुसलमान', 'उर्स', 'अजमेर', 'दरगाह', 'आते', 'सिर्फ', '300', 'पुलिस', 'वालों', 'भरोसे', '2', 'लाख', 'हिंदुओं', 'अमरनाथ', 'यात्रा', '80', 'हजार', 'कमांडो', 'पैरामिलिट्री', 'फोर्स', 'तथा', 'करोड़ों', 'उपकरण', 'लगाए', '#खतरे', '#कौन']\n",
      "['do', 'mahashaktiyan', 'mili', 'hain', 'charo', 'taraf', 'khusi', 'ki', 'leher', 'hai', 'khus', 'hone', 'wale', 'khus', 'hi', 'rhe', 'hain', 'aur', 'bhakton', 'ko', 'taklif', 'ho', 'rhi', 'hai', 'khair', 'honi', 'bhi', 'chahiye']\n",
      "['chants', 'of', 'jai', 'sri', 'ram', 'as', 'owaisi', 'takes', 'oath', 'aimim', 'chief', 'responds', 'with', 'jai', 'bhim']\n"
     ]
    }
   ],
   "source": [
    "print('pre-processing is finished')\n",
    "for sentence in sentences[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je09nozLmmMm"
   },
   "source": [
    "## 1.3 Build the vocabulary (0.5 + 0.5 points)\n",
    "\n",
    "The input to the first layer of word2vec is an one-hot encoding of the current word. The output od the model is then compared to a numeric class label of the words within the size of the skip-gram window. Now\n",
    "\n",
    "* Compile a list of all words in the development section of your corpus and save it in a variable ```V```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VpoGmTKx-AOQ"
   },
   "outputs": [],
   "source": [
    "flattened_words = [word for sentence in sentences for word in sentence]\n",
    "V = list(set(flattened_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiaVglVNoENY"
   },
   "source": [
    "* Then, write a function ```word_to_one_hot``` that returns a one-hot encoding of an arbitrary word in the vocabulary. The size of the one-hot encoding should be ```len(v)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yqPNw6IT-AOQ"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(V)\n",
    "word_to_int = {}\n",
    "int_to_word = {}\n",
    "for i, word in enumerate(V):\n",
    "    word_to_int[word] = i\n",
    "    int_to_word[i] = word\n",
    "\n",
    "def word_to_one_hot(word):\n",
    "    word_id = word_to_int[word]\n",
    "    one_hot_vector = torch.zeros(vocab_size, dtype=torch.long)\n",
    "    one_hot_vector[word_id] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out data/statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 20402\n"
     ]
    }
   ],
   "source": [
    "print(f'vocab_size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKD8zBlxVclh"
   },
   "source": [
    "## 1.4 Subsampling (0.5 points)\n",
    "\n",
    "The probability to keep a word in a context is given by:\n",
    "\n",
    "$P_{keep}(w_i) = \\Big(\\sqrt{\\frac{z(w_i)}{0.001}}+1\\Big) \\cdot \\frac{0.001}{z(w_i)}$\n",
    "\n",
    "Where $z(w_i)$ is the relative frequency of the word $w_i$ in the corpus. Now,\n",
    "* Calculate word frequencies\n",
    "* Define a function ```sampling_prob``` that takes a word (string) as input and returns the probabiliy to **keep** the word in a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Mj4sDOVMMr0b"
   },
   "outputs": [],
   "source": [
    "word_counter = Counter(flattened_words)\n",
    "def sampling_prob(word):\n",
    "    z = word_counter[word] / len(flattened_words)\n",
    "    p_keep = ((z/0.000001)**0.5 + 1) * (0.000001/z)\n",
    "    return p_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxV1P90zplxu"
   },
   "source": [
    "# 1.5 Skip-Grams (1 point)\n",
    "\n",
    "Now that you have the vocabulary and one-hot encodings at hand, you can start to do the actual work. The skip gram model requires training data of the shape ```(current_word, context)```, with ```context``` being the words before and/or after ```current_word``` within ```window_size```. \n",
    "\n",
    "* Have closer look on the original paper. If you feel to understand how skip-gram works, implement a function ```get_target_context``` that takes a sentence as input and [yield](https://docs.python.org/3.9/reference/simple_stmts.html#the-yield-statement)s a ```(current_word, context)```.\n",
    "\n",
    "* Use your ```sampling_prob``` function to drop words from contexts as you sample them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "r8CCTpVy-AOR"
   },
   "outputs": [],
   "source": [
    "def get_target_context(sentence: list(str())):\n",
    "    for i, word in enumerate(sentence):\n",
    "        word_one_hot = word_to_one_hot(word)\n",
    "        for j, context_word in enumerate(sentence[i-window_size:i+window_size+1]):\n",
    "            if j != i and random.random() < sampling_prob(context_word):\n",
    "                yield (torch.tensor(word_to_int[word], dtype=torch.long).unsqueeze(0), \n",
    "                       torch.tensor(word_to_int[context_word], dtype=torch.long).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfEFgtkmuDjL"
   },
   "source": [
    "# 1.6 Hyperparameters (0.5 points)\n",
    "\n",
    "According to the word2vec paper, what would be a good choice for the following hyperparameters? \n",
    "\n",
    "* Embedding dimension\n",
    "* Window size\n",
    "\n",
    "Initialize them in a dictionary or as independent variables in the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "d7xSKuFJcYoD"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "window_size = 10 # in the first paper, the authors use 10.\n",
    "embedding_size = 300 # the first paper doesn't state the best dimension size, they try several options. In the linked code, they use embedding_size = 300.\n",
    "\n",
    "# More hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 256\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiM2zq-YunPx"
   },
   "source": [
    "# 1.7 Pytorch Module (0.5 + 0.5 + 0.5 points)\n",
    "\n",
    "Pytorch provides a wrapper for your fancy and super-complex models: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The code block below contains a skeleton for such a wrapper. Now,\n",
    "\n",
    "* Initialize the two weight matrices of word2vec as fields of the class.\n",
    "\n",
    "* Override the ```forward``` method of this class. It should take a one-hot encoding as input, perform the matrix multiplications, and finally apply a log softmax on the output layer.\n",
    "\n",
    "* Initialize the model and save its weights in a variable. The Pytorch documentation will tell you how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9sGNytYhwxS",
    "outputId": "41645b64-e4ed-4e6a-e10f-74cb39b92230"
   },
   "outputs": [],
   "source": [
    "# Create model \n",
    "class Word2Vec(Module):\n",
    "    def __init__(self):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        # use the Embedding provided by torch instead of the above word_to_one_hot() function.\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.fc = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, word_id):\n",
    "        out = self.embed(word_id)\n",
    "        out = self.fc(out)\n",
    "        return out.squeeze(1)\n",
    "    \n",
    "    def to_embed(self, word_id):\n",
    "        return self.embed(word_id)\n",
    "    \n",
    "word2vec = Word2Vec()\n",
    "save_path = './save/word2vec.pt'\n",
    "torch.save(word2vec.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out data/statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Word2Vec(\n",
       "  (embed): Embedding(20402, 300)\n",
       "  (fc): Linear(in_features=300, out_features=20402, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XefIDMMHv5zJ"
   },
   "source": [
    "# 1.8 Loss function and optimizer (0.5 points)\n",
    "\n",
    "Initialize variables with [optimizer](https://pytorch.org/docs/stable/optim.html#module-torch.optim) and loss function. You can take what is used in the word2vec paper, but you can use alternative optimizers/loss functions if you explain your choice in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "V9-Ino-e29w3"
   },
   "outputs": [],
   "source": [
    "# Define optimizer and loss\n",
    "optimizer = optim.Adam(word2vec.parameters(), lr=learning_rate) # in paper, AdaGrad is used.\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum') # i.e. equivalent to NLL on LogSoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckTfK78Ew8wI"
   },
   "source": [
    "# 1.9 Training the model (3 points)\n",
    "\n",
    "As everything is prepared, implement a training loop that performs several passes of the data set through the model. You are free to do this as you please, but your code should:\n",
    "\n",
    "* Load the weights saved in 1.6 at the start of every execution of the code block\n",
    "* Print the accumulated loss at least after every epoch (the accumulate loss should be reset after every epoch)\n",
    "* Define a criterion for the training procedure to terminate if a certain loss value is reached. You can find the threshold by observing the loss for the development set.\n",
    "\n",
    "You can play around with the number of epochs and the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.data = []\n",
    "        for sentence in sentences:\n",
    "            for data_point in get_target_context(sentence):\n",
    "                self.data.append(data_point)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LbMGD5L0mLDx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:22<00:00, 19.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: training loss: 12.7457 over 113965 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:21<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: training loss: 12.0367 over 113758 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:22<00:00, 19.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: training loss: 10.6651 over 114004 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 447/447 [00:22<00:00, 20.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: training loss: 9.6758 over 114423 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: training loss: 8.9723 over 113814 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 19.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: training loss: 8.4702 over 113638 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:22<00:00, 19.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: training loss: 8.1053 over 113997 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:23<00:00, 19.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: training loss: 7.8091 over 114011 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:22<00:00, 20.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: training loss: 7.6485 over 114006 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 447/447 [00:22<00:00, 19.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: training loss: 7.5325 over 114246 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 447/447 [00:22<00:00, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: training loss: 7.4568 over 114274 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443/443 [00:22<00:00, 19.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: training loss: 7.3913 over 113211 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 19.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: training loss: 7.3485 over 113496 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: training loss: 7.3170 over 113817 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:23<00:00, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: training loss: 7.2866 over 114067 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:23<00:00, 19.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: training loss: 7.2755 over 113997 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:23<00:00, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: training loss: 7.2622 over 113802 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:21<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: training loss: 7.2387 over 114032 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:23<00:00, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: training loss: 7.2327 over 113798 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:21<00:00, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: training loss: 7.2269 over 114035 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:23<00:00, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: training loss: 7.2080 over 114118 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:23<00:00, 18.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: training loss: 7.1785 over 113504 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: training loss: 7.1598 over 113692 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: training loss: 7.1359 over 113819 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 20.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: training loss: 7.1132 over 113774 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:21<00:00, 20.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: training loss: 7.0920 over 113847 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:21<00:00, 20.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: training loss: 7.0859 over 114080 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:21<00:00, 20.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: training loss: 7.0721 over 114171 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: training loss: 7.0620 over 113710 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: training loss: 7.0382 over 113743 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:23<00:00, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: training loss: 6.9990 over 113556 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 20.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: training loss: 6.9990 over 113453 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:21<00:00, 20.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: training loss: 6.9706 over 113886 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: training loss: 6.9559 over 113712 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 19.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: training loss: 6.9539 over 113614 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:23<00:00, 18.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: training loss: 6.9340 over 114094 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 19.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: training loss: 6.9223 over 113420 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: training loss: 6.9060 over 113914 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:24<00:00, 18.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: training loss: 6.9014 over 114024 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:24<00:00, 18.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: training loss: 6.8978 over 113768 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: training loss: 6.8741 over 113686 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:21<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: training loss: 6.8541 over 113669 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:22<00:00, 20.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: training loss: 6.8698 over 113942 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:21<00:00, 20.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: training loss: 6.8516 over 113476 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 20.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: training loss: 6.8389 over 113561 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 19.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: training loss: 6.8280 over 113523 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: training loss: 6.8158 over 113731 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 447/447 [00:22<00:00, 19.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: training loss: 6.7931 over 114181 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:21<00:00, 20.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: training loss: 6.7950 over 113870 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:21<00:00, 20.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: training loss: 6.7869 over 113929 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 20.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: training loss: 6.7724 over 113691 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 20.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: training loss: 6.7817 over 113687 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: training loss: 6.7644 over 113886 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443/443 [00:21<00:00, 20.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: training loss: 6.7640 over 113359 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443/443 [00:22<00:00, 19.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: training loss: 6.7528 over 113390 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: training loss: 6.7506 over 113904 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:22<00:00, 19.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: training loss: 6.7427 over 114020 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:21<00:00, 20.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: training loss: 6.7462 over 113423 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:22<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: training loss: 6.7408 over 113934 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:21<00:00, 20.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: training loss: 6.7324 over 113607 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:21<00:00, 20.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: training loss: 6.7120 over 113924 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:21<00:00, 20.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: training loss: 6.7232 over 113815 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 19.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: training loss: 6.7105 over 113760 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:21<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: training loss: 6.7083 over 113470 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:21<00:00, 20.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: training loss: 6.7191 over 113717 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 20.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: training loss: 6.7022 over 113822 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 20.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: training loss: 6.6988 over 113577 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 447/447 [00:22<00:00, 20.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: training loss: 6.6903 over 114266 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 20.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: training loss: 6.6928 over 113549 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 20.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: training loss: 6.7012 over 113622 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:21<00:00, 20.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: training loss: 6.6875 over 113527 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 448/448 [00:22<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: training loss: 6.6833 over 114506 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:21<00:00, 20.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: training loss: 6.6760 over 112984 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 20.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: training loss: 6.6736 over 113560 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 443/443 [00:21<00:00, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: training loss: 6.6801 over 113375 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:21<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: training loss: 6.6531 over 113818 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:21<00:00, 20.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: training loss: 6.6563 over 113490 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:21<00:00, 20.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: training loss: 6.6592 over 113517 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446/446 [00:21<00:00, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: training loss: 6.6681 over 114048 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:22<00:00, 20.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: training loss: 6.6693 over 113737 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444/444 [00:22<00:00, 20.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: training loss: 6.6702 over 113569 training points.\n",
      "Training loss is not reducing anymore, terminate.\n",
      "Training finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load initial weights\n",
    "word2vec.load_state_dict(torch.load(save_path))\n",
    "word2vec = word2vec.to(device)\n",
    "\n",
    "# train model\n",
    "list_loss = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_dataset = W2VDataset(sentences)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    losses = 0.\n",
    "    cnt = 0\n",
    "    word2vec.train()\n",
    "    for words, context_words in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = word2vec(words.to(device))\n",
    "        loss = criterion(pred, context_words.squeeze(1).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "        cnt += len(words)\n",
    "\n",
    "    epoch_loss = losses / cnt\n",
    "    print(f'Epoch {epoch:2}: training loss: {epoch_loss:.4f} over {cnt} training points.')\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        # save embedding\n",
    "        embedding_weights = word2vec.embed.state_dict()\n",
    "        embedding_weights['weight']\n",
    "        torch.save(embedding_weights, f'./save/embedding_checkpoints/embedding_weights_{epoch}_epoch_{embedding_size}_dim_{window_size}_wsize.pt')\n",
    "        # save full model\n",
    "        torch.save(word2vec.state_dict(), f'./save/model_checkpoints/word2vec_{epoch}_epoch_{embedding_size}_dim_{window_size}_wsize.pt')\n",
    "    \n",
    "    # early-stop when training loss is not decreasing.\n",
    "    list_loss.append(epoch_loss)\n",
    "    if len(list_loss) > 5 and min(list_loss[-5:]) > min(list_loss[:-5]):\n",
    "        print('Training loss is not reducing anymore, terminate.')\n",
    "        break\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgQkaYstyj0Q"
   },
   "source": [
    "# 1.10 Train on the full dataset (0.5 points)\n",
    "\n",
    "Now, go back to 1.1 and remove the restriction on the number of sentences in your corpus. Then, reexecute code blocks 1.2, 1.3 and 1.6 (or those relevant if you created additional ones). \n",
    "\n",
    "* Then, retrain your model on the complete dataset.\n",
    "\n",
    "* Now, the input weights of the model contain the desired word embeddings! Save them together with the corresponding vocabulary items (Pytorch provides a nice [functionality](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the word-embedding layer weights\n",
    "embedding_weights = word2vec.embed.state_dict()\n",
    "torch.save(embedding_weights, f'save/embedding_weights_.pt')\n",
    "\n",
    "# save dicts for transformation word <-> int\n",
    "with open(f'save/word_to_int_dict.json', 'w') as f:\n",
    "    json.dump(word_to_int, f)\n",
    "with open(f'save/int_to_word_dict.json', 'w') as f:\n",
    "    json.dump(int_to_word, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "- We keep the hash-tag words.\n",
    "- We make use of the nn.Embedding layer from torch instead of using word_to_one_hot() function.\n",
    "- We use Adam instead of AdaGrad as stated in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNTI_final_project_task_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
