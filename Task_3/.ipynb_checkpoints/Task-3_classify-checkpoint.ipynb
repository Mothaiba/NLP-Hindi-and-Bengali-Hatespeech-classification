{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used the trained embeddings to classify hate-speeches\n",
    "This notebook creates a neural classifier.\n",
    "\n",
    "### Input:\n",
    "    - Word-embeddings.\n",
    "    - Training data.\n",
    "\n",
    "### Output:\n",
    "    - A binary classifier.\n",
    "    - Evaluation on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(26)\n",
    "np.random.seed(62)\n",
    "# torch.manual_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 15983\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "train_df = pd.read_csv('save/bengali_train_preprocessed.csv')\n",
    "train_sentences = [[int(s) for s in text.split()] for text in train_df['sentence']]\n",
    "train_labels = train_df['hate'].to_numpy()\n",
    "\n",
    "# test data\n",
    "test_df = pd.read_csv('save/bengali_test_preprocessed.csv')\n",
    "test_sentences = [[int(s) for s in text.split()] for text in test_df['sentence']]\n",
    "test_labels = test_df['hate'].to_numpy()\n",
    "\n",
    "# word <-> convertion\n",
    "with open('save/word_to_int_dict.json', 'r') as f:\n",
    "    word_to_int = json.load(f)\n",
    "with open('save/int_to_word_dict.json', 'r') as f:\n",
    "    int_to_word = json.load(f)\n",
    "    int_to_word = {int(k) : v for k, v in int_to_word.items()}\n",
    "\n",
    "# word-counter\n",
    "with open('save/word_counter.json', 'r') as f:\n",
    "    word_counter = json.load(f)\n",
    "    \n",
    "vocab_size = len(word_to_int)\n",
    "print('vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample data:\n",
      "train:\n",
      "[[12431, 11321, 507, 13590, 993, 8990, 7341, 7078], [9604, 9604]]\n",
      "[0 0]\n",
      "test:\n",
      "[[5664, 10661, 3793, 6014], [7078, 7570, 3439, 15021]]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "print('sample data:')\n",
    "print('train:')\n",
    "print(train_sentences[:2])\n",
    "print(train_labels[:2])\n",
    "print('test:')\n",
    "print(test_sentences[:2])\n",
    "print(test_labels[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_path = 'save/embedding_weights_30_epoch_300_dim_5_wsize_10_negfac.pt'\n",
    "embedding_path = 'save/embedding_weights_20_epoch_300_dim_5_wsize_GloVe.pt'\n",
    "embedding_size = 300\n",
    "att_dim = 150\n",
    "# lstm_dim = 100\n",
    "# lstm_bidirectional = True\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOFDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.data = []\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            if len(sentence):\n",
    "                self.data.append(\n",
    "                    (torch.tensor(sentence, dtype=torch.long), \n",
    "                     torch.tensor(label, dtype=torch.float))\n",
    "                )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "def preprocess_batch(batch):\n",
    "    texts, labels = list(zip(*batch))\n",
    "    seq_lens = torch.tensor([len(text) for text in texts], dtype=torch.long)\n",
    "    texts = pad_sequence(texts, padding_value=0)\n",
    "    labels = torch.tensor(labels).unsqueeze(1)\n",
    "\n",
    "    seq_lens, sorted_idx = seq_lens.sort(descending=True)\n",
    "    texts = texts[:,sorted_idx]\n",
    "    labels = labels[sorted_idx]\n",
    "    return texts, seq_lens, labels\n",
    "\n",
    "train_dataset = HOFDataset(train_sentences, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True, collate_fn=preprocess_batch)\n",
    "\n",
    "test_dataset = HOFDataset(test_sentences, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         shuffle=False, collate_fn=preprocess_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_seq(seq_lens):\n",
    "    mask = torch.zeros((len(seq_lens), max(seq_lens))).bool()\n",
    "    for i, seq_len in enumerate(seq_lens):\n",
    "        mask[i, seq_len:] = True\n",
    "    return mask\n",
    "\n",
    "class Classifier(Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "#         self.lstm_dim = lstm_dim\n",
    "#         self.bidirectional = bidirectional\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embed.load_state_dict(torch.load(embedding_path, map_location=torch.device(device)))\n",
    "        self.embed.requires_grad = False\n",
    "        # self.lstm = nn.LSTM(input_size=embedding_size, \n",
    "        #                     hidden_size=lstm_dim, \n",
    "        #                     num_layers=2, \n",
    "        #                     batch_first=False, \n",
    "        #                     dropout=0.5, \n",
    "        #                     bidirectional=bidirectional)\n",
    "        \n",
    "        # self.fc = nn.Linear(lstm_dim*2 if bidirectional else lstm_dim, 1)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(#embed_dim=lstm_dim*2 if bidirectional else lstm_dim,\n",
    "                                               embed_dim=embedding_size,\n",
    "                                               num_heads=10,\n",
    "                                               dropout=0.5,) # need to add mask for padding positions\n",
    "\n",
    "        self.fc = nn.Linear(embedding_size, 1)\n",
    "#         init_range = (2 / (embedding_size + 1)) ** 0.5 # Xavier\n",
    "#         self.fc.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, inp, seq_lens):\n",
    "        out = self.embed(inp)\n",
    "        # out = pack_padded_sequence(out, seq_lens, batch_first=False, \n",
    "                                #    enforce_sorted = True)\n",
    "        # out, _ = self.lstm(out)\n",
    "        # out, _ = pad_packed_sequence(out, batch_first=False)\n",
    "\n",
    "        pad_mask = mask_seq(seq_lens)\n",
    "        att_out, _ = self.attention(out, out, out)#, key_padding_mask=pad_mask)\n",
    "        out = F.layer_norm(out + att_out, (out.size(2), ))\n",
    "\n",
    "        out = self.fc(out)\n",
    "        out = out.squeeze(2)\n",
    "        pred = torch.zeros((out.size(1), 1))\n",
    "        for i, seq_len in enumerate(seq_lens):\n",
    "            pred[i, 0] = out[:seq_len, i].mean()\n",
    "        return pred\n",
    "\n",
    "clf = Classifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(clf.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test():\n",
    "    losses = 0\n",
    "    acc_cnt = 0\n",
    "    cnt = 0\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    clf.eval()\n",
    "    for texts, seq_lens, labels in test_loader:\n",
    "        pred = clf(texts.to(device), seq_lens).detach().to('cpu')\n",
    "        loss = criterion(pred, labels)\n",
    "        losses += loss.detach().item() * len(texts)\n",
    "        acc_cnt += sum((pred > 0) == (labels > 0)).item()\n",
    "        preds.extend(pred.view(-1))\n",
    "        true_labels.extend(labels.view(-1))\n",
    "        cnt += texts.size(1)\n",
    "    \n",
    "    preds = np.array(preds) > 0\n",
    "    macro_f1 = f1_score(true_labels, preds, average='macro')\n",
    "    weighted_f1 = f1_score(true_labels, preds, average='weighted')\n",
    "    return losses / cnt, acc_cnt / cnt, macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:15<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Train loss: 1.4464, acc: 0.5647. Test loss: 0.8853, acc: 0.6399, macro_f1: 0.6363, weighted_f1: 0.6389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:18<00:00,  8.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: Train loss: 1.3760, acc: 0.6439. Test loss: 0.8406, acc: 0.6321, macro_f1: 0.6162, weighted_f1: 0.6105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: Train loss: 1.1737, acc: 0.7250. Test loss: 0.7223, acc: 0.7566, macro_f1: 0.7565, weighted_f1: 0.7568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: Train loss: 1.0742, acc: 0.7782. Test loss: 0.6817, acc: 0.7689, macro_f1: 0.7689, weighted_f1: 0.7691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00, 10.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: Train loss: 0.9392, acc: 0.8230. Test loss: 0.6564, acc: 0.7805, macro_f1: 0.7791, weighted_f1: 0.7804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: Train loss: 0.8355, acc: 0.8479. Test loss: 0.6864, acc: 0.7736, macro_f1: 0.7736, weighted_f1: 0.7736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:13<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: Train loss: 0.7106, acc: 0.8781. Test loss: 0.7068, acc: 0.7790, macro_f1: 0.7757, weighted_f1: 0.7777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:13<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: Train loss: 0.6072, acc: 0.8970. Test loss: 0.7164, acc: 0.7828, macro_f1: 0.7816, weighted_f1: 0.7828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:13<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: Train loss: 0.5446, acc: 0.9095. Test loss: 0.7596, acc: 0.7767, macro_f1: 0.7757, weighted_f1: 0.7768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:13<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train loss: 0.4535, acc: 0.9279. Test loss: 0.8037, acc: 0.7658, macro_f1: 0.7657, weighted_f1: 0.7661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train loss: 0.4101, acc: 0.9429. Test loss: 0.8817, acc: 0.7612, macro_f1: 0.7612, weighted_f1: 0.7614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:15<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train loss: 0.3508, acc: 0.9511. Test loss: 0.9076, acc: 0.7628, macro_f1: 0.7624, weighted_f1: 0.7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train loss: 0.2928, acc: 0.9607. Test loss: 0.9571, acc: 0.7566, macro_f1: 0.7560, weighted_f1: 0.7569\n",
      "Early stopping: test accuracy does not increase after 5 epochs\n"
     ]
    }
   ],
   "source": [
    "list_test_acc = []\n",
    "early_stop = 5\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    losses = 0.\n",
    "    acc_cnt = 0\n",
    "    cnt = 0\n",
    "    clf.train()\n",
    "    for texts, seq_lens, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = clf(texts.to(device), seq_lens)\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item() * len(texts)\n",
    "        acc_cnt += sum((pred.to('cpu') > 0) == (labels > 0)).item()\n",
    "        cnt += texts.size(1)\n",
    "\n",
    "    epoch_loss = losses / cnt\n",
    "    epoch_acc = acc_cnt / cnt\n",
    "    test_loss, test_acc, test_macro_f1, test_weighted_f1 = predict_test()\n",
    "    print(f'Epoch {epoch:2}: Train loss: {epoch_loss:.4f}, acc: {epoch_acc:.4f}. '\n",
    "        f'Test loss: {test_loss:.4f}, acc: {test_acc:.4f}, '\n",
    "        f'macro_f1: {test_macro_f1:.4f}, weighted_f1: {test_weighted_f1:.4f}',\n",
    "        flush=True)\n",
    "\n",
    "    list_test_acc.append(test_acc)\n",
    "    if len(list_test_acc) > early_stop and max(list_test_acc[-early_stop:]) <= max(list_test_acc[:-early_stop]):\n",
    "        print(f'Early stopping: test accuracy does not increase after {early_stop} epochs')\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
