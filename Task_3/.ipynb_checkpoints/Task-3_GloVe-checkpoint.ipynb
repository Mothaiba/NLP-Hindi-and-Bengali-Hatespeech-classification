{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make embeddings for Bengali language\n",
    "This notebook handles the embedding process.\n",
    "\n",
    "### Input:\n",
    "    - Pre-processed training dataframe.\n",
    "\n",
    "### Output:\n",
    "    - The trained weights of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f30b0316f90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.init import xavier_normal_\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(26)\n",
    "np.random.seed(62)\n",
    "torch.manual_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_df = pd.read_csv('save/bengali_train_preprocessed.csv')\n",
    "train_sentences = [[int(s) for s in text.split()] for text in train_df['sentence']]\n",
    "train_labels = train_df['hate'].to_numpy()\n",
    "\n",
    "# test data\n",
    "test_df = pd.read_csv('save/bengali_test_preprocessed.csv')\n",
    "test_sentences = [[int(s) for s in text.split()] for text in test_df['sentence']]\n",
    "test_labels = train_df['hate'].to_numpy()\n",
    "\n",
    "# word <-> convertion\n",
    "with open('save/word_to_int_dict.json', 'r') as f:\n",
    "    word_to_int = json.load(f)\n",
    "with open('save/int_to_word_dict.json', 'r') as f:\n",
    "    int_to_word = json.load(f)\n",
    "    int_to_word = {int(k) : v for k, v in int_to_word.items()}\n",
    "\n",
    "# word-counter\n",
    "with open('save/word_counter.json', 'r') as f:\n",
    "    word_counter = json.load(f)\n",
    "    \n",
    "vocab_size = len(word_to_int)\n",
    "total_words = sum(word_counter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'save/bengali_GloVe.pt'\n",
    "\n",
    "window_size = 5\n",
    "embedding_size = 300\n",
    "weighting_alpha = 3/4\n",
    "max_cooc = 25\n",
    "learning_rate = 0.02\n",
    "lr_decay = lambda epoch: max(0.05, 0.8**epoch)\n",
    "batch_size = 256\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurence = np.zeros((total_words, total_words))\n",
    "for sentence in train_sentences:\n",
    "    for i, word in enumerate(sentence):\n",
    "        for j, context_word in enumerate(sentence[i-window_size:i+window_size+1]):\n",
    "            if i != j:\n",
    "                cooccurence[word, context_word] += 1\n",
    "                cooccurence[context_word, word] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train word-embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of GloVe(\n",
       "  (center_embed): Embedding(15983, 300)\n",
       "  (context_embed): Embedding(15983, 300)\n",
       "  (center_bias): Embedding(15983, 1)\n",
       "  (context_bias): Embedding(15983, 1)\n",
       ")>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class GloVe(Module):\n",
    "    def __init__(self, cooccurence_matrix, weighting_alpha, max_cooc):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.cooccurence_matrix = cooccurence_matrix\n",
    "        self.alpha = weighting_alpha\n",
    "        self.max_cooc = max_cooc\n",
    "        \n",
    "        self.center_embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.center_embed.weight = xavier_normal_(self.center_embed.weight)\n",
    "        \n",
    "        self.context_embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.context_embed.weight = xavier_normal_(self.context_embed.weight)\n",
    "\n",
    "        self.center_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.center_bias.weight = xavier_normal_(self.center_bias.weight)\n",
    "\n",
    "        self.context_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.context_bias.weight = xavier_normal_(self.context_bias.weight)\n",
    "\n",
    "    def forward(self, center_ids, context_ids):\n",
    "        batch_size = len(center_ids)\n",
    "\n",
    "        cooc_counts = np.array([self.cooccurence_matrix[w1, w2] + 1 for w1, w2 in zip(center_ids, context_ids)])\n",
    "        fX = (cooc_counts.clip(max=self.max_cooc) / self.max_cooc) ** self.alpha\n",
    "\n",
    "        cooc_counts = torch.from_numpy(cooc_counts).to(device)\n",
    "        fX = torch.from_numpy(fX).to(device)\n",
    "\n",
    "        center_embed = self.center_embed(center_ids)\n",
    "        center_bias = self.center_bias(center_ids)\n",
    "        context_embed = self.context_embed(context_ids)\n",
    "        context_bias = self.context_bias(context_ids)\n",
    "\n",
    "        return (fX * torch.pow(\n",
    "            ((center_embed * context_embed).sum(1) + center_bias + context_bias).squeeze(1) - torch.log(cooc_counts), 2\n",
    "        )).sum()\n",
    "    \n",
    "    def to_embed(self, center_id):\n",
    "        return self.center_embed(center_id)\n",
    "    \n",
    "word2vec = GloVe(cooccurence, weighting_alpha, max_cooc)\n",
    "torch.save(word2vec.state_dict(), model_save_path)\n",
    "\n",
    "display(word2vec.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Learning-rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(word2vec.parameters(), lr=learning_rate)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VDataset(Dataset):\n",
    "    def __init__(self, co_occurrence):\n",
    "        self.data = [(i, j) for i, j in np.argwhere(co_occurrence > 0)]\n",
    "#         for i in range(total_words):\n",
    "#             for j in range(total_words):\n",
    "#                 if co_occurrence[i, j] > 0:\n",
    "#                     self.data.append((i, j))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "train_dataset = W2VDataset(cooccurence)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.57it/s]\n",
      "  0%|          | 4/1152 [00:00<00:34, 33.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: training loss: 2155.1007 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.44it/s]\n",
      "  0%|          | 4/1152 [00:00<00:35, 32.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: training loss: 23834.2383 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.36it/s]\n",
      "  0%|          | 4/1152 [00:00<00:35, 32.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: training loss: 11014.8421 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.38it/s]\n",
      "  0%|          | 4/1152 [00:00<00:35, 32.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: training loss: 1407.1752 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.26it/s]\n",
      "  0%|          | 4/1152 [00:00<00:35, 32.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: training loss: 238.9333 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.30it/s]\n",
      "  0%|          | 4/1152 [00:00<00:35, 32.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: training loss: 121.6824 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:36<00:00, 31.98it/s]\n",
      "  0%|          | 4/1152 [00:00<00:34, 33.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: training loss: 137.4192 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.30it/s]\n",
      "  0%|          | 4/1152 [00:00<00:34, 33.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: training loss: 216.7639 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.33it/s]\n",
      "  0%|          | 4/1152 [00:00<00:34, 33.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: training loss: 290.6060 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.31it/s]\n",
      "  0%|          | 4/1152 [00:00<00:36, 31.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: training loss: 216.1969 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.16it/s]\n",
      "  0%|          | 4/1152 [00:00<00:35, 32.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: training loss: 89.8237 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.28it/s]\n",
      "  0%|          | 4/1152 [00:00<00:36, 31.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: training loss: 37.3532 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.26it/s]\n",
      "  0%|          | 4/1152 [00:00<00:34, 33.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: training loss: 24.1251 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.30it/s]\n",
      "  0%|          | 3/1152 [00:00<00:39, 29.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: training loss: 20.5832 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:34<00:00, 33.27it/s]\n",
      "  0%|          | 4/1152 [00:00<00:33, 34.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: training loss: 17.1792 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.55it/s]\n",
      "  0%|          | 4/1152 [00:00<00:35, 32.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: training loss: 18.3076 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.20it/s]\n",
      "  0%|          | 4/1152 [00:00<00:35, 32.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: training loss: 23.1825 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:35<00:00, 32.09it/s]\n",
      "  0%|          | 3/1152 [00:00<00:39, 28.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: training loss: 23.8008 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:36<00:00, 31.22it/s]\n",
      "  0%|          | 4/1152 [00:00<00:36, 31.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: training loss: 20.9828 over 294868 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [00:36<00:00, 31.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: training loss: 19.6005 over 294868 training points.\n",
      "Early stopping: training loss does not decrease after 5 epochs\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# load initial weights\n",
    "word2vec.load_state_dict(torch.load(model_save_path, map_location=torch.device(device)))\n",
    "word2vec = word2vec.to(device)\n",
    "\n",
    "early_stop = 5\n",
    "history_losses = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    \n",
    "    losses = 0.\n",
    "    cnt = 0\n",
    "    \n",
    "    word2vec.train()\n",
    "    for center_words, context_words in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = word2vec(center_words.to(device), context_words.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss\n",
    "        cnt += len(center_words)\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = losses / cnt\n",
    "    print(f'Epoch {epoch:2}: training loss: {epoch_loss:.4f} over {cnt} training points.')\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        # save embedding\n",
    "        embedding_weights = word2vec.center_embed.state_dict()\n",
    "        embedding_weights['weight']\n",
    "        torch.save(embedding_weights, f'save/embedding_weights_{epoch}_epoch_{embedding_size}_dim_{window_size}_wsize_GloVe.pt')\n",
    "    \n",
    "    history_losses.append(epoch_loss)\n",
    "    if len(history_losses) > early_stop and min(history_losses[-early_stop:]) >= min(history_losses[:-early_stop]):\n",
    "        print(f'Early stopping: training loss does not decrease after {early_stop} epochs')\n",
    "        break\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
