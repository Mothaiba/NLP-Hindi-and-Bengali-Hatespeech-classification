{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make embeddings for Bengali language\n",
    "This notebook handles the embedding process.\n",
    "\n",
    "### Input:\n",
    "    - Pre-processed training dataframe.\n",
    "\n",
    "### Output:\n",
    "    - The trained weights of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3697c7cf90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(26)\n",
    "np.random.seed(62)\n",
    "torch.manual_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_df = pd.read_csv('save/bengali_train_preprocessed.csv')\n",
    "train_sentences = [[int(s) for s in text.split()] for text in train_df['sentence']]\n",
    "train_labels = train_df['hate'].to_numpy()\n",
    "\n",
    "# test data\n",
    "test_df = pd.read_csv('save/bengali_test_preprocessed.csv')\n",
    "test_sentences = [[int(s) for s in text.split()] for text in test_df['sentence']]\n",
    "test_labels = train_df['hate'].to_numpy()\n",
    "\n",
    "# word <-> convertion\n",
    "with open('save/word_to_int_dict.json', 'r') as f:\n",
    "    word_to_int = json.load(f)\n",
    "with open('save/int_to_word_dict.json', 'r') as f:\n",
    "    int_to_word = json.load(f)\n",
    "    int_to_word = {int(k) : v for k, v in int_to_word.items()}\n",
    "\n",
    "# word-counter\n",
    "with open('save/word_counter.json', 'r') as f:\n",
    "    word_counter = json.load(f)\n",
    "    \n",
    "vocab_size = len(word_to_int)\n",
    "total_words = sum(word_counter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'save/bengali_word2vec_neg.pt'\n",
    "\n",
    "window_size = 5\n",
    "embedding_size = 300\n",
    "neg_sample_factor = 10\n",
    "noise_dist_alpha = 3/4\n",
    "learning_rate = 0.02\n",
    "lr_decay = lambda epoch: max(0.05, 0.8**epoch)\n",
    "batch_size = 256\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling probability of pair (center, context)\n",
    "def sampling_prob(word):\n",
    "    z = word_counter[word] / total_words\n",
    "    p_keep = ((z/0.001)**0.5 + 1) * (0.001/z)\n",
    "    return p_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise distribution\n",
    "noisy_words = [iw for iw in int_to_word]\n",
    "noisy_dist = np.array([(word_counter[int_to_word[iw]]/total_words)**noise_dist_alpha for iw in noisy_words])\n",
    "noisy_dist = noisy_dist / noisy_dist.sum()\n",
    "\n",
    "# noisy word generator\n",
    "def get_noise_word(batch_size, neg_factor):\n",
    "    noise_list = np.random.choice(noisy_words, batch_size*neg_factor, p=noisy_dist)\n",
    "    noise_list = noise_list.reshape((batch_size, neg_factor))\n",
    "    return torch.from_numpy(noise_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_context(sentence: list(str())):\n",
    "    for i, word in enumerate(sentence):\n",
    "        for j, context_word in enumerate(sentence[i-window_size:i+window_size+1]):\n",
    "            if j != i and random.random() < sampling_prob(int_to_word[context_word]):\n",
    "                    yield (torch.tensor(word, dtype=torch.long), \n",
    "                           torch.tensor(context_word, dtype=torch.long)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train word-embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Word2Vec(\n",
       "  (center_embed): Embedding(15983, 300)\n",
       "  (context_embed): Embedding(15983, 300)\n",
       "  (log_sigmoid): LogSigmoid()\n",
       ")>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Word2Vec(Module):\n",
    "    def __init__(self):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.center_embed = nn.Embedding(vocab_size, embedding_size)        \n",
    "        self.context_embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        init_range = (2 / (vocab_size + embedding_size)) ** 0.5\n",
    "        self.center_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        self.context_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        \n",
    "        self.log_sigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, center_ids, context_ids, negative_samples):\n",
    "        # center_ids, context_ids: [batch_size]\n",
    "        # negatve_samples: [batch_size, neg_sample_factor]\n",
    "        \n",
    "        # center_embed, context_embed: [batch_size, embedding_size]\n",
    "        center_embed = self.center_embed(center_ids)\n",
    "        context_embed = self.context_embed(context_ids)\n",
    "        \n",
    "        # pos_dot: [batch_size]\n",
    "        pos_dot = (center_embed * context_embed).sum(axis=1)\n",
    "        \n",
    "        # pos_loss: [batch_size]\n",
    "        pos_loss = self.log_sigmoid(pos_dot)\n",
    "        \n",
    "        # negative_embed: [batch_size, neg_sample_factor, embedding_size]\n",
    "        negative_embed = self.context_embed(negative_samples)\n",
    "        \n",
    "        # negs_dot: [batch_size, neg_sample_factor]\n",
    "        negs_dot = torch.bmm(negative_embed, center_embed.unsqueeze(2)).squeeze(2) * (-1)\n",
    "        \n",
    "        # neg_dot: [batch_size]\n",
    "        neg_dot = negs_dot.sum(axis=1)\n",
    "        \n",
    "        # neg_loss: [batch_size]\n",
    "        neg_loss = self.log_sigmoid(neg_dot)\n",
    "        \n",
    "        loss = -(pos_loss + neg_loss).sum()\n",
    "        return loss, -pos_loss.sum(), -neg_loss.sum()\n",
    "    \n",
    "    def to_embed(self, center_id):\n",
    "        return self.center_embed(center_id)\n",
    "    \n",
    "word2vec = Word2Vec()\n",
    "torch.save(word2vec.state_dict(), model_save_path)\n",
    "\n",
    "display(word2vec.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Learning-rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(word2vec.parameters(), lr=learning_rate)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.data = []\n",
    "        for sentence in sentences:\n",
    "            for data_point in get_target_context(sentence):\n",
    "                self.data.append(data_point)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: training loss: 2.7034 (pos: 1.8798, neg: 0.0824) over 304879 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: training loss: 1.4383 (pos: 0.9493, neg: 0.0489) over 304872 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: training loss: 0.5686 (pos: 0.3518, neg: 0.0217) over 304778 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: training loss: 0.2789 (pos: 0.1735, neg: 0.0105) over 304707 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: training loss: 0.1573 (pos: 0.0926, neg: 0.0065) over 304866 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: training loss: 0.0956 (pos: 0.0545, neg: 0.0041) over 304753 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: training loss: 0.0631 (pos: 0.0340, neg: 0.0029) over 304848 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: training loss: 0.0429 (pos: 0.0208, neg: 0.0022) over 304856 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1192/1192 [00:33<00:00, 35.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: training loss: 0.0273 (pos: 0.0107, neg: 0.0017) over 304937 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: training loss: 0.0200 (pos: 0.0077, neg: 0.0012) over 304896 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1192/1192 [00:33<00:00, 35.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: training loss: 0.0149 (pos: 0.0048, neg: 0.0010) over 304902 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: training loss: 0.0121 (pos: 0.0032, neg: 0.0009) over 304852 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: training loss: 0.0109 (pos: 0.0024, neg: 0.0008) over 304827 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: training loss: 0.0079 (pos: 0.0018, neg: 0.0006) over 304841 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: training loss: 0.0092 (pos: 0.0013, neg: 0.0008) over 304843 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: training loss: 0.0076 (pos: 0.0016, neg: 0.0006) over 304821 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: training loss: 0.0074 (pos: 0.0012, neg: 0.0006) over 304775 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1192/1192 [00:33<00:00, 35.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: training loss: 0.0084 (pos: 0.0014, neg: 0.0007) over 304961 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: training loss: 0.0078 (pos: 0.0012, neg: 0.0007) over 304715 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1192/1192 [00:33<00:00, 35.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: training loss: 0.0074 (pos: 0.0012, neg: 0.0006) over 304927 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: training loss: 0.0072 (pos: 0.0011, neg: 0.0006) over 304841 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: training loss: 0.0079 (pos: 0.0011, neg: 0.0007) over 304659 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1192/1192 [00:33<00:00, 35.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: training loss: 0.0070 (pos: 0.0012, neg: 0.0006) over 304923 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1192/1192 [00:33<00:00, 35.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: training loss: 0.0088 (pos: 0.0012, neg: 0.0008) over 304923 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1192/1192 [00:33<00:00, 35.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: training loss: 0.0075 (pos: 0.0011, neg: 0.0006) over 304957 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: training loss: 0.0085 (pos: 0.0011, neg: 0.0007) over 304798 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: training loss: 0.0078 (pos: 0.0012, neg: 0.0007) over 304780 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [00:33<00:00, 35.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: training loss: 0.0077 (pos: 0.0011, neg: 0.0007) over 304743 training points.\n",
      "Early stopping: training loss does not decrease after 5 epochs\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# load initial weights\n",
    "word2vec.load_state_dict(torch.load(model_save_path, map_location=torch.device(device)))\n",
    "word2vec = word2vec.to(device)\n",
    "\n",
    "early_stop = 5\n",
    "history_losses = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_dataset = W2VDataset(train_sentences)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    losses, pos_losses, neg_losses = 0., 0., 0.\n",
    "    cnt = 0\n",
    "    \n",
    "    word2vec.train()\n",
    "    for center_words, context_words in tqdm(train_loader):\n",
    "        negative_samples = get_noise_word(len(center_words), neg_sample_factor)\n",
    "        optimizer.zero_grad()\n",
    "        loss, pos_loss, neg_loss = word2vec(center_words.to(device), context_words.to(device), negative_samples.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss\n",
    "        cnt += len(center_words)\n",
    "        pos_losses += pos_loss\n",
    "        neg_losses += neg_loss\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = losses / cnt\n",
    "    print(f'Epoch {epoch:2}: training loss: {epoch_loss:.4f} (pos: {pos_losses/cnt:.4f}, neg: {neg_losses/(cnt*neg_sample_factor):.4f}) over {cnt} training points.')\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        # save embedding\n",
    "        embedding_weights = word2vec.center_embed.state_dict()\n",
    "        embedding_weights['weight']\n",
    "        torch.save(embedding_weights, f'save/embedding_weights_{epoch}_epoch_{embedding_size}_dim_{window_size}_wsize_{neg_sample_factor}_negfac.pt')\n",
    "    \n",
    "    history_losses.append(epoch_loss)\n",
    "    if len(history_losses) > early_stop and min(history_losses[-early_stop:]) >= min(history_losses[:-early_stop]):\n",
    "        print(f'Early stopping: training loss does not decrease after {early_stop} epochs')\n",
    "        break\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
