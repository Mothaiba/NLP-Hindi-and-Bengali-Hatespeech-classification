{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make embeddings for Bengali language\n",
    "This notebook handles the embedding process.\n",
    "\n",
    "### Input:\n",
    "    - Pre-processed training dataframe.\n",
    "\n",
    "### Output:\n",
    "    - The trained weights of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2b48382f70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(26)\n",
    "np.random.seed(62)\n",
    "torch.manual_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_df = pd.read_csv('save/bengali_train_preprocessed.csv')\n",
    "train_sentences = [[int(s) for s in text.split()] for text in train_df['sentence']]\n",
    "train_labels = train_df['hate'].to_numpy()\n",
    "\n",
    "# test data\n",
    "test_df = pd.read_csv('save/bengali_test_preprocessed.csv')\n",
    "test_sentences = [[int(s) for s in text.split()] for text in test_df['sentence']]\n",
    "test_labels = train_df['hate'].to_numpy()\n",
    "\n",
    "# word <-> convertion\n",
    "with open('save/word_to_int_dict.json', 'r') as f:\n",
    "    word_to_int = json.load(f)\n",
    "with open('save/int_to_word_dict.json', 'r') as f:\n",
    "    int_to_word = json.load(f)\n",
    "    int_to_word = {int(k) : v for k, v in int_to_word.items()}\n",
    "\n",
    "# word-counter\n",
    "with open('save/word_counter.json', 'r') as f:\n",
    "    word_counter = json.load(f)\n",
    "    \n",
    "vocab_size = len(word_to_int)\n",
    "total_words = sum(word_counter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'save/bengali_word2vec_neg.pt'\n",
    "\n",
    "window_size = 5\n",
    "embedding_size = 300\n",
    "neg_sample_factor = 10\n",
    "noise_dist_alpha = 3/4\n",
    "learning_rate = 0.02\n",
    "lr_decay = lambda epoch: max(0.05, 0.9**epoch)\n",
    "batch_size = 256\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling probability of pair (center, context)\n",
    "# def sampling_prob(word):\n",
    "#     z = word_counter[word] / total_words\n",
    "#     p_keep = ((z/0.000005)**0.5 + 1) * (0.000005/z)\n",
    "#     return p_keep\n",
    "\n",
    "def sampling_prob(word):\n",
    "    z = word_counter[word]\n",
    "    return 1. / (z ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise distribution\n",
    "noisy_words = [iw for iw in int_to_word]\n",
    "noisy_dist = np.array([(word_counter[int_to_word[iw]]/total_words)**noise_dist_alpha for iw in noisy_words])\n",
    "noisy_dist = noisy_dist / noisy_dist.sum()\n",
    "\n",
    "# noisy word generator\n",
    "def get_noise_word(batch_size, neg_factor):\n",
    "    noise_list = np.random.choice(noisy_words, batch_size*neg_factor, p=noisy_dist)\n",
    "    noise_list = noise_list.reshape((batch_size, neg_factor))\n",
    "    return torch.from_numpy(noise_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_context(sentence: list(str())):\n",
    "    for i, word in enumerate(sentence):\n",
    "        for j, context_word in enumerate(sentence[i-window_size:i+window_size+1]):\n",
    "            if j != i and random.random() < sampling_prob(int_to_word[context_word]):\n",
    "                    yield (torch.tensor(word, dtype=torch.long), \n",
    "                           torch.tensor(context_word, dtype=torch.long)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train word-embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Word2Vec(\n",
       "  (center_embed): Embedding(15983, 300)\n",
       "  (context_embed): Embedding(15983, 300)\n",
       "  (log_sigmoid): LogSigmoid()\n",
       ")>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Word2Vec(Module):\n",
    "    def __init__(self):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.center_embed = nn.Embedding(vocab_size, embedding_size)        \n",
    "        self.context_embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        init_range = (2 / (vocab_size + embedding_size)) ** 0.5\n",
    "        self.center_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        self.context_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        \n",
    "        self.log_sigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, center_ids, context_ids, negative_samples):\n",
    "        # center_ids, context_ids: [batch_size]\n",
    "        # negatve_samples: [batch_size, neg_sample_factor]\n",
    "        \n",
    "        # center_embed, context_embed: [batch_size, embedding_size]\n",
    "        center_embed = self.center_embed(center_ids)\n",
    "        context_embed = self.context_embed(context_ids)\n",
    "        \n",
    "        # pos_dot: [batch_size]\n",
    "        pos_dot = (center_embed * context_embed).sum(axis=1)\n",
    "        \n",
    "        # pos_loss: [batch_size]\n",
    "        pos_loss = self.log_sigmoid(pos_dot)\n",
    "        \n",
    "        # negative_embed: [batch_size, neg_sample_factor, embedding_size]\n",
    "        negative_embed = self.context_embed(negative_samples)\n",
    "        \n",
    "        # negs_dot: [batch_size, neg_sample_factor]\n",
    "        negs_dot = torch.bmm(negative_embed, center_embed.unsqueeze(2)).squeeze(2) * (-1)\n",
    "        \n",
    "        # neg_dot: [batch_size]\n",
    "        neg_dot = negs_dot.sum(axis=1)\n",
    "        \n",
    "        # neg_loss: [batch_size]\n",
    "        neg_loss = self.log_sigmoid(neg_dot)\n",
    "        \n",
    "        loss = -(pos_loss + neg_loss).sum()\n",
    "        return loss, -pos_loss.sum(), -neg_loss.sum()\n",
    "    \n",
    "    def to_embed(self, center_id):\n",
    "        return self.center_embed(center_id)\n",
    "    \n",
    "word2vec = Word2Vec()\n",
    "torch.save(word2vec.state_dict(), model_save_path)\n",
    "\n",
    "display(word2vec.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Learning-rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(word2vec.parameters(), lr=learning_rate)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.data = []\n",
    "        for sentence in sentences:\n",
    "            for data_point in get_target_context(sentence):\n",
    "                self.data.append(data_point)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:17<00:00, 33.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: training loss: 1.9029 (pos: 1.3237, neg: 0.0579) over 147076 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 34.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: training loss: 1.9085 (pos: 1.2469, neg: 0.0662) over 147101 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 34.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: training loss: 1.4240 (pos: 0.9547, neg: 0.0469) over 147169 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 35.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: training loss: 1.0501 (pos: 0.7215, neg: 0.0329) over 146987 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:16<00:00, 35.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: training loss: 0.7672 (pos: 0.5416, neg: 0.0226) over 146900 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:16<00:00, 35.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: training loss: 0.6063 (pos: 0.4248, neg: 0.0181) over 147293 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:16<00:00, 35.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: training loss: 0.4753 (pos: 0.3374, neg: 0.0138) over 146755 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:16<00:00, 35.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: training loss: 0.3835 (pos: 0.2675, neg: 0.0116) over 147362 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:16<00:00, 35.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: training loss: 0.2920 (pos: 0.2074, neg: 0.0085) over 146936 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 35.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: training loss: 0.2390 (pos: 0.1673, neg: 0.0072) over 146985 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 35.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: training loss: 0.1966 (pos: 0.1359, neg: 0.0061) over 147022 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [00:17<00:00, 33.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: training loss: 0.1608 (pos: 0.1091, neg: 0.0052) over 146617 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:17<00:00, 32.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: training loss: 0.1343 (pos: 0.0932, neg: 0.0041) over 147277 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:17<00:00, 33.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: training loss: 0.1068 (pos: 0.0708, neg: 0.0036) over 147218 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 35.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: training loss: 0.0977 (pos: 0.0626, neg: 0.0035) over 147105 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [00:16<00:00, 35.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: training loss: 0.0732 (pos: 0.0492, neg: 0.0024) over 146686 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:16<00:00, 35.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: training loss: 0.0686 (pos: 0.0439, neg: 0.0025) over 147224 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 34.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: training loss: 0.0573 (pos: 0.0346, neg: 0.0023) over 147091 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:16<00:00, 34.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: training loss: 0.0533 (pos: 0.0327, neg: 0.0021) over 147218 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [00:16<00:00, 34.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: training loss: 0.0449 (pos: 0.0264, neg: 0.0019) over 146671 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 34.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: training loss: 0.0387 (pos: 0.0234, neg: 0.0015) over 147129 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:16<00:00, 34.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: training loss: 0.0356 (pos: 0.0202, neg: 0.0015) over 146705 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 34.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: training loss: 0.0314 (pos: 0.0176, neg: 0.0014) over 147101 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 34.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: training loss: 0.0302 (pos: 0.0155, neg: 0.0015) over 147162 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:16<00:00, 34.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: training loss: 0.0247 (pos: 0.0140, neg: 0.0011) over 146889 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577/577 [00:16<00:00, 34.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: training loss: 0.0223 (pos: 0.0120, neg: 0.0010) over 147553 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 34.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: training loss: 0.0187 (pos: 0.0094, neg: 0.0009) over 146972 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:16<00:00, 35.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: training loss: 0.0199 (pos: 0.0084, neg: 0.0011) over 147214 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [00:16<00:00, 35.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: training loss: 0.0173 (pos: 0.0085, neg: 0.0009) over 146491 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:16<00:00, 35.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: training loss: 0.0162 (pos: 0.0074, neg: 0.0009) over 146759 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 35.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: training loss: 0.0161 (pos: 0.0074, neg: 0.0009) over 146955 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 35.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: training loss: 0.0173 (pos: 0.0074, neg: 0.0010) over 147144 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:16<00:00, 35.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: training loss: 0.0131 (pos: 0.0060, neg: 0.0007) over 147236 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 35.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: training loss: 0.0131 (pos: 0.0056, neg: 0.0008) over 146993 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:16<00:00, 35.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: training loss: 0.0130 (pos: 0.0050, neg: 0.0008) over 147313 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:16<00:00, 35.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: training loss: 0.0116 (pos: 0.0049, neg: 0.0007) over 146774 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 35.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: training loss: 0.0116 (pos: 0.0042, neg: 0.0007) over 147094 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [00:16<00:00, 35.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: training loss: 0.0108 (pos: 0.0042, neg: 0.0007) over 146614 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:16<00:00, 35.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: training loss: 0.0108 (pos: 0.0043, neg: 0.0007) over 146907 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 35.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: training loss: 0.0095 (pos: 0.0038, neg: 0.0006) over 147049 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [00:16<00:00, 35.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: training loss: 0.0097 (pos: 0.0039, neg: 0.0006) over 146622 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 35.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: training loss: 0.0101 (pos: 0.0033, neg: 0.0007) over 147174 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:16<00:00, 34.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: training loss: 0.0103 (pos: 0.0035, neg: 0.0007) over 147049 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:16<00:00, 34.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: training loss: 0.0105 (pos: 0.0030, neg: 0.0007) over 147245 training points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576/576 [00:16<00:00, 34.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: training loss: 0.0101 (pos: 0.0028, neg: 0.0007) over 147313 training points.\n",
      "Early stopping: training loss does not decrease after 5 epochs\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# load initial weights\n",
    "word2vec.load_state_dict(torch.load(model_save_path, map_location=torch.device(device)))\n",
    "word2vec = word2vec.to(device)\n",
    "\n",
    "early_stop = 5\n",
    "history_losses = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_dataset = W2VDataset(train_sentences)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    losses, pos_losses, neg_losses = 0., 0., 0.\n",
    "    cnt = 0\n",
    "    \n",
    "    word2vec.train()\n",
    "    for center_words, context_words in tqdm(train_loader):\n",
    "        negative_samples = get_noise_word(len(center_words), neg_sample_factor)\n",
    "        optimizer.zero_grad()\n",
    "        loss, pos_loss, neg_loss = word2vec(center_words.to(device), context_words.to(device), negative_samples.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss\n",
    "        cnt += len(center_words)\n",
    "        pos_losses += pos_loss\n",
    "        neg_losses += neg_loss\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = losses / cnt\n",
    "    print(f'Epoch {epoch:2}: training loss: {epoch_loss:.4f} (pos: {pos_losses/cnt:.4f}, neg: {neg_losses/(cnt*neg_sample_factor):.4f}) over {cnt} training points.')\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        # save embedding\n",
    "        embedding_weights = word2vec.center_embed.state_dict()\n",
    "        embedding_weights['weight']\n",
    "        torch.save(embedding_weights, f'save/embedding_weights_{epoch}_epoch_{embedding_size}_dim_{window_size}_wsize_{neg_sample_factor}_negfac.pt')\n",
    "    \n",
    "    history_losses.append(epoch_loss)\n",
    "    if len(history_losses) > early_stop and min(history_losses[-early_stop:]) >= min(history_losses[:-early_stop]):\n",
    "        print(f'Early stopping: training loss does not decrease after {early_stop} epochs')\n",
    "        break\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
