{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used the trained embeddings to classify hate-speeches\n",
    "This notebook creates a neural classifier.\n",
    "\n",
    "### Input:\n",
    "    - Word-embeddings.\n",
    "    - Training data.\n",
    "\n",
    "### Output:\n",
    "    - A binary classifier.\n",
    "    - Evaluation on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(26)\n",
    "np.random.seed(62)\n",
    "torch.manual_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_df = pd.read_csv('save/hindi_train_preprocessed.csv')\n",
    "train_sentences = [[int(s) for s in text.split()] for text in train_df['sentence']]\n",
    "train_df['hate'] = (train_df['task_1'] == 'HOF').astype(int)\n",
    "train_labels = train_df['hate'].to_numpy()\n",
    "\n",
    "# test data\n",
    "test_df = pd.read_csv('save/hindi_test_preprocessed.csv')\n",
    "test_sentences = [[int(s) for s in text.split()] for text in test_df['sentence']]\n",
    "test_df['hate'] = (test_df['task_1'] == 'HOF').astype(int)\n",
    "test_labels = test_df['hate'].to_numpy()\n",
    "\n",
    "# word <-> convertion\n",
    "with open('save/hindi_word_to_int_dict.json', 'r') as f:\n",
    "    word_to_int = json.load(f)\n",
    "with open('save/hindi_int_to_word_dict.json', 'r') as f:\n",
    "    int_to_word = json.load(f)\n",
    "    int_to_word = {int(k) : v for k, v in int_to_word.items()}\n",
    "\n",
    "# word-counter\n",
    "with open('save/hindi_word_counter.json', 'r') as f:\n",
    "    word_counter = json.load(f)\n",
    "    \n",
    "vocab_size = len(word_to_int)\n",
    "print('vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sample data:')\n",
    "print('train:')\n",
    "print(train_sentences[:2])\n",
    "print(train_labels[:2])\n",
    "print('test:')\n",
    "print(test_sentences[:2])\n",
    "print(test_labels[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = 'save/hindi_embedding_weights_10_negfac.pt'\n",
    "# embedding_path = 'save/hindi_embedding_weights_3_wsize_10_negfac.pt'\n",
    "embedding_size = 300\n",
    "att_dim = 150\n",
    "# lstm_dim = 100\n",
    "# lstm_bidirectional = True\n",
    "learning_rate = 5e-5\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation by removing some random words from a sentence\n",
    "augmentation_rate = 0. # value range: [0, 1]. Set to 0. to disable augmentation.\n",
    "sample_prop = (0.7, 0.95)\n",
    "\n",
    "class HOFDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, do_aug):\n",
    "        self.data = []\n",
    "        if do_aug:\n",
    "            augmentation = np.random.uniform(low=0, high=1, size=len(sentences)) < augmentation_rate\n",
    "        else:\n",
    "            augmentation = np.zeros(len(sentences))\n",
    "        subsample = np.random.uniform(low=sample_prop[0], high=sample_prop[1], size=len(sentences))\n",
    "        for sentence, label, aug, sub in zip(sentences, labels, augmentation, subsample):\n",
    "            if len(sentence):\n",
    "                if aug:\n",
    "                    length = len(sentence)\n",
    "                    selected_indices = np.random.choice(range(length), math.ceil(length*sub), replace=False)\n",
    "                    selected_indices.sort()\n",
    "                    aug_sentence = torch.tensor(sentence, dtype=torch.long)[selected_indices]\n",
    "                    aug_label = 0.5 + (label - 0.5)*sub\n",
    "                    self.data.append(\n",
    "                        (aug_sentence, \n",
    "                         torch.tensor(aug_label, dtype=torch.float))\n",
    "                    )\n",
    "                else:\n",
    "                    self.data.append(\n",
    "                        (torch.tensor(sentence, dtype=torch.long), \n",
    "                         torch.tensor(label, dtype=torch.float))\n",
    "                    )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "def preprocess_batch(batch):\n",
    "    texts, labels = list(zip(*batch))\n",
    "    seq_lens = torch.tensor([len(text) for text in texts], dtype=torch.long)\n",
    "    texts = pad_sequence(texts, padding_value=0)\n",
    "    labels = torch.tensor(labels).unsqueeze(1)\n",
    "\n",
    "    seq_lens, sorted_idx = seq_lens.sort(descending=True)\n",
    "    texts = texts[:,sorted_idx]\n",
    "    labels = labels[sorted_idx]\n",
    "    return texts, seq_lens, labels\n",
    "\n",
    "train_dataset = HOFDataset(train_sentences, train_labels, do_aug=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True, collate_fn=preprocess_batch,\n",
    "                          num_workers=2)\n",
    "\n",
    "test_dataset = HOFDataset(test_sentences, test_labels, do_aug=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         shuffle=False, collate_fn=preprocess_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "def get_pe(p, i):\n",
    "    k = i // 2\n",
    "    if i % 2 == 0:\n",
    "        return np.sin(p/(10000**(2*k/embedding_size)))\n",
    "    else:\n",
    "        return np.cos(p/(10000**(2*k/embedding_size)))\n",
    "\n",
    "pe = np.zeros((embedding_size, max_len))\n",
    "for p in range(max_len):\n",
    "    for i in range(embedding_size):\n",
    "        pe[i, p] = get_pe(p, i)\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.imshow(pe.T[:,:512])\n",
    "ax.set_title('Positional encoding')\n",
    "ax.set_xlabel('dimension')\n",
    "ax.set_ylabel('position')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "pe = pe.T # for easier addition in the future operations\n",
    "pe = torch.tensor(pe, requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_seq(seq_lens):\n",
    "    mask = torch.zeros((len(seq_lens), max(seq_lens))).bool()\n",
    "    for i, seq_len in enumerate(seq_lens):\n",
    "        mask[i, seq_len:] = True\n",
    "    return mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, batch_first, norm_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.norm_dim = norm_dim\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim,\n",
    "                                         num_heads=num_heads,\n",
    "                                         dropout=dropout,)\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc.weight = xavier_uniform_(self.fc.weight)\n",
    "    \n",
    "    def forward(self, inp, pad_mask):\n",
    "        # make batch second\n",
    "        if self.batch_first:\n",
    "            inp = inp.permute(1, 0, 2)        \n",
    "        \n",
    "        # inp, att_out: [seq dim, batch dim, embed dim]\n",
    "        att_out, _ = self.att(inp, inp, inp, key_padding_mask=pad_mask)\n",
    "        \n",
    "        # out, fc_out: [batch dim, seq dim, embed dim]\n",
    "        out = (att_out + inp).permute(1, 0, 2)\n",
    "        out = F.layer_norm(out, out.shape[self.norm_dim:])\n",
    "        out = F.layer_norm(self.fc(out) + out, out.shape[self.norm_dim:])\n",
    "        \n",
    "        # restore the original shape\n",
    "        if not self.batch_first:\n",
    "            out = out.permute(1, 0, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embed.load_state_dict(torch.load(embedding_path, map_location=torch.device(device)))\n",
    "        self.embed.requires_grad = False\n",
    "        \n",
    "        self.encoder_1 = Encoder(\n",
    "            embed_dim=embedding_size,\n",
    "            num_heads=10,\n",
    "            dropout=0.7,\n",
    "            batch_first=False,\n",
    "            norm_dim=1,\n",
    "        )\n",
    "        self.encoder_2 = Encoder(\n",
    "            embed_dim=embedding_size,\n",
    "            num_heads=5,\n",
    "            dropout=0.7,\n",
    "            batch_first=False,\n",
    "            norm_dim=1,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, inp, seq_lens):\n",
    "        # add positional encoding\n",
    "        # out: [seq dim, batch dim, embed dim]\n",
    "        out = self.embed(inp)\n",
    "        for i in range(out.shape[1]):\n",
    "            out[:, i, :] += pe[:out.shape[0], :]\n",
    "        # padding mask\n",
    "        pad_mask = mask_seq(seq_lens)\n",
    "        # encoder\n",
    "        out = self.encoder_1(out, pad_mask)\n",
    "        out = self.encoder_2(out, pad_mask)\n",
    "        # predict\n",
    "        out = self.fc(out)\n",
    "        out = out.squeeze(2)\n",
    "        pred = torch.zeros((out.size(1), 1))\n",
    "        for i, seq_len in enumerate(seq_lens):\n",
    "            pred[i, 0] = out[:seq_len, i].mean()\n",
    "        return pred\n",
    "\n",
    "clf = Classifier().to(device)\n",
    "torch.save(clf.state_dict(), 'save/hindi_hindi_initial.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(clf.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test():\n",
    "    losses = 0\n",
    "    acc_cnt = 0\n",
    "    cnt = 0\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    clf.eval()\n",
    "    for texts, seq_lens, labels in test_loader:\n",
    "        pred = clf(texts.to(device), seq_lens).detach().to('cpu')\n",
    "        loss = criterion(pred, labels)\n",
    "        losses += loss.detach().item() * len(texts)\n",
    "        acc_cnt += sum((pred > 0) == (labels > 0)).item()\n",
    "        preds.extend(pred.view(-1))\n",
    "        true_labels.extend(labels.view(-1))\n",
    "        cnt += texts.size(1)\n",
    "    \n",
    "    preds = np.array(preds) > 0\n",
    "    macro_f1 = f1_score(true_labels, preds, average='macro')\n",
    "    weighted_f1 = f1_score(true_labels, preds, average='weighted')\n",
    "    return losses / cnt, acc_cnt / cnt, macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.load_state_dict(torch.load('save/hindi_hindi_initial.pt'))\n",
    "\n",
    "list_test_acc = []\n",
    "early_stop = 3\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    losses = 0.\n",
    "    acc_cnt = 0\n",
    "    cnt = 0\n",
    "    clf.train()\n",
    "    for texts, seq_lens, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = clf(texts.to(device), seq_lens)\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item() * len(texts)\n",
    "        acc_cnt += sum((pred.to('cpu') > 0) == (labels > 0)).item()\n",
    "        cnt += texts.size(1)\n",
    "\n",
    "    epoch_loss = losses / cnt\n",
    "    epoch_acc = acc_cnt / cnt\n",
    "    test_loss, test_acc, test_macro_f1, test_weighted_f1 = predict_test()\n",
    "    print(f'Epoch {epoch:2}: Train loss: {epoch_loss:.4f}, acc: {epoch_acc:.4f}. '\n",
    "        f'Test loss: {test_loss:.4f}, acc: {test_acc:.4f}, '\n",
    "        f'macro_f1: {test_macro_f1:.4f}, weighted_f1: {test_weighted_f1:.4f}',\n",
    "        flush=True)\n",
    "\n",
    "    list_test_acc.append(test_acc)\n",
    "    if len(list_test_acc) > early_stop and max(list_test_acc[-early_stop:]) <= max(list_test_acc[:-early_stop]):\n",
    "        print(f'Early stopping: test accuracy does not increase after {early_stop} epochs')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final model\n",
    "torch.save(clf.state_dict(), 'save/hindi_hindi_final_clf.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    # restore model to untrained\n",
    "    clf.load_state_dict(torch.load('save/hindi_hindi_initial.pt'))\n",
    "    \n",
    "    # store test accuracy and macro-f1\n",
    "    list_test_acc = []\n",
    "    list_test_macro_f1 = []\n",
    "    \n",
    "    # train\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        losses = 0.\n",
    "        acc_cnt = 0\n",
    "        cnt = 0\n",
    "        clf.train()\n",
    "        for texts, seq_lens, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = clf(texts.to(device), seq_lens)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss.detach().item() * len(texts)\n",
    "            acc_cnt += sum((pred.to('cpu') > 0) == (labels > 0)).item()\n",
    "            cnt += texts.size(1)\n",
    "\n",
    "        epoch_loss = losses / cnt\n",
    "        epoch_acc = acc_cnt / cnt\n",
    "        test_loss, test_acc, test_macro_f1, test_weighted_f1 = predict_test()\n",
    "\n",
    "        list_test_acc.append(test_acc)\n",
    "        list_test_macro_f1.append(test_macro_f1)\n",
    "        \n",
    "    return list_test_acc, list_test_macro_f1\n",
    "\n",
    "def batch_train(ntime, epochs):\n",
    "    list_test_acc = []\n",
    "    list_test_macro_f1 = []\n",
    "    for i in range(ntime):\n",
    "        acc, macro_f1 = train(epochs=epochs)\n",
    "        list_test_acc.append(acc)\n",
    "        list_test_macro_f1.append(macro_f1)\n",
    "        print(f'Trained {i+1} time(s)')\n",
    "    \n",
    "    return list_test_acc, list_test_macro_f1\n",
    "\n",
    "def plot_results(list_test_acc, list_test_macro_f1):\n",
    "    # accuracy plot\n",
    "    mean_acc = [np.mean([l[i] for l in list_test_acc if len(l) > i]) for i in range(max(len(li) for li in list_test_acc))]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for i, acc_curve in enumerate(list_test_acc):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(len(acc_curve))), \n",
    "            y=acc_curve,\n",
    "            mode='lines',\n",
    "            name=f'model {i+1}',\n",
    "            opacity=0.3,\n",
    "        ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(mean_acc))), \n",
    "        y=mean_acc,\n",
    "        mode='lines+markers',\n",
    "        line_color='blue',\n",
    "        name='mean',\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Accuracy plot',\n",
    "        xaxis_title='Epoch',\n",
    "        yaxis_title='Accuracy'\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # macro-f1 plot\n",
    "    mean_mac = [np.mean([l[i] for l in list_test_macro_f1 if len(l) > i]) for i in range(max(len(li) for li in list_test_macro_f1))]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for i, mac_curve in enumerate(list_test_macro_f1):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(len(mac_curve))), \n",
    "            y=mac_curve,\n",
    "            mode='lines',\n",
    "            name=f'model {i+1}',\n",
    "            opacity=0.3,\n",
    "        ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(mean_mac))), \n",
    "        y=mean_mac,\n",
    "        mode='lines+markers',\n",
    "        line_color='blue',\n",
    "        name='mean',\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Macro-f1 plot',\n",
    "        xaxis_title='Epoch',\n",
    "        yaxis_title='Macro-f1'\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_acc, list_test_macro_f1 = batch_train(ntime=5, epochs=15)\n",
    "plot_results(list_test_acc, list_test_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
