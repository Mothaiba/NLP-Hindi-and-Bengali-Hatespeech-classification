{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Task-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZg0W-yXwrDF"
      },
      "source": [
        "# Task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVcdFAthwrDK"
      },
      "source": [
        "# Imports\n",
        "import re\n",
        "import string\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(26)\n",
        "np.random.seed(62)\n",
        "torch.manual_seed(2021)\n",
        "\n",
        "LANGUAGE = 'hi'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaxXEoGMw8kh"
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# uploaded = files.upload()\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qagn4RHowrDK"
      },
      "source": [
        "embedding_path = 'embedding_weights_hi_30_epoch_100_dim_10_wsize.pt'\n",
        "embedding_size = 100\n",
        "lstm_dim = 100\n",
        "lstm_bidirectional = True\n",
        "batch_size = 8 \n",
        "epochs = 10"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFhMmHFzwrDL"
      },
      "source": [
        "## Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "u6Mn4tS7wrDL",
        "outputId": "124fd8a6-7986-4427-ec54-9304bbee3060"
      },
      "source": [
        "train_data = pd.read_csv('hindi_dataset.tsv', sep='\\t')\n",
        "print('train:')\n",
        "display(train_data.head())\n",
        "\n",
        "train_sentences = train_data['text'].to_numpy()\n",
        "train_labels = train_data['task_1'].to_numpy()\n",
        "train_labels[train_labels=='NOT'] = 0\n",
        "train_labels[train_labels=='HOF'] = 1\n",
        "train_labels = train_labels.astype(int)\n",
        "\n",
        "test_data = pd.read_csv('hasoc2019_hi_test_gold_2919.tsv', sep='\\t')\n",
        "print('test:')\n",
        "display(test_data.head())\n",
        "\n",
        "test_sentences = test_data['text'].to_numpy()\n",
        "test_labels = test_data['task_1'].to_numpy()\n",
        "test_labels[test_labels=='NOT'] = 0\n",
        "test_labels[test_labels=='HOF'] = 1\n",
        "test_labels = test_labels.astype(int)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "      <th>task_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hasoc_hi_5556</td>\n",
              "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hasoc_hi_5648</td>\n",
              "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>UNT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hasoc_hi_164</td>\n",
              "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hasoc_hi_3530</td>\n",
              "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hasoc_hi_5206</td>\n",
              "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         text_id  ... task_3\n",
              "0  hasoc_hi_5556  ...   NONE\n",
              "1  hasoc_hi_5648  ...    UNT\n",
              "2   hasoc_hi_164  ...    TIN\n",
              "3  hasoc_hi_3530  ...   NONE\n",
              "4  hasoc_hi_5206  ...   NONE\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "test:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "      <th>task_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hasoc_hi_5061</td>\n",
              "      <td>वक्त, इन्सान और इंग्लैंड का मौसम आपको कभी भी ध...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hasoc_hi_2090</td>\n",
              "      <td>#कांग्रेस के इस #कमीने की #करतूत को देखिए देश ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hasoc_hi_2960</td>\n",
              "      <td>पाकिस्तान को फेकना था फेका गया। जो हार कर भी द...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "      <td>TIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hasoc_hi_864</td>\n",
              "      <td>जो शब्द तूम आज किसी और औरत के लिए यूज कर रहे व...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hasoc_hi_54</td>\n",
              "      <td>नेता जी हम समाजवादी सिपाही हमेशा आपके साथ है आ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         text_id  ... task_3\n",
              "0  hasoc_hi_5061  ...   NONE\n",
              "1  hasoc_hi_2090  ...    TIN\n",
              "2  hasoc_hi_2960  ...    TIN\n",
              "3   hasoc_hi_864  ...   NONE\n",
              "4    hasoc_hi_54  ...   NONE\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGdN_i0wwrDM"
      },
      "source": [
        "def preprocess_texts(sentences):\n",
        "    # remove user taggings\n",
        "    user_tag_pattern = re.compile(r'\\@\\w*')\n",
        "    sentences = [re.sub(user_tag_pattern, ' ', sentence) for sentence in sentences]\n",
        "    # lower case\n",
        "    sentences = [sentence.lower() for sentence in sentences]\n",
        "    # remove punctuations\n",
        "    http_re = re.compile('http://[^ ]*')\n",
        "    https_re = re.compile('https://[^ ]*')\n",
        "    punctuation = string.punctuation[:2] + string.punctuation[3:]\n",
        "    translator = str.maketrans(punctuation, ' '*len(punctuation))\n",
        "    def clean(s):\n",
        "        s = re.sub(http_re, ' ', s)\n",
        "        s = re.sub(https_re, ' ', s)\n",
        "        s = s.translate(translator)\n",
        "        return s\n",
        "\n",
        "    sentences = [clean(sentence) for sentence in sentences]\n",
        "    # remove number ?\n",
        "    \n",
        "    # remove stopwords\n",
        "    if LANGUAGE == 'hi':\n",
        "        stopwords = ['अंदर', 'अत', 'अदि', 'अप', 'अपना', 'अपनि', 'अपनी', 'अपने', 'अभि', 'अभी', 'आदि', \n",
        "                     'आप', 'इंहिं', 'इंहें', 'इंहों', 'इतयादि', 'इत्यादि', 'इन', 'इनका', 'इन्हीं', 'इन्हें', 'इन्हों', \n",
        "                     'इस', 'इसका', 'इसकि', 'इसकी', 'इसके', 'इसमें', 'इसि', 'इसी', 'इसे', 'उंहिं', 'उंहें', \n",
        "                     'उंहों', 'उन', 'उनका', 'उनकि', 'उनकी', 'उनके', 'उनको', 'उन्हीं', 'उन्हें', 'उन्हों', 'उस', \n",
        "                     'उसके', 'उसि', 'उसी', 'उसे', 'एक', 'एवं', 'एस', 'एसे', 'ऐसे', 'ओर', 'और', 'कइ', \n",
        "                     'कई', 'कर', 'करता', 'करते', 'करना', 'करने', 'करें', 'कहते', 'कहा', 'का', 'काफि', \n",
        "                     'काफ़ी', 'कि', 'किंहें', 'किंहों', 'कितना', 'किन्हें', 'किन्हों', 'किया', 'किर', 'किस', \n",
        "                     'किसि', 'किसी', 'किसे', 'की', 'कुछ', 'कुल', 'के', 'को', 'कोइ', 'कोई', 'कोन', \n",
        "                     'कोनसा', 'कौन', 'कौनसा', 'गया', 'घर', 'जब', 'जहाँ', 'जहां', 'जा', 'जिंहें', 'जिंहों', \n",
        "                     'जितना', 'जिधर', 'जिन', 'जिन्हें', 'जिन्हों', 'जिस', 'जिसे', 'जीधर', 'जेसा', 'जेसे', \n",
        "                     'जैसा', 'जैसे', 'जो', 'तक', 'तब', 'तरह', 'तिंहें', 'तिंहों', 'तिन', 'तिन्हें', 'तिन्हों', \n",
        "                     'तिस', 'तिसे', 'तो', 'था', 'थि', 'थी', 'थे', 'दबारा', 'दवारा', 'दिया', 'दुसरा', 'दुसरे', \n",
        "                     'दूसरे', 'दो', 'द्वारा', 'न', 'नहिं', 'नहीं', 'ना', 'निचे', 'निहायत', 'नीचे', 'ने', 'पर', \n",
        "                     'पहले', 'पुरा', 'पूरा', 'पे', 'फिर', 'बनि', 'बनी', 'बहि', 'बही', 'बहुत', 'बाद', 'बाला', \n",
        "                     'बिलकुल', 'भि', 'भितर', 'भी', 'भीतर', 'मगर', 'मानो', 'मे', 'में', 'यदि', 'यह', 'यहाँ', \n",
        "                     'यहां', 'यहि', 'यही', 'या', 'यिह', 'ये', 'रखें', 'रवासा', 'रहा', 'रहे', 'ऱ्वासा', 'लिए', \n",
        "                     'लिये', 'लेकिन', 'व', 'वगेरह', 'वरग', 'वर्ग', 'वह', 'वहाँ', 'वहां', 'वहिं', 'वहीं', 'वाले', \n",
        "                     'वुह', 'वे', 'वग़ैरह', 'संग', 'सकता', 'सकते', 'सबसे', 'सभि', 'सभी', 'साथ', 'साबुत', \n",
        "                     'साभ', 'सारा', 'से', 'सो', 'हि', 'ही', 'हुअ', 'हुआ', 'हुइ', 'हुई', 'हुए', 'हे', 'हें', \n",
        "                     'है', 'हैं', 'हो', 'होता', 'होति', 'होती', 'होते', 'होना', 'होने']\n",
        "    elif LANGUAGE == 'en':\n",
        "        stopwords = stopwords.words('english')\n",
        "\n",
        "    sentences = [[word for word in sentence.split() if word not in stopwords] for sentence in sentences]\n",
        "    \n",
        "    return sentences\n",
        "\n",
        "train_sentences = preprocess_texts(train_sentences)\n",
        "test_sentences = preprocess_texts(test_sentences)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up5B_mspwrDN",
        "outputId": "f4f200e4-2b71-43e6-a95d-c890591be0b8"
      },
      "source": [
        "# vocab_size and word->id and id->word\n",
        "flattened_words = [word for sentence in train_sentences for word in sentence]\n",
        "V = list(set(flattened_words))\n",
        "vocab_size = len(V)\n",
        "print(f'vocab_size: {vocab_size}')\n",
        "\n",
        "word_to_int = {}\n",
        "int_to_word = {}\n",
        "for i, word in enumerate(V):\n",
        "    word_to_int[word] = i\n",
        "    int_to_word[i] = word"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size: 19580\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBUS5q-xwrDN",
        "outputId": "20d16035-49cd-4416-ce66-7c150f015c31"
      },
      "source": [
        "train_sentences = [[word_to_int[word] for word in sentence] for sentence in train_sentences]\n",
        "sq_len = np.array([len(s) for s in train_sentences])\n",
        "for id in np.where(sq_len == 0)[0][::-1]:\n",
        "    print('delete training text id', id)\n",
        "    del train_sentences[id], \n",
        "    np.delete(train_labels, id)\n",
        "del sq_len\n",
        "\n",
        "test_sentences = [[word_to_int[word] for word in sentence if word in word_to_int] for sentence in test_sentences]\n",
        "print('Number of empty test sentences: ', sum([len(s) == 0 for s in test_sentences]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "delete training text id 1375\n",
            "delete training text id 428\n",
            "Number of empty test sentences:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN0zGHnywrDN"
      },
      "source": [
        "## Build datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pleyeNY4wrDO"
      },
      "source": [
        "class HOFDataset(Dataset):\n",
        "    def __init__(self, sentences, labels):\n",
        "        self.data = []\n",
        "        for sentence, label in zip(sentences, labels):\n",
        "            self.data.append(\n",
        "                (torch.tensor(sentence, dtype=torch.long), \n",
        "                 torch.tensor(label, dtype=torch.float))\n",
        "            )\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "    \n",
        "def preprocess_batch(batch):\n",
        "    texts, labels = list(zip(*batch))\n",
        "    seq_lens = torch.tensor([len(text) for text in texts], dtype=torch.long)\n",
        "    texts = pad_sequence(texts, padding_value=0)\n",
        "    labels = torch.tensor(labels).unsqueeze(1)\n",
        "    return texts, seq_lens, labels\n",
        "\n",
        "train_dataset = HOFDataset(train_sentences, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
        "                          shuffle=True, collate_fn=preprocess_batch)\n",
        "\n",
        "test_dataset = HOFDataset(test_sentences, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
        "                         shuffle=False, collate_fn=preprocess_batch)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkJKUeW1wrDO"
      },
      "source": [
        "## Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlC1yLNSwrDO"
      },
      "source": [
        "class Classifier(Module):\n",
        "    def __init__(self, lstm_dim, bidirectional):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.embed.load_state_dict(torch.load(embedding_path))\n",
        "        self.embed.requires_grad = False\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=embedding_size, \n",
        "                            hidden_size=lstm_dim, \n",
        "                            num_layers=2, \n",
        "                            batch_first=False, \n",
        "                            dropout=0.5, \n",
        "                            bidirectional=lstm_bidirectional)\n",
        "        self.fc = nn.Linear(lstm_dim*2 if bidirectional else lstm_dim, 1)\n",
        "\n",
        "    def forward(self, inp, seq_lens):\n",
        "        # print('location 1: ', inp.size())\n",
        "        out = self.embed(inp)\n",
        "        out = pack_padded_sequence(out, seq_lens, batch_first=False, \n",
        "                                   enforce_sorted = False)\n",
        "        out, _ = self.lstm(out)\n",
        "        out, _ = pad_packed_sequence(out, batch_first=False)\n",
        "        out = self.fc(out)\n",
        "        out = out.squeeze(2)\n",
        "        pred = torch.zeros((out.size(1), 1))\n",
        "        for i, seq_len in enumerate(seq_lens):\n",
        "            pred[i, 0] = out[:seq_len, i].mean()\n",
        "        return pred\n",
        "\n",
        "clf = Classifier(lstm_dim=lstm_dim, bidirectional=lstm_bidirectional).to(device)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzEHt_X9wrDP"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(clf.parameters())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxNOHE3R5hzR"
      },
      "source": [
        "def predict_test():\n",
        "    losses = 0\n",
        "    acc_cnt = 0\n",
        "    cnt = 0\n",
        "    clf.eval()\n",
        "    for texts, seq_lens, labels in test_loader:\n",
        "        pred = clf(texts.to(device), seq_lens).detach().to('cpu')\n",
        "        loss = criterion(pred, labels)\n",
        "        losses += loss.detach().item() * len(texts)\n",
        "        acc_cnt += sum((pred > 0) == (labels > 0)).item()\n",
        "        cnt += texts.size(1)\n",
        "    return losses / cnt, acc_cnt / cnt"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32KkxNPwwrDP",
        "outputId": "b4476ee5-1dbe-48f0-ea0c-1502dd1898d0"
      },
      "source": [
        "list_test_acc = []\n",
        "early_stop = 10\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    losses = 0.\n",
        "    acc_cnt = 0\n",
        "    cnt = 0\n",
        "    clf.train()\n",
        "    for texts, seq_lens, labels in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        pred = clf(texts.to(device), seq_lens)\n",
        "        loss = criterion(pred, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses += loss.detach().item() * len(texts)\n",
        "        acc_cnt += sum((pred.to('cpu') > 0) == (labels > 0)).item()\n",
        "        cnt += texts.size(1)\n",
        "\n",
        "    epoch_loss = losses / cnt\n",
        "    epoch_acc = acc_cnt / cnt\n",
        "    test_loss, test_acc = predict_test()\n",
        "    print(f'Epoch {epoch:2}: Train loss: {epoch_loss:.4f}, acc: {epoch_acc:.4f}. Test loss: {test_loss:.4f}, test acc: {test_acc:.4f}')\n",
        "\n",
        "    list_test_acc.append(test_acc)\n",
        "    if len(list_test_acc) > early_stop and max(list_test_acc[-early_stop:]) <= max(list_test_acc[:-early_stop]):\n",
        "        print(f'Early stopping: test accuracy does not increase after {early_stop} epochs')\n",
        "        break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 583/583 [00:08<00:00, 71.07it/s]\n",
            "  1%|          | 7/583 [00:00<00:08, 66.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  1: Train loss: 2.9184, acc: 0.5344. Test loss: 2.7731, test acc: 0.5296\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 583/583 [00:08<00:00, 67.97it/s]\n",
            "  1%|          | 7/583 [00:00<00:08, 67.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  2: Train loss: 2.5863, acc: 0.6665. Test loss: 2.7063, test acc: 0.6282\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 583/583 [00:08<00:00, 66.40it/s]\n",
            "  1%|▏         | 8/583 [00:00<00:08, 70.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  3: Train loss: 1.6889, acc: 0.8310. Test loss: 3.2476, test acc: 0.6017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 583/583 [00:08<00:00, 70.92it/s]\n",
            "  1%|▏         | 8/583 [00:00<00:08, 71.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  4: Train loss: 0.8027, acc: 0.9294. Test loss: 5.4810, test acc: 0.5561\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 583/583 [00:08<00:00, 70.61it/s]\n",
            "  1%|▏         | 8/583 [00:00<00:07, 74.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  5: Train loss: 0.3967, acc: 0.9693. Test loss: 5.9455, test acc: 0.5524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 583/583 [00:08<00:00, 71.06it/s]\n",
            "  1%|▏         | 8/583 [00:00<00:08, 71.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  6: Train loss: 0.1987, acc: 0.9813. Test loss: 8.6022, test acc: 0.5448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 583/583 [00:08<00:00, 70.77it/s]\n",
            "  1%|▏         | 8/583 [00:00<00:07, 72.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  7: Train loss: 0.1558, acc: 0.9867. Test loss: 7.7586, test acc: 0.5668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 583/583 [00:08<00:00, 71.13it/s]\n",
            "  1%|          | 7/583 [00:00<00:08, 65.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  8: Train loss: 0.1093, acc: 0.9876. Test loss: 10.3641, test acc: 0.5486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 583/583 [00:08<00:00, 70.52it/s]\n",
            "  1%|▏         | 8/583 [00:00<00:08, 71.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  9: Train loss: 0.1080, acc: 0.9882. Test loss: 8.8594, test acc: 0.5622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 583/583 [00:08<00:00, 69.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10: Train loss: 0.1080, acc: 0.9884. Test loss: 10.2000, test acc: 0.5622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaL79Rtk7I4_",
        "outputId": "392b2df1-5255-446a-9bfd-3e2170d060e9"
      },
      "source": [
        "len(test_labels), sum(test_labels == 0) / len(test_labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1318, 0.5409711684370258)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebq1folZ8X7V"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}