{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZg0W-yXwrDF"
   },
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cVcdFAthwrDK"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(26)\n",
    "np.random.seed(62)\n",
    "torch.manual_seed(2021)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "LANGUAGE = 'hi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GaxXEoGMw8kh"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# uploaded = files.upload()\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qagn4RHowrDK"
   },
   "outputs": [],
   "source": [
    "embedding_path = '../Task_1/save/embedding_weights_hi_30_epoch_300_dim_5_wsize.pt'\n",
    "embedding_size = 300\n",
    "att_dim = 150\n",
    "# lstm_dim = 100\n",
    "# lstm_bidirectional = True\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFhMmHFzwrDL"
   },
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "u6Mn4tS7wrDL",
    "outputId": "41ed60b3-a09f-4f43-d1f2-dc8d79abbc97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_2</th>\n",
       "      <th>task_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasoc_hi_5556</td>\n",
       "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasoc_hi_5648</td>\n",
       "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>UNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasoc_hi_164</td>\n",
       "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasoc_hi_3530</td>\n",
       "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasoc_hi_5206</td>\n",
       "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text_id                                               text task_1  \\\n",
       "0  hasoc_hi_5556  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT   \n",
       "1  hasoc_hi_5648  सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF   \n",
       "2   hasoc_hi_164  तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF   \n",
       "3  hasoc_hi_3530  बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT   \n",
       "4  hasoc_hi_5206  चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT   \n",
       "\n",
       "  task_2 task_3  \n",
       "0   NONE   NONE  \n",
       "1   PRFN    UNT  \n",
       "2   PRFN    TIN  \n",
       "3   NONE   NONE  \n",
       "4   NONE   NONE  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_2</th>\n",
       "      <th>task_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasoc_hi_5061</td>\n",
       "      <td>वक्त, इन्सान और इंग्लैंड का मौसम आपको कभी भी ध...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasoc_hi_2090</td>\n",
       "      <td>#कांग्रेस के इस #कमीने की #करतूत को देखिए देश ...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasoc_hi_2960</td>\n",
       "      <td>पाकिस्तान को फेकना था फेका गया। जो हार कर भी द...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasoc_hi_864</td>\n",
       "      <td>जो शब्द तूम आज किसी और औरत के लिए यूज कर रहे व...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasoc_hi_54</td>\n",
       "      <td>नेता जी हम समाजवादी सिपाही हमेशा आपके साथ है आ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text_id                                               text task_1  \\\n",
       "0  hasoc_hi_5061  वक्त, इन्सान और इंग्लैंड का मौसम आपको कभी भी ध...    NOT   \n",
       "1  hasoc_hi_2090  #कांग्रेस के इस #कमीने की #करतूत को देखिए देश ...    HOF   \n",
       "2  hasoc_hi_2960  पाकिस्तान को फेकना था फेका गया। जो हार कर भी द...    HOF   \n",
       "3   hasoc_hi_864  जो शब्द तूम आज किसी और औरत के लिए यूज कर रहे व...    NOT   \n",
       "4    hasoc_hi_54  नेता जी हम समाजवादी सिपाही हमेशा आपके साथ है आ...    NOT   \n",
       "\n",
       "  task_2 task_3  \n",
       "0   NONE   NONE  \n",
       "1   OFFN    TIN  \n",
       "2   OFFN    TIN  \n",
       "3   NONE   NONE  \n",
       "4   NONE   NONE  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/hindi_dataset.tsv', sep='\\t')\n",
    "print('train:')\n",
    "display(train_data.head())\n",
    "\n",
    "train_sentences = train_data['text'].to_numpy()\n",
    "train_labels = train_data['task_1'].to_numpy()\n",
    "train_labels[train_labels=='NOT'] = 0\n",
    "train_labels[train_labels=='HOF'] = 1\n",
    "train_labels = train_labels.astype(int)\n",
    "\n",
    "test_data = pd.read_csv('../data/hasoc2019_hi_test_gold_2919.tsv', sep='\\t')\n",
    "print('test:')\n",
    "display(test_data.head())\n",
    "\n",
    "test_sentences = test_data['text'].to_numpy()\n",
    "test_labels = test_data['task_1'].to_numpy()\n",
    "test_labels[test_labels=='NOT'] = 0\n",
    "test_labels[test_labels=='HOF'] = 1\n",
    "test_labels = test_labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iGdN_i0wwrDM"
   },
   "outputs": [],
   "source": [
    "def preprocess_texts(sentences):\n",
    "    # remove user taggings\n",
    "    user_tag_pattern = re.compile(r'\\@\\w*')\n",
    "    sentences = [re.sub(user_tag_pattern, ' ', sentence) for sentence in sentences]\n",
    "    # lower case\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "    # remove punctuations\n",
    "    http_re = re.compile('http://[^ ]*')\n",
    "    https_re = re.compile('https://[^ ]*')\n",
    "    punctuation = string.punctuation[:2] + string.punctuation[3:]\n",
    "    translator = str.maketrans(punctuation, ' '*len(punctuation))\n",
    "    def clean(s):\n",
    "        s = re.sub(http_re, ' ', s)\n",
    "        s = re.sub(https_re, ' ', s)\n",
    "        s = s.translate(translator)\n",
    "        return s\n",
    "\n",
    "    sentences = [clean(sentence) for sentence in sentences]\n",
    "    # remove number ?\n",
    "    \n",
    "    # remove stopwords\n",
    "    if LANGUAGE == 'hi':\n",
    "        stopwords = ['अंदर', 'अत', 'अदि', 'अप', 'अपना', 'अपनि', 'अपनी', 'अपने', 'अभि', 'अभी', 'आदि', \n",
    "                     'आप', 'इंहिं', 'इंहें', 'इंहों', 'इतयादि', 'इत्यादि', 'इन', 'इनका', 'इन्हीं', 'इन्हें', 'इन्हों', \n",
    "                     'इस', 'इसका', 'इसकि', 'इसकी', 'इसके', 'इसमें', 'इसि', 'इसी', 'इसे', 'उंहिं', 'उंहें', \n",
    "                     'उंहों', 'उन', 'उनका', 'उनकि', 'उनकी', 'उनके', 'उनको', 'उन्हीं', 'उन्हें', 'उन्हों', 'उस', \n",
    "                     'उसके', 'उसि', 'उसी', 'उसे', 'एक', 'एवं', 'एस', 'एसे', 'ऐसे', 'ओर', 'और', 'कइ', \n",
    "                     'कई', 'कर', 'करता', 'करते', 'करना', 'करने', 'करें', 'कहते', 'कहा', 'का', 'काफि', \n",
    "                     'काफ़ी', 'कि', 'किंहें', 'किंहों', 'कितना', 'किन्हें', 'किन्हों', 'किया', 'किर', 'किस', \n",
    "                     'किसि', 'किसी', 'किसे', 'की', 'कुछ', 'कुल', 'के', 'को', 'कोइ', 'कोई', 'कोन', \n",
    "                     'कोनसा', 'कौन', 'कौनसा', 'गया', 'घर', 'जब', 'जहाँ', 'जहां', 'जा', 'जिंहें', 'जिंहों', \n",
    "                     'जितना', 'जिधर', 'जिन', 'जिन्हें', 'जिन्हों', 'जिस', 'जिसे', 'जीधर', 'जेसा', 'जेसे', \n",
    "                     'जैसा', 'जैसे', 'जो', 'तक', 'तब', 'तरह', 'तिंहें', 'तिंहों', 'तिन', 'तिन्हें', 'तिन्हों', \n",
    "                     'तिस', 'तिसे', 'तो', 'था', 'थि', 'थी', 'थे', 'दबारा', 'दवारा', 'दिया', 'दुसरा', 'दुसरे', \n",
    "                     'दूसरे', 'दो', 'द्वारा', 'न', 'नहिं', 'नहीं', 'ना', 'निचे', 'निहायत', 'नीचे', 'ने', 'पर', \n",
    "                     'पहले', 'पुरा', 'पूरा', 'पे', 'फिर', 'बनि', 'बनी', 'बहि', 'बही', 'बहुत', 'बाद', 'बाला', \n",
    "                     'बिलकुल', 'भि', 'भितर', 'भी', 'भीतर', 'मगर', 'मानो', 'मे', 'में', 'यदि', 'यह', 'यहाँ', \n",
    "                     'यहां', 'यहि', 'यही', 'या', 'यिह', 'ये', 'रखें', 'रवासा', 'रहा', 'रहे', 'ऱ्वासा', 'लिए', \n",
    "                     'लिये', 'लेकिन', 'व', 'वगेरह', 'वरग', 'वर्ग', 'वह', 'वहाँ', 'वहां', 'वहिं', 'वहीं', 'वाले', \n",
    "                     'वुह', 'वे', 'वग़ैरह', 'संग', 'सकता', 'सकते', 'सबसे', 'सभि', 'सभी', 'साथ', 'साबुत', \n",
    "                     'साभ', 'सारा', 'से', 'सो', 'हि', 'ही', 'हुअ', 'हुआ', 'हुइ', 'हुई', 'हुए', 'हे', 'हें', \n",
    "                     'है', 'हैं', 'हो', 'होता', 'होति', 'होती', 'होते', 'होना', 'होने']\n",
    "    elif LANGUAGE == 'en':\n",
    "        stopwords = stopwords.words('english')\n",
    "\n",
    "    sentences = [[word for word in sentence.split() if word not in stopwords] for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "train_sentences = preprocess_texts(train_sentences)\n",
    "test_sentences = preprocess_texts(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "up5B_mspwrDN",
    "outputId": "78f078de-d47c-4724-c3b8-cb1a08718cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 19580\n"
     ]
    }
   ],
   "source": [
    "# vocab_size and word->id and id->word\n",
    "flattened_words = [word for sentence in train_sentences for word in sentence]\n",
    "V = list(set(flattened_words))\n",
    "vocab_size = len(V)\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "\n",
    "word_to_int = {}\n",
    "int_to_word = {}\n",
    "for i, word in enumerate(V):\n",
    "    word_to_int[word] = i\n",
    "    int_to_word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBUS5q-xwrDN",
    "outputId": "1fe58717-9d55-47b0-de63-2e7f5684699a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete training text id 1375\n",
      "delete training text id 428\n",
      "Number of empty test sentences:  0\n"
     ]
    }
   ],
   "source": [
    "train_sentences = [[word_to_int[word] for word in sentence] for sentence in train_sentences]\n",
    "sq_len = np.array([len(s) for s in train_sentences])\n",
    "for id in np.where(sq_len == 0)[0][::-1]:\n",
    "    print('delete training text id', id)\n",
    "    del train_sentences[id], \n",
    "    np.delete(train_labels, id)\n",
    "del sq_len\n",
    "\n",
    "test_sentences = [[word_to_int[word] for word in sentence if word in word_to_int] for sentence in test_sentences]\n",
    "print('Number of empty test sentences: ', sum([len(s) == 0 for s in test_sentences]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zN0zGHnywrDN"
   },
   "source": [
    "## Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pleyeNY4wrDO"
   },
   "outputs": [],
   "source": [
    "class HOFDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.data = []\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            self.data.append(\n",
    "                (torch.tensor(sentence, dtype=torch.long), \n",
    "                 torch.tensor(label, dtype=torch.float))\n",
    "            )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "def preprocess_batch(batch):\n",
    "    texts, labels = list(zip(*batch))\n",
    "    seq_lens = torch.tensor([len(text) for text in texts], dtype=torch.long)\n",
    "    texts = pad_sequence(texts, padding_value=0)\n",
    "    labels = torch.tensor(labels).unsqueeze(1)\n",
    "\n",
    "    seq_lens, sorted_idx = seq_lens.sort(descending=True)\n",
    "    texts = texts[:,sorted_idx]\n",
    "    labels = labels[sorted_idx]\n",
    "    return texts, seq_lens, labels\n",
    "\n",
    "train_dataset = HOFDataset(train_sentences, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True, collate_fn=preprocess_batch)\n",
    "\n",
    "test_dataset = HOFDataset(test_sentences, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         shuffle=False, collate_fn=preprocess_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkJKUeW1wrDO"
   },
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TlC1yLNSwrDO"
   },
   "outputs": [],
   "source": [
    "def mask_seq(seq_lens):\n",
    "    mask = torch.zeros((len(seq_lens), max(seq_lens))).bool()\n",
    "    for i, seq_len in enumerate(seq_lens):\n",
    "        mask[i, seq_len:] = True\n",
    "    return mask\n",
    "\n",
    "class Classifier(Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "#         self.lstm_dim = lstm_dim\n",
    "#         self.bidirectional = bidirectional\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embed.load_state_dict(torch.load(embedding_path))\n",
    "        self.embed.requires_grad = False\n",
    "        # self.lstm = nn.LSTM(input_size=embedding_size, \n",
    "        #                     hidden_size=lstm_dim, \n",
    "        #                     num_layers=2, \n",
    "        #                     batch_first=False, \n",
    "        #                     dropout=0.5, \n",
    "        #                     bidirectional=bidirectional)\n",
    "        \n",
    "        # self.fc = nn.Linear(lstm_dim*2 if bidirectional else lstm_dim, 1)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(#embed_dim=lstm_dim*2 if bidirectional else lstm_dim,\n",
    "                                               embed_dim=embedding_size,\n",
    "                                               num_heads=10,\n",
    "                                               dropout=0.5,) # need to add mask for padding positions\n",
    "\n",
    "        self.fc = nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, inp, seq_lens):\n",
    "        out = self.embed(inp)\n",
    "        # out = pack_padded_sequence(out, seq_lens, batch_first=False, \n",
    "                                #    enforce_sorted = True)\n",
    "        # out, _ = self.lstm(out)\n",
    "        # out, _ = pad_packed_sequence(out, batch_first=False)\n",
    "\n",
    "        pad_mask = mask_seq(seq_lens)\n",
    "        att_out, _ = self.attention(out, out, out)#, key_padding_mask=pad_mask)\n",
    "        out = F.layer_norm(out + att_out, (out.size(2), ))\n",
    "\n",
    "        out = self.fc(out)\n",
    "        out = out.squeeze(2)\n",
    "        pred = torch.zeros((out.size(1), 1))\n",
    "        for i, seq_len in enumerate(seq_lens):\n",
    "            pred[i, 0] = out[:seq_len, i].mean()\n",
    "        return pred\n",
    "\n",
    "clf = Classifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QzEHt_X9wrDP"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(clf.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pxNOHE3R5hzR"
   },
   "outputs": [],
   "source": [
    "def predict_test():\n",
    "    losses = 0\n",
    "    acc_cnt = 0\n",
    "    cnt = 0\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    clf.eval()\n",
    "    for texts, seq_lens, labels in test_loader:\n",
    "        pred = clf(texts.to(device), seq_lens).detach().to('cpu')\n",
    "        loss = criterion(pred, labels)\n",
    "        losses += loss.detach().item() * len(texts)\n",
    "        acc_cnt += sum((pred > 0) == (labels > 0)).item()\n",
    "        preds.extend(pred.view(-1))\n",
    "        true_labels.extend(labels.view(-1))\n",
    "        cnt += texts.size(1)\n",
    "    \n",
    "    preds = np.array(preds) > 0\n",
    "    macro_f1 = f1_score(true_labels, preds, average='macro')\n",
    "    weighted_f1 = f1_score(true_labels, preds, average='weighted')\n",
    "    return losses / cnt, acc_cnt / cnt, macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32KkxNPwwrDP",
    "outputId": "6d48726f-c501-4995-abde-c42033d51cd4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:05<00:00, 26.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Train loss: 0.8894, acc: 0.5239. Test loss: 0.8302, acc: 0.5250, macro_f1: 0.5067, weighted_f1: 0.4989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:06<00:00, 23.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: Train loss: 0.8766, acc: 0.5666. Test loss: 0.8228, acc: 0.5319, macro_f1: 0.4957, weighted_f1: 0.4846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:05<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: Train loss: 0.8620, acc: 0.5947. Test loss: 0.8181, acc: 0.5296, macro_f1: 0.4781, weighted_f1: 0.4647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:05<00:00, 25.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: Train loss: 0.8314, acc: 0.6416. Test loss: 0.7809, acc: 0.6411, macro_f1: 0.6410, weighted_f1: 0.6406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:05<00:00, 25.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: Train loss: 0.7938, acc: 0.6794. Test loss: 0.7639, acc: 0.6442, macro_f1: 0.6441, weighted_f1: 0.6439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:05<00:00, 25.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: Train loss: 0.7497, acc: 0.7156. Test loss: 0.7638, acc: 0.6396, macro_f1: 0.6393, weighted_f1: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:05<00:00, 25.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: Train loss: 0.6973, acc: 0.7341. Test loss: 0.7848, acc: 0.6305, macro_f1: 0.6301, weighted_f1: 0.6290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:06<00:00, 23.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: Train loss: 0.6233, acc: 0.7707. Test loss: 0.8321, acc: 0.6282, macro_f1: 0.6281, weighted_f1: 0.6276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:05<00:00, 25.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: Train loss: 0.5415, acc: 0.8119. Test loss: 0.9339, acc: 0.5888, macro_f1: 0.5860, weighted_f1: 0.5832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:05<00:00, 25.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train loss: 0.4676, acc: 0.8430. Test loss: 1.0243, acc: 0.5759, macro_f1: 0.5745, weighted_f1: 0.5725\n",
      "Early stopping: test accuracy does not increase after 5 epochs\n"
     ]
    }
   ],
   "source": [
    "list_test_acc = []\n",
    "early_stop = 5\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    losses = 0.\n",
    "    acc_cnt = 0\n",
    "    cnt = 0\n",
    "    clf.train()\n",
    "    for texts, seq_lens, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = clf(texts.to(device), seq_lens)\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item() * len(texts)\n",
    "        acc_cnt += sum((pred.to('cpu') > 0) == (labels > 0)).item()\n",
    "        cnt += texts.size(1)\n",
    "\n",
    "    epoch_loss = losses / cnt\n",
    "    epoch_acc = acc_cnt / cnt\n",
    "    test_loss, test_acc, test_macro_f1, test_weighted_f1 = predict_test()\n",
    "    print(f'Epoch {epoch:2}: Train loss: {epoch_loss:.4f}, acc: {epoch_acc:.4f}. '\n",
    "        f'Test loss: {test_loss:.4f}, acc: {test_acc:.4f}, '\n",
    "        f'macro_f1: {test_macro_f1:.4f}, weighted_f1: {test_weighted_f1:.4f}',\n",
    "        flush=True)\n",
    "\n",
    "    list_test_acc.append(test_acc)\n",
    "    if len(list_test_acc) > early_stop and max(list_test_acc[-early_stop:]) <= max(list_test_acc[:-early_stop]):\n",
    "        print(f'Early stopping: test accuracy does not increase after {early_stop} epochs')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KaL79Rtk7I4_",
    "outputId": "b14e49af-a57a-43ec-ddb0-bcae0cc41ef9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318, 0.5409711684370258)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels), sum(test_labels == 0) / len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKQSKIehmBRI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TaZKBcptsuax"
   },
   "outputs": [],
   "source": [
    "# The following setting got test acc: 0.6586\n",
    "# embedding_path = '../Task_1/save/embedding_weights_hi_30_epoch_300_dim_5_wsize.pt'\n",
    "# embedding_size = 300\n",
    "# att_dim = 150\n",
    "# # lstm_dim = 100\n",
    "# # lstm_bidirectional = True\n",
    "# learning_rate = 1e-4\n",
    "# batch_size = 16\n",
    "# epochs = 20\n",
    "# self.attention = nn.MultiheadAttention(#embed_dim=lstm_dim*2 if bidirectional else lstm_dim,\n",
    "#                                        embed_dim=embedding_size,\n",
    "#                                        num_heads=10,\n",
    "#                                        dropout=0.5,) # need to add mask for padding positions"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
