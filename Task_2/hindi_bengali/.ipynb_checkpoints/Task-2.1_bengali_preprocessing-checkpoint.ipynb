{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Sample Bengali dataset and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(26)\n",
    "np.random.seed(62)\n",
    "torch.manual_seed(2021)\n",
    "torch.cuda.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "\n",
    "# get vocab_size\n",
    "with open('../../Task_1/save/word_to_int_dict.json') as f:\n",
    "    hindi_word_to_int = json.load(f)\n",
    "hindi_vocab_size = len(hindi_word_to_int)\n",
    "\n",
    "# define classifier\n",
    "class HindiClassifier(Module):\n",
    "    def __init__(self):\n",
    "        super(HindiClassifier, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(hindi_vocab_size, embedding_size)\n",
    "#         self.embed.load_state_dict(torch.load(embedding_path, map_location=torch.device(device)))\n",
    "#         self.embed.requires_grad = False\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_size,\n",
    "                                               num_heads=10,\n",
    "                                               dropout=0.5,) # need to add mask for padding positions\n",
    "\n",
    "        self.fc = nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, inp, seq_lens):\n",
    "        out = self.embed(inp)\n",
    "        pad_mask = mask_seq(seq_lens)\n",
    "        att_out, _ = self.attention(out, out, out, key_padding_mask=pad_mask)\n",
    "        out = F.layer_norm(out + att_out, (out.size(2), ))\n",
    "        out = self.fc(out).squeeze(2)\n",
    "        pred = torch.zeros((out.size(1), 1))\n",
    "        for i, seq_len in enumerate(seq_lens):\n",
    "            pred[i, 0] = out[:seq_len, i].mean()\n",
    "        return pred\n",
    "\n",
    "hindi_clf = HindiClassifier()\n",
    "hindi_model_weight_path = '../hindi_hindi/save/hindi_clf.pt'\n",
    "\n",
    "hindi_clf.load_state_dict(torch.load(hindi_model_weight_path, map_location=torch.device(device)))\n",
    "hindi_clf.eval()\n",
    "\n",
    "bengali_vocab_size = 15000\n",
    "bengali_embed = nn.Embedding(bengali_vocab_size, embedding_size)\n",
    "bengali_clf = nn.Sequential(*([bengali_embed] + list(hindi_clf.children())[1:]))\n",
    "\n",
    "bengali_clf.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Bengali dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi - training data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HOF    2469\n",
       "NOT    2196\n",
       "Name: task_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hindi - test data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NOT    713\n",
       "HOF    605\n",
       "Name: task_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the size and number of HOF, NOT sentences in hindi-dataset\n",
    "print('Hindi - training data')\n",
    "hindi_train_df = pd.read_csv('../data/hindi_hatespeech.tsv', sep='\\t')\n",
    "display(hindi_train_df['task_1'].value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print('Hindi - test data')\n",
    "hindi_test_df = pd.read_csv('../data/hasoc2019_hi_test_gold_2919.tsv', sep='\\t')\n",
    "display(hindi_test_df['task_1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data from the Bengali dataset so that it has the same size and distribution as in Hindi dataset\n",
    "bengali_df = pd.read_csv('../data/bengali_hatespeech.csv')\n",
    "\n",
    "bengali_hate_sample_df = bengali_df[bengali_df['hate']==1].sample(2469+605, random_state=1, replace=False)\n",
    "bengali_hate_train, bengali_hate_test = train_test_split(bengali_hate_sample_df, train_size=2469, random_state=2)\n",
    "\n",
    "bengali_not_sample_df = bengali_df[bengali_df['hate']==0].sample(2196+713, random_state=1, replace=False)\n",
    "bengali_not_train, bengali_not_test = train_test_split(bengali_not_sample_df, train_size=2196, random_state=2)\n",
    "\n",
    "bengali_train_df = pd.concat([bengali_not_train, bengali_hate_train]).sample(frac=1)\n",
    "bengali_test_df = pd.concat([bengali_not_test, bengali_hate_test]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali - sample training data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    2469\n",
       "0    2196\n",
       "Name: hate, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bengali - sample test data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    713\n",
       "1    605\n",
       "Name: hate, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show stats of sample Bengali datasets\n",
    "print('Bengali - sample training data')\n",
    "display(bengali_train_df['hate'].value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print('Bengali - sample test data')\n",
    "display(bengali_test_df['hate'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sample Bengali datasets\n",
    "bengali_train_df.to_csv('../data/bengali_hatespeech_sample_train.csv', index=False)\n",
    "bengali_test_df.to_csv('../data/bengali_hatespeech_sample_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = bengali_train_df['sentence']\n",
    "test_sentences = bengali_test_df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(sentences):\n",
    "    # remove user taggings\n",
    "    user_tag_pattern = re.compile(r'\\@\\w*')\n",
    "    sentences = [re.sub(user_tag_pattern, ' ', sentence) for sentence in sentences]\n",
    "    # lower case\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "    # remove punctuations\n",
    "    punctuation = string.punctuation[:2] + string.punctuation[3:]\n",
    "    translator = str.maketrans(punctuation, ' '*len(punctuation))\n",
    "    def remove_punc(s):\n",
    "        s = s.translate(translator)\n",
    "        return s\n",
    "\n",
    "    sentences = [remove_punc(sentence) for sentence in sentences]\n",
    "    \n",
    "    # remove stopwords BENGALI\n",
    "    # source: https://www.ranks.nl/stopwords/bengali\n",
    "    stopwords = ['অবশ্য', 'অনেক', 'অনেকে', 'অনেকেই', 'অন্তত', 'অথবা', 'অথচ', 'অর্থাত', 'অন্য', 'আজ', 'আছে', 'আপনার', \n",
    "                 'আপনি', 'আবার', 'আমরা', 'আমাকে', 'আমাদের', 'আমার', 'আমি', 'আরও', 'আর', 'আগে', 'আগেই', 'আই', \n",
    "                 'অতএব', 'আগামী', 'অবধি', 'অনুযায়ী', 'আদ্যভাগে', 'এই', 'একই', 'একে', 'একটি', 'এখন', 'এখনও', 'এখানে', \n",
    "                 'এখানেই', 'এটি', 'এটা', 'এটাই', 'এতটাই', 'এবং', 'একবার', 'এবার', 'এদের', 'এঁদের', 'এমন', 'এমনকী', 'এল', \n",
    "                 'এর', 'এরা', 'এঁরা', 'এস', 'এত', 'এতে', 'এসে', 'একে', 'এ', 'ঐ', ' ই', 'ইহা', 'ইত্যাদি', 'উনি', 'উপর', \n",
    "                 'উপরে', 'উচিত', 'ও', 'ওই', 'ওর', 'ওরা', 'ওঁর', 'ওঁরা', 'ওকে', 'ওদের', 'ওঁদের', 'ওখানে', 'কত', 'কবে', \n",
    "                 'করতে', 'কয়েক', 'কয়েকটি', 'করবে', 'করলেন', 'করার', 'কারও', 'করা', 'করি', 'করিয়ে', 'করার', 'করাই', \n",
    "                 'করলে', 'করলেন', 'করিতে', 'করিয়া', 'করেছিলেন', 'করছে', 'করছেন', 'করেছেন', 'করেছে', 'করেন', 'করবেন', \n",
    "                 'করায়', 'করে', 'করেই', 'কাছ', 'কাছে', 'কাজে', 'কারণ', 'কিছু', 'কিছুই', 'কিন্তু', 'কিংবা', 'কি', 'কী', 'কেউ', \n",
    "                 'কেউই', 'কাউকে', 'কেন', 'কে', 'কোনও', 'কোনো', 'কোন', 'কখনও', 'ক্ষেত্রে', 'খুব\tগুলি', 'গিয়ে', 'গিয়েছে', \n",
    "                 'গেছে', 'গেল', 'গেলে', 'গোটা', 'চলে', 'ছাড়া', 'ছাড়াও', 'ছিলেন', 'ছিল', 'জন্য', 'জানা', 'ঠিক', 'তিনি', \n",
    "                 'তিনঐ', 'তিনিও', 'তখন', 'তবে', 'তবু', 'তাঁদের', 'তাঁাহারা', 'তাঁরা', 'তাঁর', 'তাঁকে', 'তাই', 'তেমন', 'তাকে', \n",
    "                 'তাহা', 'তাহাতে', 'তাহার', 'তাদের', 'তারপর', 'তারা', 'তারৈ', 'তার', 'তাহলে', 'তিনি', 'তা', 'তাও', 'তাতে', \n",
    "                 'তো', 'তত', 'তুমি', 'তোমার', 'তথা', 'থাকে', 'থাকা', 'থাকায়', 'থেকে', 'থেকেও', 'থাকবে', 'থাকেন', 'থাকবেন', \n",
    "                 'থেকেই', 'দিকে', 'দিতে', 'দিয়ে', 'দিয়েছে', 'দিয়েছেন', 'দিলেন', 'দু', 'দুটি', 'দুটো', 'দেয়', 'দেওয়া', 'দেওয়ার', \n",
    "                 'দেখা', 'দেখে', 'দেখতে', 'দ্বারা', 'ধরে', 'ধরা', 'নয়', 'নানা', 'না', 'নাকি', 'নাগাদ', 'নিতে', 'নিজে', 'নিজেই', \n",
    "                 'নিজের', 'নিজেদের', 'নিয়ে', 'নেওয়া', 'নেওয়ার', 'নেই', 'নাই', 'পক্ষে', 'পর্যন্ত', 'পাওয়া', 'পারেন', 'পারি', 'পারে', \n",
    "                 'পরে', 'পরেই', 'পরেও', 'পর', 'পেয়ে', 'প্রতি', 'প্রভৃতি', 'প্রায়', 'ফের', 'ফলে', 'ফিরে', 'ব্যবহার', 'বলতে', \n",
    "                 'বললেন', 'বলেছেন', 'বলল', 'বলা', 'বলেন', 'বলে', 'বহু', 'বসে', 'বার', 'বা', 'বিনা', 'বরং', 'বদলে', 'বাদে', \n",
    "                 'বার', 'বিশেষ', 'বিভিন্ন\tবিষয়টি', 'ব্যবহার', 'ব্যাপারে', 'ভাবে', 'ভাবেই', 'মধ্যে', 'মধ্যেই', 'মধ্যেও', 'মধ্যভাগে', \n",
    "                 'মাধ্যমে', 'মাত্র', 'মতো', 'মতোই', 'মোটেই', 'যখন', 'যদি', 'যদিও', 'যাবে', 'যায়', 'যাকে', 'যাওয়া', 'যাওয়ার', \n",
    "                 'যত', 'যতটা', 'যা', 'যার', 'যারা', 'যাঁর', 'যাঁরা', 'যাদের', 'যান', 'যাচ্ছে', 'যেতে', 'যাতে', 'যেন', 'যেমন', \n",
    "                 'যেখানে', 'যিনি', 'যে', 'রেখে', 'রাখা', 'রয়েছে', 'রকম', 'শুধু', 'সঙ্গে', 'সঙ্গেও', 'সমস্ত', 'সব', 'সবার', 'সহ', \n",
    "                 'সুতরাং', 'সহিত', 'সেই', 'সেটা', 'সেটি', 'সেটাই', 'সেটাও', 'সম্প্রতি', 'সেখান', 'সেখানে', 'সে', 'স্পষ্ট', 'স্বয়ং', \n",
    "                 'হইতে', 'হইবে', 'হৈলে', 'হইয়া', 'হচ্ছে', 'হত', 'হতে', 'হতেই', 'হবে', 'হবেন', 'হয়েছিল', 'হয়েছে', 'হয়েছেন', 'হয়ে', \n",
    "                 'হয়নি', 'হয়', 'হয়েই', 'হয়তো', 'হল', 'হলে', 'হলেই', 'হলেও', 'হলো', 'হিসাবে', 'হওয়া', 'হওয়ার', 'হওয়ায়', 'হন', \n",
    "                 'হোক', 'জন', 'জনকে', 'জনের', 'জানতে', 'জানায়', 'জানিয়ে', 'জানানো', 'জানিয়েছে', 'জন্য', 'জন্যওজে', 'জে', \n",
    "                 'বেশ', 'দেন', 'তুলে', 'ছিলেন', 'চান', 'চায়', 'চেয়ে', 'মোট', 'যথেষ্ট', 'টি']\n",
    "\n",
    "    sentences = [[word for word in sentence.split() if word not in stopwords] for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "train_sentences = preprocess_texts(train_sentences)\n",
    "train_texts = [' '.join(l) for l in train_sentences]\n",
    "bengali_train_df['sentence'] = train_texts\n",
    "\n",
    "test_sentences = preprocess_texts(test_sentences)\n",
    "test_texts = [' '.join(l) for l in test_sentences]\n",
    "bengali_test_df['sentence'] = test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20264</th>\n",
       "      <td>ভায়েরা আপনাদের ধন্যোবাদ এগিয়ে জাও পাসে আছি ভাই</td>\n",
       "      <td>0</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29036</th>\n",
       "      <td>নাউজুবিল্লাহ নাউজুবিল্লাহ</td>\n",
       "      <td>0</td>\n",
       "      <td>Meme, TikTok and others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18435</th>\n",
       "      <td>দুইজন অপরাধ সরকারি চাকরি হিসেবে দুইজনকে বাংলাদ...</td>\n",
       "      <td>0</td>\n",
       "      <td>crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12232</th>\n",
       "      <td>উড়িয়ে ই মারলো পেরেরা</td>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14640</th>\n",
       "      <td>পুরুষ এক জাত অনেকসময় বোঝেনা সময় বুঝতে চায়না...</td>\n",
       "      <td>0</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  hate  \\\n",
       "20264     ভায়েরা আপনাদের ধন্যোবাদ এগিয়ে জাও পাসে আছি ভাই     0   \n",
       "29036                          নাউজুবিল্লাহ নাউজুবিল্লাহ     0   \n",
       "18435  দুইজন অপরাধ সরকারি চাকরি হিসেবে দুইজনকে বাংলাদ...     0   \n",
       "12232                               উড়িয়ে ই মারলো পেরেরা     0   \n",
       "14640  পুরুষ এক জাত অনেকসময় বোঝেনা সময় বুঝতে চায়না...     0   \n",
       "\n",
       "                      category  \n",
       "20264                 religion  \n",
       "29036  Meme, TikTok and others  \n",
       "18435                    crime  \n",
       "12232                   sports  \n",
       "14640            entertainment  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25861</th>\n",
       "      <td>লেখাটি ফুটবল বুঝেই লেখা।</td>\n",
       "      <td>0</td>\n",
       "      <td>celebrity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11360</th>\n",
       "      <td>ভাই কথা শুনে কান্না আসলো।</td>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>খানকি নাইকা</td>\n",
       "      <td>1</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7442</th>\n",
       "      <td>মাগি তোরে ঠিইক সতন বাগুন দিলে ভালও হইত তোর ভালওনা</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>কয়টা বিচারকদের চোদার দরকার 😠😠😠😠😈😈😈😡</td>\n",
       "      <td>1</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  hate       category\n",
       "25861                           লেখাটি ফুটবল বুঝেই লেখা।     0      celebrity\n",
       "11360                          ভাই কথা শুনে কান্না আসলো।     0         sports\n",
       "1691                                         খানকি নাইকা     1  entertainment\n",
       "7442   মাগি তোরে ঠিইক সতন বাগুন দিলে ভালও হইত তোর ভালওনা     1       politics\n",
       "2569                 কয়টা বিচারকদের চোদার দরকার 😠😠😠😠😈😈😈😡     1  entertainment"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(bengali_train_df.head())\n",
    "display(bengali_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali_train_df.to_csv('../data/bengali_hatespeech_sample_train_preprocessed.csv', index=False)\n",
    "bengali_test_df.to_csv('../data/bengali_hatespeech_sample_test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
