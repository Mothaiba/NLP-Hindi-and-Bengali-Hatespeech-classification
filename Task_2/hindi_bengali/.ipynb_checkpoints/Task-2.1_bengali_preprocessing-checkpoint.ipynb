{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Sample Bengali dataset and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(26)\n",
    "np.random.seed(62)\n",
    "torch.manual_seed(2021)\n",
    "torch.cuda.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "\n",
    "# get vocab_size\n",
    "with open('../../Task_1/save/word_to_int_dict.json') as f:\n",
    "    hindi_word_to_int = json.load(f)\n",
    "hindi_vocab_size = len(hindi_word_to_int)\n",
    "\n",
    "# define classifier\n",
    "class HindiClassifier(Module):\n",
    "    def __init__(self):\n",
    "        super(HindiClassifier, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(hindi_vocab_size, embedding_size)\n",
    "#         self.embed.load_state_dict(torch.load(embedding_path, map_location=torch.device(device)))\n",
    "#         self.embed.requires_grad = False\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_size,\n",
    "                                               num_heads=10,\n",
    "                                               dropout=0.5,) # need to add mask for padding positions\n",
    "\n",
    "        self.fc = nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, inp, seq_lens):\n",
    "        out = self.embed(inp)\n",
    "        pad_mask = mask_seq(seq_lens)\n",
    "        att_out, _ = self.attention(out, out, out, key_padding_mask=pad_mask)\n",
    "        out = F.layer_norm(out + att_out, (out.size(2), ))\n",
    "        out = self.fc(out).squeeze(2)\n",
    "        pred = torch.zeros((out.size(1), 1))\n",
    "        for i, seq_len in enumerate(seq_lens):\n",
    "            pred[i, 0] = out[:seq_len, i].mean()\n",
    "        return pred\n",
    "\n",
    "hindi_clf = HindiClassifier()\n",
    "hindi_model_weight_path = '../hindi_hindi/save/hindi_clf.pt'\n",
    "\n",
    "hindi_clf.load_state_dict(torch.load(hindi_model_weight_path, map_location=torch.device(device)))\n",
    "hindi_clf.eval()\n",
    "\n",
    "bengali_vocab_size = 15000\n",
    "bengali_embed = nn.Embedding(bengali_vocab_size, embedding_size)\n",
    "bengali_clf = nn.Sequential(*([bengali_embed] + list(hindi_clf.children())[1:]))\n",
    "\n",
    "bengali_clf.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Bengali dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi - training data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HOF    2469\n",
       "NOT    2196\n",
       "Name: task_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hindi - test data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NOT    713\n",
       "HOF    605\n",
       "Name: task_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the size and number of HOF, NOT sentences in hindi-dataset\n",
    "print('Hindi - training data')\n",
    "hindi_train_df = pd.read_csv('../data/hindi_hatespeech.tsv', sep='\\t')\n",
    "display(hindi_train_df['task_1'].value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print('Hindi - test data')\n",
    "hindi_test_df = pd.read_csv('../data/hasoc2019_hi_test_gold_2919.tsv', sep='\\t')\n",
    "display(hindi_test_df['task_1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data from the Bengali dataset so that it has the same size and distribution as in Hindi dataset\n",
    "bengali_df = pd.read_csv('../data/bengali_hatespeech.csv')\n",
    "\n",
    "bengali_hate_sample_df = bengali_df[bengali_df['hate']==1].sample(2469+605, random_state=1, replace=False)\n",
    "bengali_hate_train, bengali_hate_test = train_test_split(bengali_hate_sample_df, train_size=2469, random_state=2)\n",
    "\n",
    "bengali_not_sample_df = bengali_df[bengali_df['hate']==0].sample(2196+713, random_state=1, replace=False)\n",
    "bengali_not_train, bengali_not_test = train_test_split(bengali_not_sample_df, train_size=2196, random_state=2)\n",
    "\n",
    "bengali_train_df = pd.concat([bengali_not_train, bengali_hate_train]).sample(frac=1)\n",
    "bengali_test_df = pd.concat([bengali_not_test, bengali_hate_test]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali - sample training data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    2469\n",
       "0    2196\n",
       "Name: hate, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bengali - sample test data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    713\n",
       "1    605\n",
       "Name: hate, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show stats of sample Bengali datasets\n",
    "print('Bengali - sample training data')\n",
    "display(bengali_train_df['hate'].value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print('Bengali - sample test data')\n",
    "display(bengali_test_df['hate'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sample Bengali datasets\n",
    "bengali_train_df.to_csv('../data/bengali_hatespeech_sample_train.csv', index=False)\n",
    "bengali_test_df.to_csv('../data/bengali_hatespeech_sample_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = bengali_train_df['sentence']\n",
    "test_sentences = bengali_test_df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(sentences):\n",
    "    # remove user taggings\n",
    "    user_tag_pattern = re.compile(r'\\@\\w*')\n",
    "    sentences = [re.sub(user_tag_pattern, ' ', sentence) for sentence in sentences]\n",
    "    # lower case\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "    # remove punctuations\n",
    "    punctuation = string.punctuation[:2] + string.punctuation[3:]\n",
    "    translator = str.maketrans(punctuation, ' '*len(punctuation))\n",
    "    def remove_punc(s):\n",
    "        s = s.translate(translator)\n",
    "        return s\n",
    "\n",
    "    sentences = [remove_punc(sentence) for sentence in sentences]\n",
    "    \n",
    "    # remove stopwords BENGALI\n",
    "    # source: https://www.ranks.nl/stopwords/bengali\n",
    "    stopwords = ['‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø', '‡¶Ö‡¶®‡ßá‡¶ï', '‡¶Ö‡¶®‡ßá‡¶ï‡ßá', '‡¶Ö‡¶®‡ßá‡¶ï‡ßá‡¶á', '‡¶Ö‡¶®‡ßç‡¶§‡¶§', '‡¶Ö‡¶•‡¶¨‡¶æ', '‡¶Ö‡¶•‡¶ö', '‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡¶§', '‡¶Ö‡¶®‡ßç‡¶Ø', '‡¶Ü‡¶ú', '‡¶Ü‡¶õ‡ßá', '‡¶Ü‡¶™‡¶®‡¶æ‡¶∞', \n",
    "                 '‡¶Ü‡¶™‡¶®‡¶ø', '‡¶Ü‡¶¨‡¶æ‡¶∞', '‡¶Ü‡¶Æ‡¶∞‡¶æ', '‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá', '‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞', '‡¶Ü‡¶Æ‡¶æ‡¶∞', '‡¶Ü‡¶Æ‡¶ø', '‡¶Ü‡¶∞‡¶ì', '‡¶Ü‡¶∞', '‡¶Ü‡¶ó‡ßá', '‡¶Ü‡¶ó‡ßá‡¶á', '‡¶Ü‡¶á', \n",
    "                 '‡¶Ö‡¶§‡¶è‡¶¨', '‡¶Ü‡¶ó‡¶æ‡¶Æ‡ßÄ', '‡¶Ö‡¶¨‡¶ß‡¶ø', '‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ', '‡¶Ü‡¶¶‡ßç‡¶Ø‡¶≠‡¶æ‡¶ó‡ßá', '‡¶è‡¶á', '‡¶è‡¶ï‡¶á', '‡¶è‡¶ï‡ßá', '‡¶è‡¶ï‡¶ü‡¶ø', '‡¶è‡¶ñ‡¶®', '‡¶è‡¶ñ‡¶®‡¶ì', '‡¶è‡¶ñ‡¶æ‡¶®‡ßá', \n",
    "                 '‡¶è‡¶ñ‡¶æ‡¶®‡ßá‡¶á', '‡¶è‡¶ü‡¶ø', '‡¶è‡¶ü‡¶æ', '‡¶è‡¶ü‡¶æ‡¶á', '‡¶è‡¶§‡¶ü‡¶æ‡¶á', '‡¶è‡¶¨‡¶Ç', '‡¶è‡¶ï‡¶¨‡¶æ‡¶∞', '‡¶è‡¶¨‡¶æ‡¶∞', '‡¶è‡¶¶‡ßá‡¶∞', '‡¶è‡¶Å‡¶¶‡ßá‡¶∞', '‡¶è‡¶Æ‡¶®', '‡¶è‡¶Æ‡¶®‡¶ï‡ßÄ', '‡¶è‡¶≤', \n",
    "                 '‡¶è‡¶∞', '‡¶è‡¶∞‡¶æ', '‡¶è‡¶Å‡¶∞‡¶æ', '‡¶è‡¶∏', '‡¶è‡¶§', '‡¶è‡¶§‡ßá', '‡¶è‡¶∏‡ßá', '‡¶è‡¶ï‡ßá', '‡¶è', '‡¶ê', ' ‡¶á', '‡¶á‡¶π‡¶æ', '‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø', '‡¶â‡¶®‡¶ø', '‡¶â‡¶™‡¶∞', \n",
    "                 '‡¶â‡¶™‡¶∞‡ßá', '‡¶â‡¶ö‡¶ø‡¶§', '‡¶ì', '‡¶ì‡¶á', '‡¶ì‡¶∞', '‡¶ì‡¶∞‡¶æ', '‡¶ì‡¶Å‡¶∞', '‡¶ì‡¶Å‡¶∞‡¶æ', '‡¶ì‡¶ï‡ßá', '‡¶ì‡¶¶‡ßá‡¶∞', '‡¶ì‡¶Å‡¶¶‡ßá‡¶∞', '‡¶ì‡¶ñ‡¶æ‡¶®‡ßá', '‡¶ï‡¶§', '‡¶ï‡¶¨‡ßá', \n",
    "                 '‡¶ï‡¶∞‡¶§‡ßá', '‡¶ï‡ßü‡ßá‡¶ï', '‡¶ï‡ßü‡ßá‡¶ï‡¶ü‡¶ø', '‡¶ï‡¶∞‡¶¨‡ßá', '‡¶ï‡¶∞‡¶≤‡ßá‡¶®', '‡¶ï‡¶∞‡¶æ‡¶∞', '‡¶ï‡¶æ‡¶∞‡¶ì', '‡¶ï‡¶∞‡¶æ', '‡¶ï‡¶∞‡¶ø', '‡¶ï‡¶∞‡¶ø‡ßü‡ßá', '‡¶ï‡¶∞‡¶æ‡¶∞', '‡¶ï‡¶∞‡¶æ‡¶á', \n",
    "                 '‡¶ï‡¶∞‡¶≤‡ßá', '‡¶ï‡¶∞‡¶≤‡ßá‡¶®', '‡¶ï‡¶∞‡¶ø‡¶§‡ßá', '‡¶ï‡¶∞‡¶ø‡ßü‡¶æ', '‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®', '‡¶ï‡¶∞‡¶õ‡ßá', '‡¶ï‡¶∞‡¶õ‡ßá‡¶®', '‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®', '‡¶ï‡¶∞‡ßá‡¶õ‡ßá', '‡¶ï‡¶∞‡ßá‡¶®', '‡¶ï‡¶∞‡¶¨‡ßá‡¶®', \n",
    "                 '‡¶ï‡¶∞‡¶æ‡ßü', '‡¶ï‡¶∞‡ßá', '‡¶ï‡¶∞‡ßá‡¶á', '‡¶ï‡¶æ‡¶õ', '‡¶ï‡¶æ‡¶õ‡ßá', '‡¶ï‡¶æ‡¶ú‡ßá', '‡¶ï‡¶æ‡¶∞‡¶£', '‡¶ï‡¶ø‡¶õ‡ßÅ', '‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á', '‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ', '‡¶ï‡¶ø‡¶Ç‡¶¨‡¶æ', '‡¶ï‡¶ø', '‡¶ï‡ßÄ', '‡¶ï‡ßá‡¶â', \n",
    "                 '‡¶ï‡ßá‡¶â‡¶á', '‡¶ï‡¶æ‡¶â‡¶ï‡ßá', '‡¶ï‡ßá‡¶®', '‡¶ï‡ßá', '‡¶ï‡ßã‡¶®‡¶ì', '‡¶ï‡ßã‡¶®‡ßã', '‡¶ï‡ßã‡¶®', '‡¶ï‡¶ñ‡¶®‡¶ì', '‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá', '‡¶ñ‡ßÅ‡¶¨\t‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶ó‡¶ø‡ßü‡ßá', '‡¶ó‡¶ø‡ßü‡ßá‡¶õ‡ßá', \n",
    "                 '‡¶ó‡ßá‡¶õ‡ßá', '‡¶ó‡ßá‡¶≤', '‡¶ó‡ßá‡¶≤‡ßá', '‡¶ó‡ßã‡¶ü‡¶æ', '‡¶ö‡¶≤‡ßá', '‡¶õ‡¶æ‡ßú‡¶æ', '‡¶õ‡¶æ‡ßú‡¶æ‡¶ì', '‡¶õ‡¶ø‡¶≤‡ßá‡¶®', '‡¶õ‡¶ø‡¶≤', '‡¶ú‡¶®‡ßç‡¶Ø', '‡¶ú‡¶æ‡¶®‡¶æ', '‡¶†‡¶ø‡¶ï', '‡¶§‡¶ø‡¶®‡¶ø', \n",
    "                 '‡¶§‡¶ø‡¶®‡¶ê', '‡¶§‡¶ø‡¶®‡¶ø‡¶ì', '‡¶§‡¶ñ‡¶®', '‡¶§‡¶¨‡ßá', '‡¶§‡¶¨‡ßÅ', '‡¶§‡¶æ‡¶Å‡¶¶‡ßá‡¶∞', '‡¶§‡¶æ‡¶Å‡¶æ‡¶π‡¶æ‡¶∞‡¶æ', '‡¶§‡¶æ‡¶Å‡¶∞‡¶æ', '‡¶§‡¶æ‡¶Å‡¶∞', '‡¶§‡¶æ‡¶Å‡¶ï‡ßá', '‡¶§‡¶æ‡¶á', '‡¶§‡ßá‡¶Æ‡¶®', '‡¶§‡¶æ‡¶ï‡ßá', \n",
    "                 '‡¶§‡¶æ‡¶π‡¶æ', '‡¶§‡¶æ‡¶π‡¶æ‡¶§‡ßá', '‡¶§‡¶æ‡¶π‡¶æ‡¶∞', '‡¶§‡¶æ‡¶¶‡ßá‡¶∞', '‡¶§‡¶æ‡¶∞‡¶™‡¶∞', '‡¶§‡¶æ‡¶∞‡¶æ', '‡¶§‡¶æ‡¶∞‡ßà', '‡¶§‡¶æ‡¶∞', '‡¶§‡¶æ‡¶π‡¶≤‡ßá', '‡¶§‡¶ø‡¶®‡¶ø', '‡¶§‡¶æ', '‡¶§‡¶æ‡¶ì', '‡¶§‡¶æ‡¶§‡ßá', \n",
    "                 '‡¶§‡ßã', '‡¶§‡¶§', '‡¶§‡ßÅ‡¶Æ‡¶ø', '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞', '‡¶§‡¶•‡¶æ', '‡¶•‡¶æ‡¶ï‡ßá', '‡¶•‡¶æ‡¶ï‡¶æ', '‡¶•‡¶æ‡¶ï‡¶æ‡ßü', '‡¶•‡ßá‡¶ï‡ßá', '‡¶•‡ßá‡¶ï‡ßá‡¶ì', '‡¶•‡¶æ‡¶ï‡¶¨‡ßá', '‡¶•‡¶æ‡¶ï‡ßá‡¶®', '‡¶•‡¶æ‡¶ï‡¶¨‡ßá‡¶®', \n",
    "                 '‡¶•‡ßá‡¶ï‡ßá‡¶á', '‡¶¶‡¶ø‡¶ï‡ßá', '‡¶¶‡¶ø‡¶§‡ßá', '‡¶¶‡¶ø‡ßü‡ßá', '‡¶¶‡¶ø‡ßü‡ßá‡¶õ‡ßá', '‡¶¶‡¶ø‡ßü‡ßá‡¶õ‡ßá‡¶®', '‡¶¶‡¶ø‡¶≤‡ßá‡¶®', '‡¶¶‡ßÅ', '‡¶¶‡ßÅ‡¶ü‡¶ø', '‡¶¶‡ßÅ‡¶ü‡ßã', '‡¶¶‡ßá‡ßü', '‡¶¶‡ßá‡¶ì‡ßü‡¶æ', '‡¶¶‡ßá‡¶ì‡ßü‡¶æ‡¶∞', \n",
    "                 '‡¶¶‡ßá‡¶ñ‡¶æ', '‡¶¶‡ßá‡¶ñ‡ßá', '‡¶¶‡ßá‡¶ñ‡¶§‡ßá', '‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ', '‡¶ß‡¶∞‡ßá', '‡¶ß‡¶∞‡¶æ', '‡¶®‡ßü', '‡¶®‡¶æ‡¶®‡¶æ', '‡¶®‡¶æ', '‡¶®‡¶æ‡¶ï‡¶ø', '‡¶®‡¶æ‡¶ó‡¶æ‡¶¶', '‡¶®‡¶ø‡¶§‡ßá', '‡¶®‡¶ø‡¶ú‡ßá', '‡¶®‡¶ø‡¶ú‡ßá‡¶á', \n",
    "                 '‡¶®‡¶ø‡¶ú‡ßá‡¶∞', '‡¶®‡¶ø‡¶ú‡ßá‡¶¶‡ßá‡¶∞', '‡¶®‡¶ø‡ßü‡ßá', '‡¶®‡ßá‡¶ì‡ßü‡¶æ', '‡¶®‡ßá‡¶ì‡ßü‡¶æ‡¶∞', '‡¶®‡ßá‡¶á', '‡¶®‡¶æ‡¶á', '‡¶™‡¶ï‡ßç‡¶∑‡ßá', '‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§', '‡¶™‡¶æ‡¶ì‡ßü‡¶æ', '‡¶™‡¶æ‡¶∞‡ßá‡¶®', '‡¶™‡¶æ‡¶∞‡¶ø', '‡¶™‡¶æ‡¶∞‡ßá', \n",
    "                 '‡¶™‡¶∞‡ßá', '‡¶™‡¶∞‡ßá‡¶á', '‡¶™‡¶∞‡ßá‡¶ì', '‡¶™‡¶∞', '‡¶™‡ßá‡ßü‡ßá', '‡¶™‡ßç‡¶∞‡¶§‡¶ø', '‡¶™‡ßç‡¶∞‡¶≠‡ßÉ‡¶§‡¶ø', '‡¶™‡ßç‡¶∞‡¶æ‡ßü', '‡¶´‡ßá‡¶∞', '‡¶´‡¶≤‡ßá', '‡¶´‡¶ø‡¶∞‡ßá', '‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞', '‡¶¨‡¶≤‡¶§‡ßá', \n",
    "                 '‡¶¨‡¶≤‡¶≤‡ßá‡¶®', '‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶®', '‡¶¨‡¶≤‡¶≤', '‡¶¨‡¶≤‡¶æ', '‡¶¨‡¶≤‡ßá‡¶®', '‡¶¨‡¶≤‡ßá', '‡¶¨‡¶π‡ßÅ', '‡¶¨‡¶∏‡ßá', '‡¶¨‡¶æ‡¶∞', '‡¶¨‡¶æ', '‡¶¨‡¶ø‡¶®‡¶æ', '‡¶¨‡¶∞‡¶Ç', '‡¶¨‡¶¶‡¶≤‡ßá', '‡¶¨‡¶æ‡¶¶‡ßá', \n",
    "                 '‡¶¨‡¶æ‡¶∞', '‡¶¨‡¶ø‡¶∂‡ßá‡¶∑', '‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶®\t‡¶¨‡¶ø‡¶∑‡ßü‡¶ü‡¶ø', '‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞', '‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡¶æ‡¶∞‡ßá', '‡¶≠‡¶æ‡¶¨‡ßá', '‡¶≠‡¶æ‡¶¨‡ßá‡¶á', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá‡¶á', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá‡¶ì', '‡¶Æ‡¶ß‡ßç‡¶Ø‡¶≠‡¶æ‡¶ó‡ßá', \n",
    "                 '‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá', '‡¶Æ‡¶æ‡¶§‡ßç‡¶∞', '‡¶Æ‡¶§‡ßã', '‡¶Æ‡¶§‡ßã‡¶á', '‡¶Æ‡ßã‡¶ü‡ßá‡¶á', '‡¶Ø‡¶ñ‡¶®', '‡¶Ø‡¶¶‡¶ø', '‡¶Ø‡¶¶‡¶ø‡¶ì', '‡¶Ø‡¶æ‡¶¨‡ßá', '‡¶Ø‡¶æ‡ßü', '‡¶Ø‡¶æ‡¶ï‡ßá', '‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ', '‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ‡¶∞', \n",
    "                 '‡¶Ø‡¶§', '‡¶Ø‡¶§‡¶ü‡¶æ', '‡¶Ø‡¶æ', '‡¶Ø‡¶æ‡¶∞', '‡¶Ø‡¶æ‡¶∞‡¶æ', '‡¶Ø‡¶æ‡¶Å‡¶∞', '‡¶Ø‡¶æ‡¶Å‡¶∞‡¶æ', '‡¶Ø‡¶æ‡¶¶‡ßá‡¶∞', '‡¶Ø‡¶æ‡¶®', '‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá', '‡¶Ø‡ßá‡¶§‡ßá', '‡¶Ø‡¶æ‡¶§‡ßá', '‡¶Ø‡ßá‡¶®', '‡¶Ø‡ßá‡¶Æ‡¶®', \n",
    "                 '‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá', '‡¶Ø‡¶ø‡¶®‡¶ø', '‡¶Ø‡ßá', '‡¶∞‡ßá‡¶ñ‡ßá', '‡¶∞‡¶æ‡¶ñ‡¶æ', '‡¶∞‡ßü‡ßá‡¶õ‡ßá', '‡¶∞‡¶ï‡¶Æ', '‡¶∂‡ßÅ‡¶ß‡ßÅ', '‡¶∏‡¶ô‡ßç‡¶ó‡ßá', '‡¶∏‡¶ô‡ßç‡¶ó‡ßá‡¶ì', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶§', '‡¶∏‡¶¨', '‡¶∏‡¶¨‡¶æ‡¶∞', '‡¶∏‡¶π', \n",
    "                 '‡¶∏‡ßÅ‡¶§‡¶∞‡¶æ‡¶Ç', '‡¶∏‡¶π‡¶ø‡¶§', '‡¶∏‡ßá‡¶á', '‡¶∏‡ßá‡¶ü‡¶æ', '‡¶∏‡ßá‡¶ü‡¶ø', '‡¶∏‡ßá‡¶ü‡¶æ‡¶á', '‡¶∏‡ßá‡¶ü‡¶æ‡¶ì', '‡¶∏‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø', '‡¶∏‡ßá‡¶ñ‡¶æ‡¶®', '‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá', '‡¶∏‡ßá', '‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü', '‡¶∏‡ßç‡¶¨‡ßü‡¶Ç', \n",
    "                 '‡¶π‡¶á‡¶§‡ßá', '‡¶π‡¶á‡¶¨‡ßá', '‡¶π‡ßà‡¶≤‡ßá', '‡¶π‡¶á‡ßü‡¶æ', '‡¶π‡¶ö‡ßç‡¶õ‡ßá', '‡¶π‡¶§', '‡¶π‡¶§‡ßá', '‡¶π‡¶§‡ßá‡¶á', '‡¶π‡¶¨‡ßá', '‡¶π‡¶¨‡ßá‡¶®', '‡¶π‡ßü‡ßá‡¶õ‡¶ø‡¶≤', '‡¶π‡ßü‡ßá‡¶õ‡ßá', '‡¶π‡ßü‡ßá‡¶õ‡ßá‡¶®', '‡¶π‡ßü‡ßá', \n",
    "                 '‡¶π‡ßü‡¶®‡¶ø', '‡¶π‡ßü', '‡¶π‡ßü‡ßá‡¶á', '‡¶π‡ßü‡¶§‡ßã', '‡¶π‡¶≤', '‡¶π‡¶≤‡ßá', '‡¶π‡¶≤‡ßá‡¶á', '‡¶π‡¶≤‡ßá‡¶ì', '‡¶π‡¶≤‡ßã', '‡¶π‡¶ø‡¶∏‡¶æ‡¶¨‡ßá', '‡¶π‡¶ì‡ßü‡¶æ', '‡¶π‡¶ì‡ßü‡¶æ‡¶∞', '‡¶π‡¶ì‡ßü‡¶æ‡ßü', '‡¶π‡¶®', \n",
    "                 '‡¶π‡ßã‡¶ï', '‡¶ú‡¶®', '‡¶ú‡¶®‡¶ï‡ßá', '‡¶ú‡¶®‡ßá‡¶∞', '‡¶ú‡¶æ‡¶®‡¶§‡ßá', '‡¶ú‡¶æ‡¶®‡¶æ‡ßü', '‡¶ú‡¶æ‡¶®‡¶ø‡ßü‡ßá', '‡¶ú‡¶æ‡¶®‡¶æ‡¶®‡ßã', '‡¶ú‡¶æ‡¶®‡¶ø‡ßü‡ßá‡¶õ‡ßá', '‡¶ú‡¶®‡ßç‡¶Ø', '‡¶ú‡¶®‡ßç‡¶Ø‡¶ì‡¶ú‡ßá', '‡¶ú‡ßá', \n",
    "                 '‡¶¨‡ßá‡¶∂', '‡¶¶‡ßá‡¶®', '‡¶§‡ßÅ‡¶≤‡ßá', '‡¶õ‡¶ø‡¶≤‡ßá‡¶®', '‡¶ö‡¶æ‡¶®', '‡¶ö‡¶æ‡ßü', '‡¶ö‡ßá‡ßü‡ßá', '‡¶Æ‡ßã‡¶ü', '‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü', '‡¶ü‡¶ø']\n",
    "\n",
    "    sentences = [[word for word in sentence.split() if word not in stopwords] for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "train_sentences = preprocess_texts(train_sentences)\n",
    "train_texts = [' '.join(l) for l in train_sentences]\n",
    "bengali_train_df['sentence'] = train_texts\n",
    "\n",
    "test_sentences = preprocess_texts(test_sentences)\n",
    "test_texts = [' '.join(l) for l in test_sentences]\n",
    "bengali_test_df['sentence'] = test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20264</th>\n",
       "      <td>‡¶≠‡¶æ‡ßü‡ßá‡¶∞‡¶æ ‡¶Ü‡¶™‡¶®‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ß‡¶®‡ßç‡¶Ø‡ßã‡¶¨‡¶æ‡¶¶ ‡¶è‡¶ó‡¶ø‡ßü‡ßá ‡¶ú‡¶æ‡¶ì ‡¶™‡¶æ‡¶∏‡ßá ‡¶Ü‡¶õ‡¶ø ‡¶≠‡¶æ‡¶á</td>\n",
       "      <td>0</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29036</th>\n",
       "      <td>‡¶®‡¶æ‡¶â‡¶ú‡ßÅ‡¶¨‡¶ø‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶®‡¶æ‡¶â‡¶ú‡ßÅ‡¶¨‡¶ø‡¶≤‡ßç‡¶≤‡¶æ‡¶π</td>\n",
       "      <td>0</td>\n",
       "      <td>Meme, TikTok and others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18435</th>\n",
       "      <td>‡¶¶‡ßÅ‡¶á‡¶ú‡¶® ‡¶Ö‡¶™‡¶∞‡¶æ‡¶ß ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞‡¶ø ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¶‡ßÅ‡¶á‡¶ú‡¶®‡¶ï‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶...</td>\n",
       "      <td>0</td>\n",
       "      <td>crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12232</th>\n",
       "      <td>‡¶â‡ßú‡¶ø‡ßü‡ßá ‡¶á ‡¶Æ‡¶æ‡¶∞‡¶≤‡ßã ‡¶™‡ßá‡¶∞‡ßá‡¶∞‡¶æ</td>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14640</th>\n",
       "      <td>‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶è‡¶ï ‡¶ú‡¶æ‡¶§ ‡¶Ö‡¶®‡ßá‡¶ï‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶¨‡ßã‡¶ù‡ßá‡¶®‡¶æ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶ö‡¶æ‡¶Ø‡¶º‡¶®‡¶æ...</td>\n",
       "      <td>0</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  hate  \\\n",
       "20264     ‡¶≠‡¶æ‡ßü‡ßá‡¶∞‡¶æ ‡¶Ü‡¶™‡¶®‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ß‡¶®‡ßç‡¶Ø‡ßã‡¶¨‡¶æ‡¶¶ ‡¶è‡¶ó‡¶ø‡ßü‡ßá ‡¶ú‡¶æ‡¶ì ‡¶™‡¶æ‡¶∏‡ßá ‡¶Ü‡¶õ‡¶ø ‡¶≠‡¶æ‡¶á     0   \n",
       "29036                          ‡¶®‡¶æ‡¶â‡¶ú‡ßÅ‡¶¨‡¶ø‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶®‡¶æ‡¶â‡¶ú‡ßÅ‡¶¨‡¶ø‡¶≤‡ßç‡¶≤‡¶æ‡¶π     0   \n",
       "18435  ‡¶¶‡ßÅ‡¶á‡¶ú‡¶® ‡¶Ö‡¶™‡¶∞‡¶æ‡¶ß ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞‡¶ø ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¶‡ßÅ‡¶á‡¶ú‡¶®‡¶ï‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶...     0   \n",
       "12232                               ‡¶â‡ßú‡¶ø‡ßü‡ßá ‡¶á ‡¶Æ‡¶æ‡¶∞‡¶≤‡ßã ‡¶™‡ßá‡¶∞‡ßá‡¶∞‡¶æ     0   \n",
       "14640  ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶è‡¶ï ‡¶ú‡¶æ‡¶§ ‡¶Ö‡¶®‡ßá‡¶ï‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶¨‡ßã‡¶ù‡ßá‡¶®‡¶æ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶ö‡¶æ‡¶Ø‡¶º‡¶®‡¶æ...     0   \n",
       "\n",
       "                      category  \n",
       "20264                 religion  \n",
       "29036  Meme, TikTok and others  \n",
       "18435                    crime  \n",
       "12232                   sports  \n",
       "14640            entertainment  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25861</th>\n",
       "      <td>‡¶≤‡ßá‡¶ñ‡¶æ‡¶ü‡¶ø ‡¶´‡ßÅ‡¶ü‡¶¨‡¶≤ ‡¶¨‡ßÅ‡¶ù‡ßá‡¶á ‡¶≤‡ßá‡¶ñ‡¶æ‡•§</td>\n",
       "      <td>0</td>\n",
       "      <td>celebrity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11360</th>\n",
       "      <td>‡¶≠‡¶æ‡¶á ‡¶ï‡¶•‡¶æ ‡¶∂‡ßÅ‡¶®‡ßá ‡¶ï‡¶æ‡¶®‡ßç‡¶®‡¶æ ‡¶Ü‡¶∏‡¶≤‡ßã‡•§</td>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø ‡¶®‡¶æ‡¶á‡¶ï‡¶æ</td>\n",
       "      <td>1</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7442</th>\n",
       "      <td>‡¶Æ‡¶æ‡¶ó‡¶ø ‡¶§‡ßã‡¶∞‡ßá ‡¶†‡¶ø‡¶á‡¶ï ‡¶∏‡¶§‡¶® ‡¶¨‡¶æ‡¶ó‡ßÅ‡¶® ‡¶¶‡¶ø‡¶≤‡ßá ‡¶≠‡¶æ‡¶≤‡¶ì ‡¶π‡¶á‡¶§ ‡¶§‡ßã‡¶∞ ‡¶≠‡¶æ‡¶≤‡¶ì‡¶®‡¶æ</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>‡¶ï‡ßü‡¶ü‡¶æ ‡¶¨‡¶ø‡¶ö‡¶æ‡¶∞‡¶ï‡¶¶‡ßá‡¶∞ ‡¶ö‡ßã‡¶¶‡¶æ‡¶∞ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ üò†üò†üò†üò†üòàüòàüòàüò°</td>\n",
       "      <td>1</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  hate       category\n",
       "25861                           ‡¶≤‡ßá‡¶ñ‡¶æ‡¶ü‡¶ø ‡¶´‡ßÅ‡¶ü‡¶¨‡¶≤ ‡¶¨‡ßÅ‡¶ù‡ßá‡¶á ‡¶≤‡ßá‡¶ñ‡¶æ‡•§     0      celebrity\n",
       "11360                          ‡¶≠‡¶æ‡¶á ‡¶ï‡¶•‡¶æ ‡¶∂‡ßÅ‡¶®‡ßá ‡¶ï‡¶æ‡¶®‡ßç‡¶®‡¶æ ‡¶Ü‡¶∏‡¶≤‡ßã‡•§     0         sports\n",
       "1691                                         ‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø ‡¶®‡¶æ‡¶á‡¶ï‡¶æ     1  entertainment\n",
       "7442   ‡¶Æ‡¶æ‡¶ó‡¶ø ‡¶§‡ßã‡¶∞‡ßá ‡¶†‡¶ø‡¶á‡¶ï ‡¶∏‡¶§‡¶® ‡¶¨‡¶æ‡¶ó‡ßÅ‡¶® ‡¶¶‡¶ø‡¶≤‡ßá ‡¶≠‡¶æ‡¶≤‡¶ì ‡¶π‡¶á‡¶§ ‡¶§‡ßã‡¶∞ ‡¶≠‡¶æ‡¶≤‡¶ì‡¶®‡¶æ     1       politics\n",
       "2569                 ‡¶ï‡ßü‡¶ü‡¶æ ‡¶¨‡¶ø‡¶ö‡¶æ‡¶∞‡¶ï‡¶¶‡ßá‡¶∞ ‡¶ö‡ßã‡¶¶‡¶æ‡¶∞ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ üò†üò†üò†üò†üòàüòàüòàüò°     1  entertainment"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(bengali_train_df.head())\n",
    "display(bengali_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali_train_df.to_csv('../data/bengali_hatespeech_sample_train_preprocessed.csv', index=False)\n",
    "bengali_test_df.to_csv('../data/bengali_hatespeech_sample_test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
