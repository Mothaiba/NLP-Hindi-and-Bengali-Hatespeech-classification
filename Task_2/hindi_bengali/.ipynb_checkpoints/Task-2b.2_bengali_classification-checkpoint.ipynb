{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "import random\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(234)\n",
    "np.random.seed(345)\n",
    "random.seed(456)\n",
    "torch.manual_seed(567)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>সমকামী হুজুর</td>\n",
       "      <td>0</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ছাএলীগ সালা দের নিসিদ্দ হক</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>কাওয়া গদি ছারলে বুজবে জুতা কেমনে খায়</td>\n",
       "      <td>0</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>কাউয়া কাদের বড় মাগীখোর ভিডিও পিক দেখলে বুঝা লু...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>অপু ভালো কথা ছোট করবেনা</td>\n",
       "      <td>0</td>\n",
       "      <td>Meme, TikTok and others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  hate  \\\n",
       "0                                       সমকামী হুজুর     0   \n",
       "1                         ছাএলীগ সালা দের নিসিদ্দ হক     1   \n",
       "2               কাওয়া গদি ছারলে বুজবে জুতা কেমনে খায়     0   \n",
       "3  কাউয়া কাদের বড় মাগীখোর ভিডিও পিক দেখলে বুঝা লু...     1   \n",
       "4                            অপু ভালো কথা ছোট করবেনা     0   \n",
       "\n",
       "                  category  \n",
       "0                 religion  \n",
       "1                 politics  \n",
       "2                 politics  \n",
       "3                 politics  \n",
       "4  Meme, TikTok and others  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ben_train_df = pd.read_csv('save/bengali_hatespeech_sample_train_preprocessed.csv')\n",
    "ben_test_df = pd.read_csv('save/bengali_hatespeech_sample_test_preprocessed.csv')\n",
    "\n",
    "display(ben_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "[['সমকামী', 'হুজুর'], ['ছাএলীগ', 'সালা', 'দের', 'নিসিদ্দ', 'হক'], ['কাওয়া', 'গদি', 'ছারলে', 'বুজবে', 'জুতা', 'কেমনে', 'খায়']]\n",
      "[0 1 0 ... 1 0 0]\n",
      "\n",
      "Test data:\n",
      "[['মহিলাকে', 'রিমানডে'], ['তুর', 'রিপাতকে', 'মন', 'চাইছিল', 'ছেড়ে', 'গেলি', 'সাথে', 'মিত্যে', 'অভিনয়', 'করলি', 'দুনিয়া', 'উঠালি', 'তুর', 'নরকেও', 'ঠাঁই', 'হবেনা', 'তুই', 'আকাশের', 'মিতুর', 'মত', 'করলি', 'তুর', 'মত', 'বিশ্বাস', 'ঘাতকনীর', 'ফাসি', 'হউক', 'রিফাত', 'তোকে', 'ফেলে', 'যেত', 'হয়তবা', 'বেঁচে', 'যেত', 'সরল', 'ভালবাসাকে', 'হত্যা', 'করলি', 'তুই', 'নারী', 'জাতের', 'কলংক', 'তুকে', 'দেখলে', 'বুঝা', 'আসলে', 'তোর', 'পরিক্ষলপনা', 'তোর', 'চোখ', 'মুখ', 'সাক্ষী', 'তুইয়ি', 'জরিত', 'নারী', 'জাতের', 'কলংক'], ['হুমায়ুন', 'আজাদ', 'এতো', 'বড়', 'ক্রাক', 'মাতাল']]\n",
      "[0 0 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# remove empty texts\n",
    "ben_train_df = ben_train_df[ben_train_df.sentence.str.len() > 0]\n",
    "# extract sentences and labels\n",
    "train_sentences = [text.split() for text in ben_train_df['sentence']]\n",
    "train_labels = ben_train_df['hate'].to_numpy()\n",
    "\n",
    "# remove empty texts\n",
    "ben_test_df = ben_test_df[ben_test_df.sentence.str.len() > 0]\n",
    "# extract sentences and labels\n",
    "test_sentences = [text.split() for text in ben_test_df['sentence']]\n",
    "test_labels = ben_test_df['hate'].to_numpy()\n",
    "\n",
    "print('Train data:')\n",
    "print(train_sentences[:3])\n",
    "print(train_labels)\n",
    "print()\n",
    "print('Test data:')\n",
    "print(test_sentences[:3])\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare vocab set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 15231\n"
     ]
    }
   ],
   "source": [
    "# load mapping {word -> id} and {id -> word}\n",
    "with open('save/bengali_word_to_int_dict.json') as f:\n",
    "    word_to_int = json.load(f)\n",
    "with open('save/bengali_int_to_word_dict.json') as f:\n",
    "    int_to_word = json.load(f)\n",
    "    int_to_word = {int(k) : v for k, v in int_to_word.items()}\n",
    "\n",
    "# get vocab_size\n",
    "vocab_size = len(word_to_int)\n",
    "print(f'vocab_size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [[word_to_int[word] for word in sentence] for sentence in train_sentences]\n",
    "test_sentences = [[word_to_int[word] for word in sentence if word in word_to_int] for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = 'save/embedding_weights.pt'\n",
    "embedding_size = 300\n",
    "att_dim = 150\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOFDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.data = []\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            if len(sentence):\n",
    "                self.data.append(\n",
    "                    (torch.tensor(sentence, dtype=torch.long), \n",
    "                     torch.tensor(label, dtype=torch.float))\n",
    "                )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "def preprocess_batch(batch):\n",
    "    texts, labels = list(zip(*batch))\n",
    "    seq_lens = torch.tensor([len(text) for text in texts], dtype=torch.long)\n",
    "    texts = pad_sequence(texts, padding_value=0)\n",
    "    labels = torch.tensor(labels).unsqueeze(1)\n",
    "\n",
    "    seq_lens, sorted_idx = seq_lens.sort(descending=True)\n",
    "    texts = texts[:,sorted_idx]\n",
    "    labels = labels[sorted_idx]\n",
    "    return texts, seq_lens, labels\n",
    "\n",
    "train_dataset = HOFDataset(train_sentences, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True, collate_fn=preprocess_batch)\n",
    "\n",
    "test_dataset = HOFDataset(test_sentences, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         shuffle=False, collate_fn=preprocess_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_seq(seq_lens):\n",
    "    mask = torch.zeros((len(seq_lens), max(seq_lens))).bool()\n",
    "    for i, seq_len in enumerate(seq_lens):\n",
    "        mask[i, seq_len:] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi classifier:\n",
      "Classifier(\n",
      "  (embed): Embedding(20402, 300)\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "Bengali classifier:\n",
      "Classifier(\n",
      "  (embed): Embedding(15231, 300)\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# get hindi_vocab_size\n",
    "with open('../../Task_1/save/word_to_int_dict.json') as f:\n",
    "    hindi_word_to_int = json.load(f)\n",
    "hindi_vocab_size = len(hindi_word_to_int)\n",
    "\n",
    "# define classifier\n",
    "class Classifier(Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(hindi_vocab_size, embedding_size)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_size,\n",
    "                                               num_heads=10,\n",
    "                                               dropout=0.5,)\n",
    "        self.attention.requires_grad = False # fix all layers except embedding.\n",
    "\n",
    "        self.fc = nn.Linear(embedding_size, 1)\n",
    "        self.fc.requires_grad = False # fix all layers except embedding.\n",
    "\n",
    "    def forward(self, inp, seq_lens):\n",
    "        out = self.embed(inp)\n",
    "        pad_mask = mask_seq(seq_lens)\n",
    "        att_out, _ = self.attention(out, out, out, key_padding_mask=pad_mask)\n",
    "        out = F.layer_norm(out + att_out, (out.size(2), ))\n",
    "        out = self.fc(out).squeeze(2)\n",
    "        pred = torch.zeros((out.size(1), 1))\n",
    "        for i, seq_len in enumerate(seq_lens):\n",
    "            pred[i, 0] = out[:seq_len, i].mean()\n",
    "        return pred\n",
    "\n",
    "# load pre-trained hindi classifier\n",
    "hindi_clf = Classifier().to(device)\n",
    "hindi_model_weight_path = '../hindi_hindi/save/hindi_clf.pt'\n",
    "hindi_clf.load_state_dict(torch.load(hindi_model_weight_path, map_location=torch.device(device)))\n",
    "print('Hindi classifier:')\n",
    "print(hindi_clf.eval())\n",
    "\n",
    "# replace the embedding layer to make it a bengali classifier\n",
    "bengali_embed = nn.Embedding(vocab_size, embedding_size)\n",
    "bengali_clf = hindi_clf\n",
    "bengali_clf._modules['embed'] = bengali_embed\n",
    "print('Bengali classifier:')\n",
    "print(bengali_clf.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(bengali_clf.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test():\n",
    "    losses = 0\n",
    "    acc_cnt = 0\n",
    "    cnt = 0\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    for texts, seq_lens, labels in test_loader:\n",
    "        pred = bengali_clf(texts.to(device), seq_lens).detach().to('cpu')\n",
    "        loss = criterion(pred, labels)\n",
    "        losses += loss.detach().item() * len(texts)\n",
    "        acc_cnt += sum((pred > 0) == (labels > 0)).item()\n",
    "        preds.extend(pred.view(-1))\n",
    "        true_labels.extend(labels.view(-1))\n",
    "        cnt += texts.size(1)\n",
    "    \n",
    "    preds = np.array(preds) > 0\n",
    "    macro_f1 = f1_score(true_labels, preds, average='macro')\n",
    "    weighted_f1 = f1_score(true_labels, preds, average='weighted')\n",
    "    return losses / cnt, acc_cnt / cnt, macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:11<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Train loss: 1.1500, acc: 0.5812. Test loss: 0.8553, acc: 0.6209, macro_f1: 0.6193, weighted_f1: 0.6175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:11<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: Train loss: 1.0646, acc: 0.6808. Test loss: 0.8179, acc: 0.6364, macro_f1: 0.6358, weighted_f1: 0.6348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:13<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: Train loss: 0.9888, acc: 0.7177. Test loss: 0.8067, acc: 0.6651, macro_f1: 0.6651, weighted_f1: 0.6649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: Train loss: 0.8955, acc: 0.7516. Test loss: 0.8345, acc: 0.6791, macro_f1: 0.6791, weighted_f1: 0.6792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: Train loss: 0.8612, acc: 0.7713. Test loss: 0.8405, acc: 0.6767, macro_f1: 0.6767, weighted_f1: 0.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: Train loss: 0.7838, acc: 0.7962. Test loss: 0.8344, acc: 0.6899, macro_f1: 0.6898, weighted_f1: 0.6902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:13<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: Train loss: 0.7298, acc: 0.8153. Test loss: 0.8495, acc: 0.6946, macro_f1: 0.6945, weighted_f1: 0.6949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:11<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: Train loss: 0.6725, acc: 0.8327. Test loss: 0.8845, acc: 0.6992, macro_f1: 0.6987, weighted_f1: 0.6996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:11<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: Train loss: 0.6380, acc: 0.8556. Test loss: 0.8895, acc: 0.7054, macro_f1: 0.7053, weighted_f1: 0.7057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:11<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train loss: 0.5720, acc: 0.8698. Test loss: 0.9649, acc: 0.7085, macro_f1: 0.7083, weighted_f1: 0.7089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:11<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train loss: 0.5317, acc: 0.8880. Test loss: 1.0052, acc: 0.7023, macro_f1: 0.7021, weighted_f1: 0.7027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train loss: 0.4825, acc: 0.8985. Test loss: 0.9923, acc: 0.7054, macro_f1: 0.7054, weighted_f1: 0.7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train loss: 0.4160, acc: 0.9168. Test loss: 1.0584, acc: 0.7101, macro_f1: 0.7099, weighted_f1: 0.7104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:11<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train loss: 0.3330, acc: 0.9277. Test loss: 1.0871, acc: 0.7078, macro_f1: 0.7077, weighted_f1: 0.7080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:13<00:00, 11.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train loss: 0.3236, acc: 0.9434. Test loss: 1.0962, acc: 0.7171, macro_f1: 0.7166, weighted_f1: 0.7174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train loss: 0.2632, acc: 0.9502. Test loss: 1.2154, acc: 0.7171, macro_f1: 0.7168, weighted_f1: 0.7174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train loss: 0.2257, acc: 0.9610. Test loss: 1.2940, acc: 0.7163, macro_f1: 0.7161, weighted_f1: 0.7166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:13<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train loss: 0.2057, acc: 0.9695. Test loss: 1.3737, acc: 0.7302, macro_f1: 0.7300, weighted_f1: 0.7306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train loss: 0.2035, acc: 0.9715. Test loss: 1.3253, acc: 0.7233, macro_f1: 0.7231, weighted_f1: 0.7235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train loss: 0.1493, acc: 0.9760. Test loss: 1.4247, acc: 0.7271, macro_f1: 0.7269, weighted_f1: 0.7275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train loss: 0.1468, acc: 0.9779. Test loss: 1.4180, acc: 0.7341, macro_f1: 0.7338, weighted_f1: 0.7344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:13<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train loss: 0.1282, acc: 0.9850. Test loss: 1.5971, acc: 0.7171, macro_f1: 0.7169, weighted_f1: 0.7174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train loss: 0.1165, acc: 0.9837. Test loss: 1.6503, acc: 0.7209, macro_f1: 0.7207, weighted_f1: 0.7213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train loss: 0.1024, acc: 0.9867. Test loss: 1.6572, acc: 0.7248, macro_f1: 0.7246, weighted_f1: 0.7251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train loss: 0.0920, acc: 0.9884. Test loss: 1.6331, acc: 0.7202, macro_f1: 0.7199, weighted_f1: 0.7205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train loss: 0.0791, acc: 0.9895. Test loss: 1.6353, acc: 0.7194, macro_f1: 0.7192, weighted_f1: 0.7197\n",
      "Early stopping: test accuracy does not increase after 5 epochs\n"
     ]
    }
   ],
   "source": [
    "list_test_acc = []\n",
    "early_stop = 5\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    losses = 0.\n",
    "    acc_cnt = 0\n",
    "    cnt = 0\n",
    "    bengali_clf.train()\n",
    "    for texts, seq_lens, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = bengali_clf(texts.to(device), seq_lens)\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item() * len(texts)\n",
    "        acc_cnt += sum((pred.to('cpu') > 0) == (labels > 0)).item()\n",
    "        cnt += texts.size(1)\n",
    "\n",
    "    epoch_loss = losses / cnt\n",
    "    epoch_acc = acc_cnt / cnt\n",
    "    test_loss, test_acc, test_macro_f1, test_weighted_f1 = predict_test()\n",
    "    print(f'Epoch {epoch:2}: Train loss: {epoch_loss:.4f}, acc: {epoch_acc:.4f}. '\n",
    "        f'Test loss: {test_loss:.4f}, acc: {test_acc:.4f}, '\n",
    "        f'macro_f1: {test_macro_f1:.4f}, weighted_f1: {test_weighted_f1:.4f}',\n",
    "        flush=True)\n",
    "\n",
    "    list_test_acc.append(test_acc)\n",
    "    if len(list_test_acc) > early_stop and max(list_test_acc[-early_stop:]) <= max(list_test_acc[:-early_stop]):\n",
    "        print(f'Early stopping: test accuracy does not increase after {early_stop} epochs')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the word-embedding layer weights\n",
    "embedding_weights = bengali_clf.embed.state_dict()\n",
    "torch.save(embedding_weights, f'save/bengali_embedding_weights_.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
