{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7b691cff90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(26)\n",
    "np.random.seed(62)\n",
    "torch.manual_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ভায়েরা আপনাদের ধন্যোবাদ এগিয়ে জাও পাসে আছি ভাই</td>\n",
       "      <td>0</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>নাউজুবিল্লাহ নাউজুবিল্লাহ</td>\n",
       "      <td>0</td>\n",
       "      <td>Meme, TikTok and others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>দুইজন অপরাধ সরকারি চাকরি হিসেবে দুইজনকে বাংলাদ...</td>\n",
       "      <td>0</td>\n",
       "      <td>crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>উড়িয়ে ই মারলো পেরেরা</td>\n",
       "      <td>0</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>পুরুষ এক জাত অনেকসময় বোঝেনা সময় বুঝতে চায়না...</td>\n",
       "      <td>0</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  hate  \\\n",
       "0     ভায়েরা আপনাদের ধন্যোবাদ এগিয়ে জাও পাসে আছি ভাই     0   \n",
       "1                          নাউজুবিল্লাহ নাউজুবিল্লাহ     0   \n",
       "2  দুইজন অপরাধ সরকারি চাকরি হিসেবে দুইজনকে বাংলাদ...     0   \n",
       "3                               উড়িয়ে ই মারলো পেরেরা     0   \n",
       "4  পুরুষ এক জাত অনেকসময় বোঝেনা সময় বুঝতে চায়না...     0   \n",
       "\n",
       "                  category  \n",
       "0                 religion  \n",
       "1  Meme, TikTok and others  \n",
       "2                    crime  \n",
       "3                   sports  \n",
       "4            entertainment  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ben_train_df = pd.read_csv('../../data/bengali_hatespeech_sample_train_preprocessed.csv')\n",
    "ben_test_df = pd.read_csv('../../data/bengali_hatespeech_sample_test_preprocessed.csv')\n",
    "\n",
    "display(ben_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "[['ভায়েরা', 'আপনাদের', 'ধন্যোবাদ', 'এগিয়ে', 'জাও', 'পাসে', 'আছি', 'ভাই'], ['নাউজুবিল্লাহ', 'নাউজুবিল্লাহ'], ['দুইজন', 'অপরাধ', 'সরকারি', 'চাকরি', 'হিসেবে', 'দুইজনকে', 'বাংলাদেশ', 'বের', 'দেওয়া', 'আপনারা', 'কমেন্ট', 'জানাবেন']]\n",
      "[0 0 0 ... 0 0 0]\n",
      "\n",
      "Test data:\n",
      "[['লেখাটি', 'ফুটবল', 'বুঝেই', 'লেখা।'], ['ভাই', 'কথা', 'শুনে', 'কান্না', 'আসলো।'], ['খানকি', 'নাইকা']]\n",
      "[0 0 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# remove empty texts\n",
    "ben_train_df = ben_train_df[ben_train_df.sentence.str.len() > 0]\n",
    "# extract sentences and labels\n",
    "train_sentences = [text.split() for text in ben_train_df['sentence']]\n",
    "train_labels = ben_train_df['hate'].to_numpy()\n",
    "\n",
    "# remove empty texts\n",
    "ben_test_df = ben_test_df[ben_test_df.sentence.str.len() > 0]\n",
    "# extract sentences and labels\n",
    "test_sentences = [text.split() for text in ben_test_df['sentence']]\n",
    "test_labels = ben_test_df['hate'].to_numpy()\n",
    "\n",
    "print('Train data:')\n",
    "print(train_sentences[:3])\n",
    "print(train_labels)\n",
    "print()\n",
    "print('Test data:')\n",
    "print(test_sentences[:3])\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare vocab set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 16005\n"
     ]
    }
   ],
   "source": [
    "# load mapping {word -> id} and {id -> word}\n",
    "with open('save/word_to_int_dict.json') as f:\n",
    "    word_to_int = json.load(f)\n",
    "with open('save/int_to_word_dict.json') as f:\n",
    "    int_to_word = json.load(f)\n",
    "\n",
    "# get vocab_size\n",
    "vocab_size = len(word_to_int)\n",
    "print(f'vocab_size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [[word_to_int[word] for word in sentence] for sentence in train_sentences]\n",
    "test_sentences = [[word_to_int[word] for word in sentence if word in word_to_int] for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = 'save/embedding_weights.pt'\n",
    "embedding_size = 300\n",
    "att_dim = 150\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOFDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.data = []\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            if len(sentence):\n",
    "                self.data.append(\n",
    "                    (torch.tensor(sentence, dtype=torch.long), \n",
    "                     torch.tensor(label, dtype=torch.float))\n",
    "                )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "def preprocess_batch(batch):\n",
    "    texts, labels = list(zip(*batch))\n",
    "    seq_lens = torch.tensor([len(text) for text in texts], dtype=torch.long)\n",
    "    texts = pad_sequence(texts, padding_value=0)\n",
    "    labels = torch.tensor(labels).unsqueeze(1)\n",
    "\n",
    "    seq_lens, sorted_idx = seq_lens.sort(descending=True)\n",
    "    texts = texts[:,sorted_idx]\n",
    "    labels = labels[sorted_idx]\n",
    "    return texts, seq_lens, labels\n",
    "\n",
    "train_dataset = HOFDataset(train_sentences, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True, collate_fn=preprocess_batch)\n",
    "\n",
    "test_dataset = HOFDataset(test_sentences, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         shuffle=False, collate_fn=preprocess_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_seq(seq_lens):\n",
    "    mask = torch.zeros((len(seq_lens), max(seq_lens))).bool()\n",
    "    for i, seq_len in enumerate(seq_lens):\n",
    "        mask[i, seq_len:] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi classifier\n",
      "Classifier(\n",
      "  (embed): Embedding(20402, 300)\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "Bengali classifier\n",
      "Classifier(\n",
      "  (embed): Embedding(16005, 300)\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# get hindi_vocab_size\n",
    "with open('../../Task_1/save/word_to_int_dict.json') as f:\n",
    "    hindi_word_to_int = json.load(f)\n",
    "hindi_vocab_size = len(hindi_word_to_int)\n",
    "\n",
    "# define classifier\n",
    "class Classifier(Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(hindi_vocab_size, embedding_size)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_size,\n",
    "                                               num_heads=10,\n",
    "                                               dropout=0.5,)\n",
    "        self.attention.requires_grad = False # fix all layers except embedding.\n",
    "\n",
    "        self.fc = nn.Linear(embedding_size, 1)\n",
    "        self.fc.requires_grad = False # fix all layers except embedding.\n",
    "\n",
    "    def forward(self, inp, seq_lens):\n",
    "        out = self.embed(inp)\n",
    "        pad_mask = mask_seq(seq_lens)\n",
    "        att_out, _ = self.attention(out, out, out, key_padding_mask=pad_mask)\n",
    "        out = F.layer_norm(out + att_out, (out.size(2), ))\n",
    "        out = self.fc(out).squeeze(2)\n",
    "        pred = torch.zeros((out.size(1), 1))\n",
    "        for i, seq_len in enumerate(seq_lens):\n",
    "            pred[i, 0] = out[:seq_len, i].mean()\n",
    "        return pred\n",
    "\n",
    "# load pre-trained hindi classifier\n",
    "hindi_clf = Classifier().to(device)\n",
    "hindi_model_weight_path = '../hindi_hindi/save/hindi_clf.pt'\n",
    "hindi_clf.load_state_dict(torch.load(hindi_model_weight_path, map_location=torch.device(device)))\n",
    "print('Hindi classifier:')\n",
    "print(hindi_clf.eval())\n",
    "\n",
    "# replace the embedding layer to make it a bengali classifier\n",
    "bengali_embed = nn.Embedding(vocab_size, embedding_size)\n",
    "bengali_clf = hindi_clf\n",
    "bengali_clf._modules['embed'] = bengali_embed\n",
    "print('Bengali classifier:')\n",
    "print(bengali_clf.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(bengali_clf.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test():\n",
    "    losses = 0\n",
    "    acc_cnt = 0\n",
    "    cnt = 0\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    for texts, seq_lens, labels in test_loader:\n",
    "        pred = bengali_clf(texts.to(device), seq_lens).detach().to('cpu')\n",
    "        loss = criterion(pred, labels)\n",
    "        losses += loss.detach().item() * len(texts)\n",
    "        acc_cnt += sum((pred > 0) == (labels > 0)).item()\n",
    "        preds.extend(pred.view(-1))\n",
    "        true_labels.extend(labels.view(-1))\n",
    "        cnt += texts.size(1)\n",
    "    \n",
    "    preds = np.array(preds) > 0\n",
    "    macro_f1 = f1_score(true_labels, preds, average='macro')\n",
    "    weighted_f1 = f1_score(true_labels, preds, average='weighted')\n",
    "    return losses / cnt, acc_cnt / cnt, macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:13<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Train loss: 1.5486, acc: 0.5793. Test loss: 0.8961, acc: 0.6206, macro_f1: 0.6201, weighted_f1: 0.6191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:13<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: Train loss: 1.3249, acc: 0.6644. Test loss: 0.8336, acc: 0.6553, macro_f1: 0.6552, weighted_f1: 0.6548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: Train loss: 1.1857, acc: 0.7063. Test loss: 0.8147, acc: 0.6723, macro_f1: 0.6723, weighted_f1: 0.6723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:15<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: Train loss: 1.1552, acc: 0.7325. Test loss: 0.8060, acc: 0.6971, macro_f1: 0.6970, weighted_f1: 0.6973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: Train loss: 1.0559, acc: 0.7552. Test loss: 0.7990, acc: 0.7002, macro_f1: 0.6999, weighted_f1: 0.7005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:15<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: Train loss: 1.0046, acc: 0.7760. Test loss: 0.8048, acc: 0.7009, macro_f1: 0.7009, weighted_f1: 0.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:15<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: Train loss: 0.9330, acc: 0.7968. Test loss: 0.8201, acc: 0.7117, macro_f1: 0.7116, weighted_f1: 0.7121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:15<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: Train loss: 0.8813, acc: 0.8176. Test loss: 0.8292, acc: 0.7110, macro_f1: 0.7109, weighted_f1: 0.7113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:15<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: Train loss: 0.8410, acc: 0.8350. Test loss: 0.8628, acc: 0.7195, macro_f1: 0.7192, weighted_f1: 0.7198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train loss: 0.7128, acc: 0.8507. Test loss: 0.8848, acc: 0.7141, macro_f1: 0.7139, weighted_f1: 0.7144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train loss: 0.6705, acc: 0.8685. Test loss: 0.8744, acc: 0.7372, macro_f1: 0.7368, weighted_f1: 0.7376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:15<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train loss: 0.6088, acc: 0.8902. Test loss: 0.9307, acc: 0.7349, macro_f1: 0.7346, weighted_f1: 0.7353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train loss: 0.5165, acc: 0.9024. Test loss: 0.9813, acc: 0.7249, macro_f1: 0.7248, weighted_f1: 0.7252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:15<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train loss: 0.4561, acc: 0.9191. Test loss: 1.0004, acc: 0.7311, macro_f1: 0.7309, weighted_f1: 0.7314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train loss: 0.4054, acc: 0.9333. Test loss: 1.0575, acc: 0.7388, macro_f1: 0.7387, weighted_f1: 0.7390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train loss: 0.3348, acc: 0.9446. Test loss: 1.1214, acc: 0.7342, macro_f1: 0.7338, weighted_f1: 0.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:15<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train loss: 0.2978, acc: 0.9547. Test loss: 1.2018, acc: 0.7280, macro_f1: 0.7278, weighted_f1: 0.7283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train loss: 0.2586, acc: 0.9605. Test loss: 1.2930, acc: 0.7326, macro_f1: 0.7325, weighted_f1: 0.7328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train loss: 0.2239, acc: 0.9678. Test loss: 1.3448, acc: 0.7233, macro_f1: 0.7233, weighted_f1: 0.7235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:14<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train loss: 0.2226, acc: 0.9725. Test loss: 1.3909, acc: 0.7334, macro_f1: 0.7332, weighted_f1: 0.7337\n",
      "Early stopping: test accuracy does not increase after 5 epochs\n"
     ]
    }
   ],
   "source": [
    "list_test_acc = []\n",
    "early_stop = 5\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    losses = 0.\n",
    "    acc_cnt = 0\n",
    "    cnt = 0\n",
    "    bengali_clf.train()\n",
    "    for texts, seq_lens, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = bengali_clf(texts.to(device), seq_lens)\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item() * len(texts)\n",
    "        acc_cnt += sum((pred.to('cpu') > 0) == (labels > 0)).item()\n",
    "        cnt += texts.size(1)\n",
    "\n",
    "    epoch_loss = losses / cnt\n",
    "    epoch_acc = acc_cnt / cnt\n",
    "    test_loss, test_acc, test_macro_f1, test_weighted_f1 = predict_test()\n",
    "    print(f'Epoch {epoch:2}: Train loss: {epoch_loss:.4f}, acc: {epoch_acc:.4f}. '\n",
    "        f'Test loss: {test_loss:.4f}, acc: {test_acc:.4f}, '\n",
    "        f'macro_f1: {test_macro_f1:.4f}, weighted_f1: {test_weighted_f1:.4f}',\n",
    "        flush=True)\n",
    "\n",
    "    list_test_acc.append(test_acc)\n",
    "    if len(list_test_acc) > early_stop and max(list_test_acc[-early_stop:]) <= max(list_test_acc[:-early_stop]):\n",
    "        print(f'Early stopping: test accuracy does not increase after {early_stop} epochs')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
