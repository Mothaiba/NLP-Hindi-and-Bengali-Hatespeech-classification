{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Sample Bengali dataset and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "import random\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(234)\n",
    "np.random.seed(345)\n",
    "random.seed(456)\n",
    "torch.manual_seed(567)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Bengali dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi - training data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HOF    2469\n",
       "NOT    2196\n",
       "Name: task_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hindi - test data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NOT    713\n",
       "HOF    605\n",
       "Name: task_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the size and number of HOF, NOT sentences in hindi-dataset\n",
    "print('Hindi - training data')\n",
    "hindi_train_df = pd.read_csv('../../data/hindi_hatespeech.tsv', sep='\\t')\n",
    "display(hindi_train_df['task_1'].value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print('Hindi - test data')\n",
    "hindi_test_df = pd.read_csv('../../data/hasoc2019_hi_test_gold_2919.tsv', sep='\\t')\n",
    "display(hindi_test_df['task_1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data from the Bengali dataset so that it has the same size and distribution as in Hindi dataset.\n",
    "# We also save and preprocess the remaining bengali data (excluding sample train and test), this can\n",
    "# be useful for Task 3.\n",
    "\n",
    "bengali_df = pd.read_csv('../../data/bengali_hatespeech.csv')\n",
    "\n",
    "# separate HATE and NOT-HATE observations\n",
    "bengali_hate_df = bengali_df[bengali_df['hate']==1]\n",
    "bengali_not_df = bengali_df[bengali_df['hate']==0]\n",
    "\n",
    "# sampling data for train/test/other datasets\n",
    "bengali_hate_sample_df, bengali_hate_other_df = train_test_split(bengali_hate_df, train_size=2469+605, random_state=1)\n",
    "bengali_hate_train_df, bengali_hate_test_df = train_test_split(bengali_hate_sample_df, train_size=2469, random_state=2)\n",
    "\n",
    "bengali_not_sample_df, bengali_not_other_df = train_test_split(bengali_not_df, train_size=2196+713, random_state=3)\n",
    "bengali_not_train_df, bengali_not_test_df = train_test_split(bengali_not_sample_df, train_size=2196, random_state=4)\n",
    "\n",
    "# form train data, test data, other-data\n",
    "bengali_train_df = pd.concat([bengali_hate_train_df, bengali_not_train_df]).sample(frac=1)\n",
    "bengali_test_df = pd.concat([bengali_hate_test_df, bengali_not_test_df]).sample(frac=1)\n",
    "bengali_other_df = pd.concat([bengali_hate_other_df, bengali_not_other_df]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali - sample training data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    2469\n",
       "0    2196\n",
       "Name: hate, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bengali - sample test data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    713\n",
       "1    605\n",
       "Name: hate, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bengali - remaining data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    17091\n",
       "1     6926\n",
       "Name: hate, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show stats of sample Bengali datasets\n",
    "print('Bengali - sample training data')\n",
    "display(bengali_train_df['hate'].value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print('Bengali - sample test data')\n",
    "display(bengali_test_df['hate'].value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print('Bengali - remaining data')\n",
    "display(bengali_other_df['hate'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sample Bengali datasets\n",
    "bengali_train_df.to_csv('save/bengali_hatespeech_sample_train.csv', index=False)\n",
    "bengali_test_df.to_csv('save/bengali_hatespeech_sample_test.csv', index=False)\n",
    "bengali_other_df.to_csv('save/bengali_hatespeech_other.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = bengali_train_df['sentence']\n",
    "test_sentences = bengali_test_df['sentence']\n",
    "other_sentences = bengali_other_df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(sentences):\n",
    "    # remove user taggings\n",
    "    user_tag_pattern = re.compile(r'\\@\\w*')\n",
    "    sentences = [re.sub(user_tag_pattern, ' ', sentence) for sentence in sentences]\n",
    "    # lower case\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "    # remove punctuations\n",
    "    punctuation = string.punctuation[:2] + string.punctuation[3:]\n",
    "    translator = str.maketrans(punctuation, ' '*len(punctuation))\n",
    "    def remove_punc(s):\n",
    "        s = s.translate(translator)\n",
    "        return s\n",
    "\n",
    "    sentences = [remove_punc(sentence) for sentence in sentences]\n",
    "    \n",
    "    # remove stopwords BENGALI\n",
    "    # source: https://www.ranks.nl/stopwords/bengali\n",
    "    stopwords = ['‡¶Ö‡¶¨‡¶∂‡ßç‡¶Ø', '‡¶Ö‡¶®‡ßá‡¶ï', '‡¶Ö‡¶®‡ßá‡¶ï‡ßá', '‡¶Ö‡¶®‡ßá‡¶ï‡ßá‡¶á', '‡¶Ö‡¶®‡ßç‡¶§‡¶§', '‡¶Ö‡¶•‡¶¨‡¶æ', '‡¶Ö‡¶•‡¶ö', '‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡¶§', '‡¶Ö‡¶®‡ßç‡¶Ø', '‡¶Ü‡¶ú', '‡¶Ü‡¶õ‡ßá', '‡¶Ü‡¶™‡¶®‡¶æ‡¶∞', \n",
    "                 '‡¶Ü‡¶™‡¶®‡¶ø', '‡¶Ü‡¶¨‡¶æ‡¶∞', '‡¶Ü‡¶Æ‡¶∞‡¶æ', '‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá', '‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞', '‡¶Ü‡¶Æ‡¶æ‡¶∞', '‡¶Ü‡¶Æ‡¶ø', '‡¶Ü‡¶∞‡¶ì', '‡¶Ü‡¶∞', '‡¶Ü‡¶ó‡ßá', '‡¶Ü‡¶ó‡ßá‡¶á', '‡¶Ü‡¶á', \n",
    "                 '‡¶Ö‡¶§‡¶è‡¶¨', '‡¶Ü‡¶ó‡¶æ‡¶Æ‡ßÄ', '‡¶Ö‡¶¨‡¶ß‡¶ø', '‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ', '‡¶Ü‡¶¶‡ßç‡¶Ø‡¶≠‡¶æ‡¶ó‡ßá', '‡¶è‡¶á', '‡¶è‡¶ï‡¶á', '‡¶è‡¶ï‡ßá', '‡¶è‡¶ï‡¶ü‡¶ø', '‡¶è‡¶ñ‡¶®', '‡¶è‡¶ñ‡¶®‡¶ì', '‡¶è‡¶ñ‡¶æ‡¶®‡ßá', \n",
    "                 '‡¶è‡¶ñ‡¶æ‡¶®‡ßá‡¶á', '‡¶è‡¶ü‡¶ø', '‡¶è‡¶ü‡¶æ', '‡¶è‡¶ü‡¶æ‡¶á', '‡¶è‡¶§‡¶ü‡¶æ‡¶á', '‡¶è‡¶¨‡¶Ç', '‡¶è‡¶ï‡¶¨‡¶æ‡¶∞', '‡¶è‡¶¨‡¶æ‡¶∞', '‡¶è‡¶¶‡ßá‡¶∞', '‡¶è‡¶Å‡¶¶‡ßá‡¶∞', '‡¶è‡¶Æ‡¶®', '‡¶è‡¶Æ‡¶®‡¶ï‡ßÄ', '‡¶è‡¶≤', \n",
    "                 '‡¶è‡¶∞', '‡¶è‡¶∞‡¶æ', '‡¶è‡¶Å‡¶∞‡¶æ', '‡¶è‡¶∏', '‡¶è‡¶§', '‡¶è‡¶§‡ßá', '‡¶è‡¶∏‡ßá', '‡¶è‡¶ï‡ßá', '‡¶è', '‡¶ê', ' ‡¶á', '‡¶á‡¶π‡¶æ', '‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø', '‡¶â‡¶®‡¶ø', '‡¶â‡¶™‡¶∞', \n",
    "                 '‡¶â‡¶™‡¶∞‡ßá', '‡¶â‡¶ö‡¶ø‡¶§', '‡¶ì', '‡¶ì‡¶á', '‡¶ì‡¶∞', '‡¶ì‡¶∞‡¶æ', '‡¶ì‡¶Å‡¶∞', '‡¶ì‡¶Å‡¶∞‡¶æ', '‡¶ì‡¶ï‡ßá', '‡¶ì‡¶¶‡ßá‡¶∞', '‡¶ì‡¶Å‡¶¶‡ßá‡¶∞', '‡¶ì‡¶ñ‡¶æ‡¶®‡ßá', '‡¶ï‡¶§', '‡¶ï‡¶¨‡ßá', \n",
    "                 '‡¶ï‡¶∞‡¶§‡ßá', '‡¶ï‡ßü‡ßá‡¶ï', '‡¶ï‡ßü‡ßá‡¶ï‡¶ü‡¶ø', '‡¶ï‡¶∞‡¶¨‡ßá', '‡¶ï‡¶∞‡¶≤‡ßá‡¶®', '‡¶ï‡¶∞‡¶æ‡¶∞', '‡¶ï‡¶æ‡¶∞‡¶ì', '‡¶ï‡¶∞‡¶æ', '‡¶ï‡¶∞‡¶ø', '‡¶ï‡¶∞‡¶ø‡ßü‡ßá', '‡¶ï‡¶∞‡¶æ‡¶∞', '‡¶ï‡¶∞‡¶æ‡¶á', \n",
    "                 '‡¶ï‡¶∞‡¶≤‡ßá', '‡¶ï‡¶∞‡¶≤‡ßá‡¶®', '‡¶ï‡¶∞‡¶ø‡¶§‡ßá', '‡¶ï‡¶∞‡¶ø‡ßü‡¶æ', '‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®', '‡¶ï‡¶∞‡¶õ‡ßá', '‡¶ï‡¶∞‡¶õ‡ßá‡¶®', '‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®', '‡¶ï‡¶∞‡ßá‡¶õ‡ßá', '‡¶ï‡¶∞‡ßá‡¶®', '‡¶ï‡¶∞‡¶¨‡ßá‡¶®', \n",
    "                 '‡¶ï‡¶∞‡¶æ‡ßü', '‡¶ï‡¶∞‡ßá', '‡¶ï‡¶∞‡ßá‡¶á', '‡¶ï‡¶æ‡¶õ', '‡¶ï‡¶æ‡¶õ‡ßá', '‡¶ï‡¶æ‡¶ú‡ßá', '‡¶ï‡¶æ‡¶∞‡¶£', '‡¶ï‡¶ø‡¶õ‡ßÅ', '‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á', '‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ', '‡¶ï‡¶ø‡¶Ç‡¶¨‡¶æ', '‡¶ï‡¶ø', '‡¶ï‡ßÄ', '‡¶ï‡ßá‡¶â', \n",
    "                 '‡¶ï‡ßá‡¶â‡¶á', '‡¶ï‡¶æ‡¶â‡¶ï‡ßá', '‡¶ï‡ßá‡¶®', '‡¶ï‡ßá', '‡¶ï‡ßã‡¶®‡¶ì', '‡¶ï‡ßã‡¶®‡ßã', '‡¶ï‡ßã‡¶®', '‡¶ï‡¶ñ‡¶®‡¶ì', '‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá', '‡¶ñ‡ßÅ‡¶¨\t‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶ó‡¶ø‡ßü‡ßá', '‡¶ó‡¶ø‡ßü‡ßá‡¶õ‡ßá', \n",
    "                 '‡¶ó‡ßá‡¶õ‡ßá', '‡¶ó‡ßá‡¶≤', '‡¶ó‡ßá‡¶≤‡ßá', '‡¶ó‡ßã‡¶ü‡¶æ', '‡¶ö‡¶≤‡ßá', '‡¶õ‡¶æ‡ßú‡¶æ', '‡¶õ‡¶æ‡ßú‡¶æ‡¶ì', '‡¶õ‡¶ø‡¶≤‡ßá‡¶®', '‡¶õ‡¶ø‡¶≤', '‡¶ú‡¶®‡ßç‡¶Ø', '‡¶ú‡¶æ‡¶®‡¶æ', '‡¶†‡¶ø‡¶ï', '‡¶§‡¶ø‡¶®‡¶ø', \n",
    "                 '‡¶§‡¶ø‡¶®‡¶ê', '‡¶§‡¶ø‡¶®‡¶ø‡¶ì', '‡¶§‡¶ñ‡¶®', '‡¶§‡¶¨‡ßá', '‡¶§‡¶¨‡ßÅ', '‡¶§‡¶æ‡¶Å‡¶¶‡ßá‡¶∞', '‡¶§‡¶æ‡¶Å‡¶æ‡¶π‡¶æ‡¶∞‡¶æ', '‡¶§‡¶æ‡¶Å‡¶∞‡¶æ', '‡¶§‡¶æ‡¶Å‡¶∞', '‡¶§‡¶æ‡¶Å‡¶ï‡ßá', '‡¶§‡¶æ‡¶á', '‡¶§‡ßá‡¶Æ‡¶®', '‡¶§‡¶æ‡¶ï‡ßá', \n",
    "                 '‡¶§‡¶æ‡¶π‡¶æ', '‡¶§‡¶æ‡¶π‡¶æ‡¶§‡ßá', '‡¶§‡¶æ‡¶π‡¶æ‡¶∞', '‡¶§‡¶æ‡¶¶‡ßá‡¶∞', '‡¶§‡¶æ‡¶∞‡¶™‡¶∞', '‡¶§‡¶æ‡¶∞‡¶æ', '‡¶§‡¶æ‡¶∞‡ßà', '‡¶§‡¶æ‡¶∞', '‡¶§‡¶æ‡¶π‡¶≤‡ßá', '‡¶§‡¶ø‡¶®‡¶ø', '‡¶§‡¶æ', '‡¶§‡¶æ‡¶ì', '‡¶§‡¶æ‡¶§‡ßá', \n",
    "                 '‡¶§‡ßã', '‡¶§‡¶§', '‡¶§‡ßÅ‡¶Æ‡¶ø', '‡¶§‡ßã‡¶Æ‡¶æ‡¶∞', '‡¶§‡¶•‡¶æ', '‡¶•‡¶æ‡¶ï‡ßá', '‡¶•‡¶æ‡¶ï‡¶æ', '‡¶•‡¶æ‡¶ï‡¶æ‡ßü', '‡¶•‡ßá‡¶ï‡ßá', '‡¶•‡ßá‡¶ï‡ßá‡¶ì', '‡¶•‡¶æ‡¶ï‡¶¨‡ßá', '‡¶•‡¶æ‡¶ï‡ßá‡¶®', '‡¶•‡¶æ‡¶ï‡¶¨‡ßá‡¶®', \n",
    "                 '‡¶•‡ßá‡¶ï‡ßá‡¶á', '‡¶¶‡¶ø‡¶ï‡ßá', '‡¶¶‡¶ø‡¶§‡ßá', '‡¶¶‡¶ø‡ßü‡ßá', '‡¶¶‡¶ø‡ßü‡ßá‡¶õ‡ßá', '‡¶¶‡¶ø‡ßü‡ßá‡¶õ‡ßá‡¶®', '‡¶¶‡¶ø‡¶≤‡ßá‡¶®', '‡¶¶‡ßÅ', '‡¶¶‡ßÅ‡¶ü‡¶ø', '‡¶¶‡ßÅ‡¶ü‡ßã', '‡¶¶‡ßá‡ßü', '‡¶¶‡ßá‡¶ì‡ßü‡¶æ', '‡¶¶‡ßá‡¶ì‡ßü‡¶æ‡¶∞', \n",
    "                 '‡¶¶‡ßá‡¶ñ‡¶æ', '‡¶¶‡ßá‡¶ñ‡ßá', '‡¶¶‡ßá‡¶ñ‡¶§‡ßá', '‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ', '‡¶ß‡¶∞‡ßá', '‡¶ß‡¶∞‡¶æ', '‡¶®‡ßü', '‡¶®‡¶æ‡¶®‡¶æ', '‡¶®‡¶æ', '‡¶®‡¶æ‡¶ï‡¶ø', '‡¶®‡¶æ‡¶ó‡¶æ‡¶¶', '‡¶®‡¶ø‡¶§‡ßá', '‡¶®‡¶ø‡¶ú‡ßá', '‡¶®‡¶ø‡¶ú‡ßá‡¶á', \n",
    "                 '‡¶®‡¶ø‡¶ú‡ßá‡¶∞', '‡¶®‡¶ø‡¶ú‡ßá‡¶¶‡ßá‡¶∞', '‡¶®‡¶ø‡ßü‡ßá', '‡¶®‡ßá‡¶ì‡ßü‡¶æ', '‡¶®‡ßá‡¶ì‡ßü‡¶æ‡¶∞', '‡¶®‡ßá‡¶á', '‡¶®‡¶æ‡¶á', '‡¶™‡¶ï‡ßç‡¶∑‡ßá', '‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§', '‡¶™‡¶æ‡¶ì‡ßü‡¶æ', '‡¶™‡¶æ‡¶∞‡ßá‡¶®', '‡¶™‡¶æ‡¶∞‡¶ø', '‡¶™‡¶æ‡¶∞‡ßá', \n",
    "                 '‡¶™‡¶∞‡ßá', '‡¶™‡¶∞‡ßá‡¶á', '‡¶™‡¶∞‡ßá‡¶ì', '‡¶™‡¶∞', '‡¶™‡ßá‡ßü‡ßá', '‡¶™‡ßç‡¶∞‡¶§‡¶ø', '‡¶™‡ßç‡¶∞‡¶≠‡ßÉ‡¶§‡¶ø', '‡¶™‡ßç‡¶∞‡¶æ‡ßü', '‡¶´‡ßá‡¶∞', '‡¶´‡¶≤‡ßá', '‡¶´‡¶ø‡¶∞‡ßá', '‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞', '‡¶¨‡¶≤‡¶§‡ßá', \n",
    "                 '‡¶¨‡¶≤‡¶≤‡ßá‡¶®', '‡¶¨‡¶≤‡ßá‡¶õ‡ßá‡¶®', '‡¶¨‡¶≤‡¶≤', '‡¶¨‡¶≤‡¶æ', '‡¶¨‡¶≤‡ßá‡¶®', '‡¶¨‡¶≤‡ßá', '‡¶¨‡¶π‡ßÅ', '‡¶¨‡¶∏‡ßá', '‡¶¨‡¶æ‡¶∞', '‡¶¨‡¶æ', '‡¶¨‡¶ø‡¶®‡¶æ', '‡¶¨‡¶∞‡¶Ç', '‡¶¨‡¶¶‡¶≤‡ßá', '‡¶¨‡¶æ‡¶¶‡ßá', \n",
    "                 '‡¶¨‡¶æ‡¶∞', '‡¶¨‡¶ø‡¶∂‡ßá‡¶∑', '‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶®\t‡¶¨‡¶ø‡¶∑‡ßü‡¶ü‡¶ø', '‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞', '‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡¶æ‡¶∞‡ßá', '‡¶≠‡¶æ‡¶¨‡ßá', '‡¶≠‡¶æ‡¶¨‡ßá‡¶á', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá‡¶á', '‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá‡¶ì', '‡¶Æ‡¶ß‡ßç‡¶Ø‡¶≠‡¶æ‡¶ó‡ßá', \n",
    "                 '‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá', '‡¶Æ‡¶æ‡¶§‡ßç‡¶∞', '‡¶Æ‡¶§‡ßã', '‡¶Æ‡¶§‡ßã‡¶á', '‡¶Æ‡ßã‡¶ü‡ßá‡¶á', '‡¶Ø‡¶ñ‡¶®', '‡¶Ø‡¶¶‡¶ø', '‡¶Ø‡¶¶‡¶ø‡¶ì', '‡¶Ø‡¶æ‡¶¨‡ßá', '‡¶Ø‡¶æ‡ßü', '‡¶Ø‡¶æ‡¶ï‡ßá', '‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ', '‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ‡¶∞', \n",
    "                 '‡¶Ø‡¶§', '‡¶Ø‡¶§‡¶ü‡¶æ', '‡¶Ø‡¶æ', '‡¶Ø‡¶æ‡¶∞', '‡¶Ø‡¶æ‡¶∞‡¶æ', '‡¶Ø‡¶æ‡¶Å‡¶∞', '‡¶Ø‡¶æ‡¶Å‡¶∞‡¶æ', '‡¶Ø‡¶æ‡¶¶‡ßá‡¶∞', '‡¶Ø‡¶æ‡¶®', '‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá', '‡¶Ø‡ßá‡¶§‡ßá', '‡¶Ø‡¶æ‡¶§‡ßá', '‡¶Ø‡ßá‡¶®', '‡¶Ø‡ßá‡¶Æ‡¶®', \n",
    "                 '‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá', '‡¶Ø‡¶ø‡¶®‡¶ø', '‡¶Ø‡ßá', '‡¶∞‡ßá‡¶ñ‡ßá', '‡¶∞‡¶æ‡¶ñ‡¶æ', '‡¶∞‡ßü‡ßá‡¶õ‡ßá', '‡¶∞‡¶ï‡¶Æ', '‡¶∂‡ßÅ‡¶ß‡ßÅ', '‡¶∏‡¶ô‡ßç‡¶ó‡ßá', '‡¶∏‡¶ô‡ßç‡¶ó‡ßá‡¶ì', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶§', '‡¶∏‡¶¨', '‡¶∏‡¶¨‡¶æ‡¶∞', '‡¶∏‡¶π', \n",
    "                 '‡¶∏‡ßÅ‡¶§‡¶∞‡¶æ‡¶Ç', '‡¶∏‡¶π‡¶ø‡¶§', '‡¶∏‡ßá‡¶á', '‡¶∏‡ßá‡¶ü‡¶æ', '‡¶∏‡ßá‡¶ü‡¶ø', '‡¶∏‡ßá‡¶ü‡¶æ‡¶á', '‡¶∏‡ßá‡¶ü‡¶æ‡¶ì', '‡¶∏‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡¶§‡¶ø', '‡¶∏‡ßá‡¶ñ‡¶æ‡¶®', '‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá', '‡¶∏‡ßá', '‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü', '‡¶∏‡ßç‡¶¨‡ßü‡¶Ç', \n",
    "                 '‡¶π‡¶á‡¶§‡ßá', '‡¶π‡¶á‡¶¨‡ßá', '‡¶π‡ßà‡¶≤‡ßá', '‡¶π‡¶á‡ßü‡¶æ', '‡¶π‡¶ö‡ßç‡¶õ‡ßá', '‡¶π‡¶§', '‡¶π‡¶§‡ßá', '‡¶π‡¶§‡ßá‡¶á', '‡¶π‡¶¨‡ßá', '‡¶π‡¶¨‡ßá‡¶®', '‡¶π‡ßü‡ßá‡¶õ‡¶ø‡¶≤', '‡¶π‡ßü‡ßá‡¶õ‡ßá', '‡¶π‡ßü‡ßá‡¶õ‡ßá‡¶®', '‡¶π‡ßü‡ßá', \n",
    "                 '‡¶π‡ßü‡¶®‡¶ø', '‡¶π‡ßü', '‡¶π‡ßü‡ßá‡¶á', '‡¶π‡ßü‡¶§‡ßã', '‡¶π‡¶≤', '‡¶π‡¶≤‡ßá', '‡¶π‡¶≤‡ßá‡¶á', '‡¶π‡¶≤‡ßá‡¶ì', '‡¶π‡¶≤‡ßã', '‡¶π‡¶ø‡¶∏‡¶æ‡¶¨‡ßá', '‡¶π‡¶ì‡ßü‡¶æ', '‡¶π‡¶ì‡ßü‡¶æ‡¶∞', '‡¶π‡¶ì‡ßü‡¶æ‡ßü', '‡¶π‡¶®', \n",
    "                 '‡¶π‡ßã‡¶ï', '‡¶ú‡¶®', '‡¶ú‡¶®‡¶ï‡ßá', '‡¶ú‡¶®‡ßá‡¶∞', '‡¶ú‡¶æ‡¶®‡¶§‡ßá', '‡¶ú‡¶æ‡¶®‡¶æ‡ßü', '‡¶ú‡¶æ‡¶®‡¶ø‡ßü‡ßá', '‡¶ú‡¶æ‡¶®‡¶æ‡¶®‡ßã', '‡¶ú‡¶æ‡¶®‡¶ø‡ßü‡ßá‡¶õ‡ßá', '‡¶ú‡¶®‡ßç‡¶Ø', '‡¶ú‡¶®‡ßç‡¶Ø‡¶ì‡¶ú‡ßá', '‡¶ú‡ßá', \n",
    "                 '‡¶¨‡ßá‡¶∂', '‡¶¶‡ßá‡¶®', '‡¶§‡ßÅ‡¶≤‡ßá', '‡¶õ‡¶ø‡¶≤‡ßá‡¶®', '‡¶ö‡¶æ‡¶®', '‡¶ö‡¶æ‡ßü', '‡¶ö‡ßá‡ßü‡ßá', '‡¶Æ‡ßã‡¶ü', '‡¶Ø‡¶•‡ßá‡¶∑‡ßç‡¶ü', '‡¶ü‡¶ø']\n",
    "\n",
    "    sentences = [[word for word in sentence.split() if word not in stopwords] for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "train_sentences = preprocess_texts(train_sentences)\n",
    "train_texts = [' '.join(l) for l in train_sentences]\n",
    "bengali_train_df['sentence'] = train_texts\n",
    "\n",
    "test_sentences = preprocess_texts(test_sentences)\n",
    "test_texts = [' '.join(l) for l in test_sentences]\n",
    "bengali_test_df['sentence'] = test_texts\n",
    "\n",
    "other_sentences = preprocess_texts(other_sentences)\n",
    "other_texts = [' '.join(l) for l in other_sentences]\n",
    "bengali_other_df['sentence'] = other_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20440</th>\n",
       "      <td>‡¶∏‡¶Æ‡¶ï‡¶æ‡¶Æ‡ßÄ ‡¶π‡ßÅ‡¶ú‡ßÅ‡¶∞</td>\n",
       "      <td>0</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>‡¶õ‡¶æ‡¶è‡¶≤‡ßÄ‡¶ó ‡¶∏‡¶æ‡¶≤‡¶æ ‡¶¶‡ßá‡¶∞ ‡¶®‡¶ø‡¶∏‡¶ø‡¶¶‡ßç‡¶¶ ‡¶π‡¶ï</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22116</th>\n",
       "      <td>‡¶ï‡¶æ‡¶ì‡ßü‡¶æ ‡¶ó‡¶¶‡¶ø ‡¶õ‡¶æ‡¶∞‡¶≤‡ßá ‡¶¨‡ßÅ‡¶ú‡¶¨‡ßá ‡¶ú‡ßÅ‡¶§‡¶æ ‡¶ï‡ßá‡¶Æ‡¶®‡ßá ‡¶ñ‡¶æ‡ßü</td>\n",
       "      <td>0</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>‡¶ï‡¶æ‡¶â‡ßü‡¶æ ‡¶ï‡¶æ‡¶¶‡ßá‡¶∞ ‡¶¨‡ßú ‡¶Æ‡¶æ‡¶ó‡ßÄ‡¶ñ‡ßã‡¶∞ ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶™‡¶ø‡¶ï ‡¶¶‡ßá‡¶ñ‡¶≤‡ßá ‡¶¨‡ßÅ‡¶ù‡¶æ ‡¶≤‡ßÅ...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27690</th>\n",
       "      <td>‡¶Ö‡¶™‡ßÅ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶•‡¶æ ‡¶õ‡ßã‡¶ü ‡¶ï‡¶∞‡¶¨‡ßá‡¶®‡¶æ</td>\n",
       "      <td>0</td>\n",
       "      <td>Meme, TikTok and others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  hate  \\\n",
       "20440                                       ‡¶∏‡¶Æ‡¶ï‡¶æ‡¶Æ‡ßÄ ‡¶π‡ßÅ‡¶ú‡ßÅ‡¶∞     0   \n",
       "7678                          ‡¶õ‡¶æ‡¶è‡¶≤‡ßÄ‡¶ó ‡¶∏‡¶æ‡¶≤‡¶æ ‡¶¶‡ßá‡¶∞ ‡¶®‡¶ø‡¶∏‡¶ø‡¶¶‡ßç‡¶¶ ‡¶π‡¶ï     1   \n",
       "22116               ‡¶ï‡¶æ‡¶ì‡ßü‡¶æ ‡¶ó‡¶¶‡¶ø ‡¶õ‡¶æ‡¶∞‡¶≤‡ßá ‡¶¨‡ßÅ‡¶ú‡¶¨‡ßá ‡¶ú‡ßÅ‡¶§‡¶æ ‡¶ï‡ßá‡¶Æ‡¶®‡ßá ‡¶ñ‡¶æ‡ßü     0   \n",
       "7368   ‡¶ï‡¶æ‡¶â‡ßü‡¶æ ‡¶ï‡¶æ‡¶¶‡ßá‡¶∞ ‡¶¨‡ßú ‡¶Æ‡¶æ‡¶ó‡ßÄ‡¶ñ‡ßã‡¶∞ ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶™‡¶ø‡¶ï ‡¶¶‡ßá‡¶ñ‡¶≤‡ßá ‡¶¨‡ßÅ‡¶ù‡¶æ ‡¶≤‡ßÅ...     1   \n",
       "27690                            ‡¶Ö‡¶™‡ßÅ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶•‡¶æ ‡¶õ‡ßã‡¶ü ‡¶ï‡¶∞‡¶¨‡ßá‡¶®‡¶æ     0   \n",
       "\n",
       "                      category  \n",
       "20440                 religion  \n",
       "7678                  politics  \n",
       "22116                 politics  \n",
       "7368                  politics  \n",
       "27690  Meme, TikTok and others  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17168</th>\n",
       "      <td>‡¶Æ‡¶π‡¶ø‡¶≤‡¶æ‡¶ï‡ßá ‡¶∞‡¶ø‡¶Æ‡¶æ‡¶®‡¶°‡ßá</td>\n",
       "      <td>0</td>\n",
       "      <td>crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17512</th>\n",
       "      <td>‡¶§‡ßÅ‡¶∞ ‡¶∞‡¶ø‡¶™‡¶æ‡¶§‡¶ï‡ßá ‡¶Æ‡¶® ‡¶ö‡¶æ‡¶á‡¶õ‡¶ø‡¶≤ ‡¶õ‡ßá‡¶°‡¶º‡ßá ‡¶ó‡ßá‡¶≤‡¶ø ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶ø‡¶§‡ßç‡¶Ø‡ßá ‡¶Ö...</td>\n",
       "      <td>0</td>\n",
       "      <td>crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21184</th>\n",
       "      <td>‡¶π‡ßÅ‡¶Æ‡¶æ‡ßü‡ßÅ‡¶® ‡¶Ü‡¶ú‡¶æ‡¶¶ ‡¶è‡¶§‡ßã ‡¶¨‡ßú ‡¶ï‡ßç‡¶∞‡¶æ‡¶ï ‡¶Æ‡¶æ‡¶§‡¶æ‡¶≤</td>\n",
       "      <td>0</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22900</th>\n",
       "      <td>‡¶¨‡¶æ‡¶Ç‡¶æ‡¶≤ ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶∏‡¶æ‡¶π‡¶∏‡¶ø ‡¶á‡¶â‡¶ü‡ßÅ‡¶¨‡¶æ‡¶∞üëçüëçüëç</td>\n",
       "      <td>0</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7063</th>\n",
       "      <td>‡¶ï‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ß‡¶®‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶•‡¶æ ‡¶ö‡ßã‡¶∞ ‡¶ï‡¶æ‡¶¶‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶´‡¶æ‡¶ü‡¶æ ‡¶ï‡ßá‡¶∑‡ßç‡¶ü ‡¶ï‡ßã‡¶á ...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  hate  category\n",
       "17168                                    ‡¶Æ‡¶π‡¶ø‡¶≤‡¶æ‡¶ï‡ßá ‡¶∞‡¶ø‡¶Æ‡¶æ‡¶®‡¶°‡ßá     0     crime\n",
       "17512  ‡¶§‡ßÅ‡¶∞ ‡¶∞‡¶ø‡¶™‡¶æ‡¶§‡¶ï‡ßá ‡¶Æ‡¶® ‡¶ö‡¶æ‡¶á‡¶õ‡¶ø‡¶≤ ‡¶õ‡ßá‡¶°‡¶º‡ßá ‡¶ó‡ßá‡¶≤‡¶ø ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶ø‡¶§‡ßç‡¶Ø‡ßá ‡¶Ö...     0     crime\n",
       "21184                    ‡¶π‡ßÅ‡¶Æ‡¶æ‡ßü‡ßÅ‡¶® ‡¶Ü‡¶ú‡¶æ‡¶¶ ‡¶è‡¶§‡ßã ‡¶¨‡ßú ‡¶ï‡ßç‡¶∞‡¶æ‡¶ï ‡¶Æ‡¶æ‡¶§‡¶æ‡¶≤     0  religion\n",
       "22900                ‡¶¨‡¶æ‡¶Ç‡¶æ‡¶≤ ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶∏‡¶æ‡¶π‡¶∏‡¶ø ‡¶á‡¶â‡¶ü‡ßÅ‡¶¨‡¶æ‡¶∞üëçüëçüëç     0  politics\n",
       "7063   ‡¶ï‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ß‡¶®‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶•‡¶æ ‡¶ö‡ßã‡¶∞ ‡¶ï‡¶æ‡¶¶‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶´‡¶æ‡¶ü‡¶æ ‡¶ï‡ßá‡¶∑‡ßç‡¶ü ‡¶ï‡ßã‡¶á ...     1  politics"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13576</th>\n",
       "      <td>‡¶õ‡¶¨‡¶ø‡¶ü‡¶æ ‡¶ö‡¶ñ‡ßá ‡¶™‡¶æ‡¶®‡¶ø ‡¶¨‡¶æ‡¶≤ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶õ‡¶¨‡¶ø</td>\n",
       "      <td>0</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>‡¶™‡¶æ‡¶™‡¶® ‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø‡¶∞ ‡¶™‡ßã‡¶≤‡¶æ ‡¶Æ‡ßá‡¶á‡¶® ‡¶∂‡ßü‡¶§‡¶æ‡¶® ‡¶ì‡¶∞‡ßá ‡¶≤‡¶æ‡¶•‡¶•‡ßÄ ‡¶Æ‡¶æ‡¶á‡¶∞‡¶æ ‡¶¨‡¶ø...</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17401</th>\n",
       "      <td>‡¶™‡¶∞‡¶ø‡¶¨‡ßá‡¶∂‡¶ü‡¶æ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞ ‡¶°‡¶ø‡¶∏‡¶ø ‡¶ï‡ßá‡¶π ‡¶ó‡¶æ‡¶≤‡¶ø ‡¶°‡¶ø‡¶∏‡¶ø ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶¢‡ßá‡¶≤‡ßá...</td>\n",
       "      <td>0</td>\n",
       "      <td>crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19883</th>\n",
       "      <td>‡¶á‡ßü‡¶æ ‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶á‡¶´‡¶§‡¶æ‡¶∞‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶¶‡ßã‡ßü‡¶æ ‡¶ï‡¶¨‡ßÅ‡¶≤‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶∏‡¶Æ‡ßü ‡¶¶‡ßã‡ßü...</td>\n",
       "      <td>0</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9722</th>\n",
       "      <td>‡¶Æ‡¶æ‡¶¶‡¶æ‡¶∞‡¶ö‡ßã‡¶¶ ‡¶á‡¶π‡ßÅ‡¶ß‡¶ø‡¶∞ ‡¶¶‡¶æ‡¶≤‡¶æ‡¶≤ ‡¶¨‡¶æ‡¶¨‡¶∞‡¶ø ‡¶Æ‡¶∏‡¶ú‡¶ø‡¶¶ ‡¶≠‡¶æ‡¶Ç‡¶≤‡ßá ‡¶Æ‡¶®‡ßç‡¶ß‡¶ø‡¶∞...</td>\n",
       "      <td>1</td>\n",
       "      <td>Meme, TikTok and others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  hate  \\\n",
       "13576                        ‡¶õ‡¶¨‡¶ø‡¶ü‡¶æ ‡¶ö‡¶ñ‡ßá ‡¶™‡¶æ‡¶®‡¶ø ‡¶¨‡¶æ‡¶≤ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶õ‡¶¨‡¶ø     0   \n",
       "974    ‡¶™‡¶æ‡¶™‡¶® ‡¶ñ‡¶æ‡¶®‡¶ï‡¶ø‡¶∞ ‡¶™‡ßã‡¶≤‡¶æ ‡¶Æ‡ßá‡¶á‡¶® ‡¶∂‡ßü‡¶§‡¶æ‡¶® ‡¶ì‡¶∞‡ßá ‡¶≤‡¶æ‡¶•‡¶•‡ßÄ ‡¶Æ‡¶æ‡¶á‡¶∞‡¶æ ‡¶¨‡¶ø...     1   \n",
       "17401  ‡¶™‡¶∞‡¶ø‡¶¨‡ßá‡¶∂‡¶ü‡¶æ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞ ‡¶°‡¶ø‡¶∏‡¶ø ‡¶ï‡ßá‡¶π ‡¶ó‡¶æ‡¶≤‡¶ø ‡¶°‡¶ø‡¶∏‡¶ø ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶¢‡ßá‡¶≤‡ßá...     0   \n",
       "19883  ‡¶á‡ßü‡¶æ ‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶π ‡¶á‡¶´‡¶§‡¶æ‡¶∞‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶¶‡ßã‡ßü‡¶æ ‡¶ï‡¶¨‡ßÅ‡¶≤‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶∏‡¶Æ‡ßü ‡¶¶‡ßã‡ßü...     0   \n",
       "9722   ‡¶Æ‡¶æ‡¶¶‡¶æ‡¶∞‡¶ö‡ßã‡¶¶ ‡¶á‡¶π‡ßÅ‡¶ß‡¶ø‡¶∞ ‡¶¶‡¶æ‡¶≤‡¶æ‡¶≤ ‡¶¨‡¶æ‡¶¨‡¶∞‡¶ø ‡¶Æ‡¶∏‡¶ú‡¶ø‡¶¶ ‡¶≠‡¶æ‡¶Ç‡¶≤‡ßá ‡¶Æ‡¶®‡ßç‡¶ß‡¶ø‡¶∞...     1   \n",
       "\n",
       "                      category  \n",
       "13576            entertainment  \n",
       "974                     sports  \n",
       "17401                    crime  \n",
       "19883                 religion  \n",
       "9722   Meme, TikTok and others  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('train:')\n",
    "display(bengali_train_df.head())\n",
    "print('test:')\n",
    "display(bengali_test_df.head())\n",
    "print('other:')\n",
    "display(bengali_other_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare vocab set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 15231\n"
     ]
    }
   ],
   "source": [
    "train_sentences = [text.split() for text in bengali_train_df['sentence']]\n",
    "\n",
    "# flattened_words: all words in all sentences\n",
    "flattened_words = [word for sentence in train_sentences for word in sentence]\n",
    "\n",
    "# word_counter: count occurences of each word\n",
    "word_counter = Counter(flattened_words)\n",
    "\n",
    "# V: vocabulary\n",
    "V = list(set(flattened_words))\n",
    "vocab_size = len(V)\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "\n",
    "# mappings word->id and id->word\n",
    "word_to_int = {}\n",
    "int_to_word = {}\n",
    "for i, word in enumerate(V):\n",
    "    word_to_int[word] = i\n",
    "    int_to_word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dicts for transformation word <-> int\n",
    "with open('save/bengali_word_to_int_dict.json', 'w') as f:\n",
    "    json.dump(word_to_int, f)\n",
    "with open('save/bengali_int_to_word_dict.json', 'w') as f:\n",
    "    json.dump(int_to_word, f)\n",
    "with open('save/bengali_word_counter.json', 'w') as f:\n",
    "    json.dump(word_counter, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali_train_df.to_csv('save/bengali_hatespeech_sample_train_preprocessed.csv', index=False)\n",
    "bengali_test_df.to_csv('save/bengali_hatespeech_sample_test_preprocessed.csv', index=False)\n",
    "bengali_other_df.to_csv('save/bengali_hatespeech_other_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
